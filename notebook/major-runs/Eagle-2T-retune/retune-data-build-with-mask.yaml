#
# Custom multiple datasource, built as a single datapack
#
datapack:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # If using relative path, this should be relative to the trainer script path
  data_path: /datapath/eval-retune/pack-with-mask/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Mixing mode to use, this is used to alternate between datasets
  #
  # - concat  : Keep It Simple Silly, lets just concat the datasets together
  # - shuffle : Dataset is mixed on a per sample level
  #
  # (@TODO: Advance operations)
  # - batch   : Meaning one dataset worth per batch, partial batches are discarded
  mixing_mode: "shuffle"

#
# Default settings used across all datasets in the datapack
# These settings can be overriden by the dataset specific settings
#
default:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # Datapath here is entirely optional, and only used if you intend to save each individual dataset
  # seperately (makes it easier to tweak and rebuild the datapack if it crash mid-way)
  #
  # The dataset index will be appended to the default value, if set
  # ---
  data_path: /datapath/eval-retune/partial-with-mask/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Additional source dataset params, used to grab subsets of the dataset
  # ---
  # source_dataset_params:
  #   language: en

  # Sort the dataset by length, useful to reduce gpu waiting time (also useful for RWKV long context coherence)
  # ---
  # sort_by_length: false
  # sort_asc: True # Sort in ascending order, true = shortest first, false = longest first

  # Limit the document count, to an offset/length limit
  # If an int value is used, it is interprated as document count
  # If a floating value (<1.0) is used, it is interprated as a percentage of the dataset
  # ---
  # dataset_offset: -1
  # dataset_length: -1

  # Use data_dir, if you are using source=text/json/etc
  # If using relative path, this should be relative to the trainer script path
  # source_data_dir: ../dataset-text/

  # After loading the dataset, split out test data used for validation, 
  # This process is skipped if the dataset includes a test split
  #
  # If given a float value, a percentage of the dataset is used (1.0 being 100%)
  # If given an int value, the number of data sample is used.
  #
  # Due to the limitaitons in the trainer process, there is always a minimum of 1 test sample
  test_split: 1 # Intentionally set to a low sample for test, cause the real eval is humans
  test_split_shuffle: True

  # Tokenizer to use, use either the inbuilt 'neox', or 'world' tokenizer
  # If using a custom tokenizer, provide the HF tokenizer name/path
  # ---
  tokenizer: world

  # Minimum / Maximum token size of the dataset to use
  # useful for filtering out small noisy data samples from large datasets
  # (eg. removal of small articles of less then 1024 tokens from wikipedia)
  #
  # This is ignored, if set to -1
  # ---
  min_token_size: -1
  max_token_size: -1

  # Custom text column to use, useful for dataset with alternative training columns labels
  # This is checked before multi column merging, default is null (disabled)
  # eg: 'code'
  # ---
  # custom_text_key: 'code'

  # Multi Column merging process, default setting is used to support and merge
  # "instruction", "input", "output", datasets. To disable set multi_column_keys to []
  #
  # A minimum of 2 columns is required, with non empty data, for the merge to occur
  # If no match is found, this will fallback to the default prompt/completion or text column, 
  # or throw an error if the default fallback is not found
  #
  # IMPORTANT NOTE: as newlines are commonly used for multi_column_suffix, etc. 
  #                 you should use single quotes to ensure such values dun get escaped.
  #                 eg. multi_column_suffix: ['\n\n']
  #
  # See: https://github.com/RWKV/RWKV-infctx-trainer/issues/34
  # Need to use " or the new lines won't be tokenized properly
  # ---
  # multi_column_keys: ["instruction", "input", "output"]
  # multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
  # multi_column_suffix: ["\n\n", "\n\n", "\n\n"]
  # multi_column_train_mask: [true, false, true]
  # multi_column_separator: "\n\n"
  
  # Conversation merging process
  # useful for merging full conversational datasets, into single documents
  # default is off, (or set conversation_key to [])
  # conversation_formatting supports "iopairs" or "sender" for now.
  # ---
  # conversation_format: 'iopairs'
  # conversation_key: 'conversation'
  # conversation_end_of_conversation: "\n\nUser:"

  # Iopairs specific config
  # This means that every object in the conversation object is a pair of input output.
  # In future it will also support a format where one of the keys dictates the format style
  # if conversation_key is set to null, it will use the root object as the conversation object
  # ---
  # conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
  # conversation_input_key_mask: {'input': false, 'output': True}
  # conversation_sender_suffix: {'input': "", 'output': ""}

  # Sender specific config
  # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
  # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
  # conversation_input_key_map: {'message': "\n\n{sender}: ", 'context': ''}
  # conversation_sender_key: 'sender'
  # conversation_sender_value_map: {'user': 'User', 'assistant': 'Assistant', 'system': 'System'}
  # conversation_sender_mask: {'user': false, 'assistant': True, 'system': false}
  # conversation_sender_suffix: {'user': "", 'assistant': "", 'system': ""}

  # If processing prompt/completion jsonl pairs, the prompt is masked by default
  # use this flag to disable this default behaviour
  # ---
  # disable_prompt_completion_mask: false

  # ----------------------------
  # Dataset split usage
  # ----------------------------

  source_dataset_split: "train"
  test_dataset_split: "do-not-use-test-split"

  # ----------------------------
  # Rechunking support
  # ----------------------------

  # Rechunking of text dataset, this is done only when source is set as 'text'
  # and will merge the various sentencees, into larger chunks up to the target size
  #
  # Defaults to 2048
  #
  # This is ignored, if source is not set as text (unless text_rechunk_force)
  # This is ignored, if set to zero / -1
  # ---
  text_rechunk_size: 4096

  # Apply text rechunk to the dataset, even if its not a 'text' source
  # This is done only after dataset filtering, and if source is not 'text'
  # ---
  text_rechunk_force: False

  # Used to disable the automated text rechunkin for text files, if set as false
  # ---
  text_rechunk_auto: True

  # ----------------------------
  # Dataset packing support
  # Recommended to be used with mixed documents sized finetuning
  # For foundation model "from scratch", rechunking is typically used instead
  # ----------------------------

  # Boolean flag to enable / disable dataset packing
  packing_enable: True

  # Used to ensure all training samples wihin this batch size is the same length
  # Ideally this should align exactly with your real "batch size"
  #
  # Uses, `8 * (3 * 4 * 5 * 6 * 7) = 20160` for default, as it should align across
  # a large number of batch size combinations. This helps reduce the amount of
  # misaligned batches, and thus reduce the amount of wasted training time.
  #
  # This is tagged to datapack.batchsize, unless overriden here or on a dataset level
  # ---
  # packing_batchsize: 20160

  # Chunking size to align within each batch, this ideally should be equal to
  # the training context length used.
  packing_chunksize: 4096

  # Minimum size to pack up to, this should be a multiple of packing_chunksize
  # defautls to -1, which equals to packing_chunksize
  packing_min_ctx_len: 4096

  # Pack the data sequentially if possible, in accordance to the dataset sequence
  # this can be used together with sort_by_length, otherwise a shuffle will be done
  packing_in_sequence: False

  # ----------------------------
  # Specal use caes flags
  # ----------------------------

  # Reverse the training dataset order before saving, this is useful for,
  # optimizing dataset packing process, when using packing_in_sequence
  # and sort_by_length desc order together
  reverse_train_dataset_before_save: False

#
# The dataset specific settings
# 
dataset:
  
  # ---
  # Text based dataset
  # ---

  - # Lambada training text
    # https://huggingface.co/datasets/lambada
    source: "lambada"
    name: "lambada-train"
    # 4k rechunk forced
    text_rechunk_force: True

  - # Enwiki training text
    # https://huggingface.co/datasets/teven/enwiki_100k
    source: "teven/enwiki_100k"
    name: "enwiki-train"
    # 4k rechunk forced
    min_token_size: 256
    text_rechunk_force: True

  # ---
  # Copa style
  # ---

  # Copa trained using
  # https://huggingface.co/datasets/pkavumba/balanced-copa

  - # Balanced copa, framed as choices
    source: "pkavumba/balanced-copa"
    name: "balanced-copa-choices"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["premise", "question", "choice1", "choice2", "label"]
    multi_column_prefix: ["### Premise:\n", "\n\n### Question:\nWhich choice was the", "1) ", "2) ", "\n### Answer:\n"]
    multi_column_suffix: ["", "?\n\n", "\n", "\n", ""]
    multi_column_train_mask: [false, false, false, false, true]
    multi_column_separator: ""
  
  - # Balanced copa, framed as options
    source: "pkavumba/balanced-copa"
    name: "balanced-copa-options"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["premise", "question", "choice1", "choice2", "label"]
    multi_column_prefix: ["Context: ", "\n\nQuestion: Which option was the", "1. ", "2. ", "\nAnswer: "]
    multi_column_suffix: ["", "?\n\n", "\n", "\n", ""]
    multi_column_train_mask: [false, false, false, false, true]
    multi_column_separator: ""
  
  # ---
  # Prompt completion / Q&A datasets
  # ---

  - # Question answer pair medical text
    # https://huggingface.co/datasets/BI55/MedText
    source: "BI55/MedText"   
    name: "MedText-QA"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["Prompt", "Completion"]
    multi_column_prefix: ["Question:\n", "Answer:\n"]
    multi_column_suffix: ["", ""]
    multi_column_train_mask: [false, true]
    multi_column_separator: "\n\n"
  
  - # Language translation prompt/completion
    # https://huggingface.co/datasets/kristaller486/ALMA-prompt-completion
    source: "kristaller486/ALMA-prompt-completion"
    name: "ALMA-prompt-completion"
    # 4k packing
    packing_enable: True
    # Prompt completion, nothing else else

  # ---
  # openbookqa
  # ---

  # openbookqa
  # https://huggingface.co/datasets/allenai/openbookqa

  - # Openbookqa training, with the json
    source: "allenai/openbookqa"
    name: "openbookqa-answer-choice"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["fact1", "question_stem", "choices", "answerKey"]
    multi_column_prefix: [">>> Premise:\n", "\n\nChoose the best option to complete the following:\n", "\n\nUsing the text options found in the following JSON:\n", "\n\n>>> Answer:\n"]
    multi_column_suffix: ["", "", "\n\nAnswer using only the label given in the json", ""]
    multi_column_train_mask: [false, false, false, true]
    multi_column_separator: ""
  
  # ---
  # Winogrande
  # ---

  # Copa trained using
  # https://huggingface.co/datasets/winogrande

  - # Balanced copa, framed as choices
    source: "winogrande"
    name: "winogrande-debiased-choices"
    # 4k packing
    packing_enable: True
    source_dataset_params:
      name: winogrande_debiased

    # Question / Answer pairings
    multi_column_keys: ["sentence", "option1", "option2", "answer"]
    multi_column_prefix: ["For the following sentence:\n", "\n1) ", "\n2) ", "\n\nAnswer:\n"]
    multi_column_suffix: ["\n\n Choose either 1 or 2, for which option is the best fit to replace _ in the sentence\n", "", "", ""]
    multi_column_train_mask: [false, false, false, true]
    multi_column_separator: ""
  
  - # Balanced copa, framed as choices
    source: "winogrande"
    name: "winogrande-l-choices"
    # 4k packing
    packing_enable: True
    source_dataset_params:
      name: winogrande_l 
    
    # Question / Answer pairings
    multi_column_keys: ["sentence", "option1", "option2", "answer"]
    multi_column_prefix: ["For the following statement: `", "\n1. ", "\n2. ", "\n\nAnswer:\n"]
    multi_column_suffix: ["`\n\n Choose 1 or 2, for which choice is the best fit to replace _ in the statement, answer only with the number given\n", "", "", ""]
    multi_column_train_mask: [false, false, false, true]
    multi_column_separator: ""
  
  # ---
  # logiqa
  # ---

  # logiqa
  # https://huggingface.co/datasets/lucasmccabe/logiqa
  # ( This has a pyarrow error somehow ?, probably cause its an array/list internally )

  # - # Openbookqa training, with the json
  #   source: "lucasmccabe/logiqa"
  #   name: "logiqa-options"
  #   # 4k packing
  #   packing_enable: True

  #   # Question / Answer pairings
  #   multi_column_keys: ["context", "query", "options", "correct_option"]
  #   multi_column_prefix: [">>> Context:\n", "\n\n>>> Query:\n", "\n\nAnswer with the array index position (starting from 0), for the most appropriate option for the given query: ", "\n\n>>> Answer:\n"]
  #   multi_column_suffix: ["", "", "", ""]
  #   multi_column_train_mask: [false, false, false, true]
  #   multi_column_separator: ""
  
  # ---
  # arc_easy
  # ---

  # arc_easy
  # https://huggingface.co/datasets/ibragim-bad/arc_easy

  - # Openbookqa training, with the json
    source: "ibragim-bad/arc_easy"
    name: "arc_easy-answer-choice"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["question", "choices", "answerKey"]
    multi_column_prefix: ["Question: ", "\n\nUsing the text options found in the following JSON:\n", "\n\nAnswer: "]
    multi_column_suffix: ["", "\n\nAnswer using only the corresponding label given in the json", ""]
    multi_column_train_mask: [false, false, true]
    multi_column_separator: ""
  
  # ---
  # arc_challenge
  # ---

  # arc_easy
  # https://huggingface.co/datasets/ibragim-bad/arc_challenge

  - # Openbookqa training, with the json
    source: "ibragim-bad/arc_challenge"
    name: "arc_challenge-answer-choice"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["choices", "question", "answerKey"]
    multi_column_prefix: ["Using the text found in the following:\n", "\n\nQuestion: ", "\n\nAnswer: "]
    multi_column_suffix: ["\n\nAnswer using only the respective label given", "", ""]
    multi_column_train_mask: [false, false, true]
    multi_column_separator: ""
  
  # ---
  # Piqa
  # ---

  # Copa trained using
  # https://huggingface.co/datasets/piqa

  - # Balanced copa, framed as choices
    source: "piqa"
    name: "piqa-choices"
    # 4k packing
    packing_enable: True

    # Question / Answer pairings
    multi_column_keys: ["goal", "sol1", "sol2", "label"]
    multi_column_prefix: ["# Goal: ", "\n\n0) ", "\n1) ", "\n\n# Answer: "]
    multi_column_suffix: ["", "", "", ""]
    multi_column_train_mask: [false, false, false, true]
    multi_column_separator: ""
  
  # ---
  # Instruct datasets
  # ---

  # - # Instruct, input, output format
  #   # https://huggingface.co/datasets/teknium/openhermes
  #   source: "Open-Orca/OpenOrca"
  #   name: "OpenOrca"

  #   multi_column_keys: ["system_prompt", "question", "response"]
  #   multi_column_prefix: ["Instruction:\n", "", ""]
  #   multi_column_suffix: ["\n\n", "\n\n", "\n\n"]
  #   multi_column_train_mask: [false, false, true]
  #   multi_column_separator: ""
    
  # - # Instruct, input, output format
  #   # https://huggingface.co/datasets/teknium/openhermes
  #   source: "teknium/openhermes"
  #   name: "openhermes-1-instruct"

  #   multi_column_keys: ["instruction", "input", "output"]
  #   multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
  #   multi_column_suffix: ["", "", ""]
  #   multi_column_train_mask: [false, false, true]
  #   multi_column_separator: "\n\n"
    
  # ---
  # Chat datasets
  # ---

  # - # Conversation format
  #   # https://huggingface.co/datasets/teknium/OpenHermes-2.5
  #   source: "LDJnr/Capybara"
  #   name: "Capybara-chat"
      
  #   # Conversation merging process=
  #   # ---
  #   conversation_format: 'iopairs'
  #   conversation_key: 'conversation'
  #   conversation_end_of_conversation: "\n\n>>> User: "

  #   # Iopairs specific config
  #   # ---
  #   conversation_input_key_prefix_map: {'input': "\n\n>>> User: ", 'output': "\n\n>>> Assistant: "}
  #   conversation_input_key_mask: {'input': false, 'output': True}
  #   conversation_sender_suffix: {'input': "", 'output': ""}

  # - # Conversation format
  #   # https://huggingface.co/datasets/teknium/OpenHermes-2.5
  #   source: "LDJnr/Pure-Dove"
  #   name: "Pure-Dove"
      
  #   # Conversation merging process=
  #   # ---
  #   conversation_format: 'iopairs'
  #   conversation_key: 'conversation'
  #   conversation_end_of_conversation: "\n\nUser: "

  #   # Iopairs specific config
  #   # ---
  #   conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
  #   conversation_input_key_mask: {'input': false, 'output': True}
  #   conversation_sender_suffix: {'input': "", 'output': ""}
 
  # ---
  # Other datasets
  # ---

  # - # Conversation format
  #   # https://huggingface.co/datasets/teknium/OpenHermes-2.5
  #   source: "teknium/OpenHermes-2.5"
  #   name: "openhermes-2-convo"
      
  #   # Conversation merging process
  #   # useful for merging full conversational datasets, into single documents
  #   # default is off, (or set conversation_key to [])
  #   # conversation_formatting supports "iopairs" or "sender" for now.
  #   # ---
  #   conversation_format: 'sender'
  #   conversation_key: 'conversations'
  #   conversation_end_of_conversation: "\n\nUser: "

  #   # Sender specific config
  #   # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
  #   # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
  #   # ---
  #   conversation_input_key_map: {'value': "\n\n{sender}: "}
  #   conversation_sender_key: 'from'
  #   conversation_sender_value_map: {'user': 'User', 'human': 'User', 'assistant': 'Assistant', 'gpt': 'Assistant', 'system': 'System'}
  #   conversation_sender_mask: {'user': false, 'human': false, 'assistant': True, 'gpt': True, 'system': false}
  #   conversation_sender_suffix: {'user': "", 'human': "", 'assistant': "", 'gpt': "", 'system': ""}

  # - # Instruct, input, output format
  #   # With the instruction format changed, to fix the formatting
  #   # https://huggingface.co/datasets/Darok/Lamini-instructions-to-french
  #   source: "Darok/Lamini-instructions-to-french"
  #   name: "Lamini-instructions-to-french"

  #   multi_column_keys: ["Input", "Response"]
  #   multi_column_prefix: ["### Instruction:\nPlease translate the next sentence into French\n\n### Input:\n", "### Output:\n"]
  #   multi_column_suffix: ["", ""]
  #   multi_column_train_mask: [false, true]
  #   multi_column_separator: "\n\n"
    
  # - # Long range instruction format
  #   # https://huggingface.co/datasets/THUDM/LongAlign-10k/
  #   source: "THUDM/LongAlign-10k"
  #   name: "LongAlign-10k"

  #   # Conversation merging process
  #   # useful for merging full conversational datasets, into single documents
  #   # default is off, (or set conversation_key to [])
  #   # conversation_formatting supports "iopairs" or "sender" for now.
  #   # ---
  #   conversation_format: 'sender'
  #   conversation_key: 'messages'
  #   conversation_end_of_conversation: "\n\nUser: "

  #   # Sender specific config
  #   # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
  #   # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
  #   # ---
  #   conversation_input_key_map: {'content': "\n\n{sender}: "}
  #   conversation_sender_key: 'role'
  #   conversation_sender_value_map: {'user': 'User', 'human': 'User', 'assistant': 'Assistant', 'gpt': 'Assistant', 'system': 'System'}
  #   conversation_sender_mask: {'user': false, 'human': false, 'assistant': True, 'gpt': True, 'system': false}
  #   conversation_sender_suffix: {'user': "", 'human': "", 'assistant': "", 'gpt': "", 'system': ""}

  ######################################################
  # Note: You can probably throw in enwiki if you want
  ######################################################
  # - # Text book is all you need
  #   # https://huggingface.co/datasets/TanvirOnHF/muse_textbooks
  #   source: "teven/enwiki_100k" 

  #   # Optional, provide a name for the dataset
  #   name: "enwiki_100k"

  #   # Minimum / Maximum token size of the dataset to use
  #   min_token_size: 1024
  #   max_token_size: -1

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: True
  #   packing_enable: False
  #   max_token_size: -1

  # - # SuperWiki (Multi-lingual)
  #   # https://huggingface.co/datasets/RyokoExtra/SuperWIKI-Cleaned
  #   source: "RyokoExtra/SuperWIKI-Cleaned" 

  #   # Optional, provide a name for the dataset
  #   name: "super_wiki"

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: True
  #   packing_enable: False
  #   max_token_size: -1

  #   source_dataset_split: lang25

  #   # Custom text column to use, useful for dataset with alternative training columns labels
  #   # This is checked before multi column merging, default is null (disabled)
  #   # If set this takes priority
  #   # eg: 'code'
  #   # ---
  #   custom_text_key: 'text'

  #   # All other settings found in default can be overriden here
  #   # ---
  #   # ...

  ######################################################
  # Note: We found the ML generated textbooks
  # too low in perplexity that it hurts the model
  # so we are using the original enwiki_100k & superwiki
  ######################################################
  # - # Text book is all you need
  #   # https://huggingface.co/datasets/TanvirOnHF/muse_textbooks
  #   source: "TanvirOnHF/muse_textbooks" 

  #   # Optional, provide a name for the dataset
  #   name: "muse_textbooks"

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: True
  #   packing_enable: False
  #   max_token_size: -1
  ######################################################

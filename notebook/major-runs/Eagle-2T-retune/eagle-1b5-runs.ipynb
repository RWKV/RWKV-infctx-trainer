{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Retune runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Eagle-Retune\"\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "LEARNING_RATE=\"5e-6\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# The model to start from\n",
    "MODEL_PATH=\"/workspace/main-models/RWKV-5-World-1B5-v2-20231025-ctx4096.pth\"\n",
    "MICROBATCH_SIZE=16\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 03:11:29,596] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-train-no-mask.yaml', '--model.load_model=/workspace/main-models/RWKV-5-World-1B5-v2-20231025-ctx4096.pth', '--model.lr_init=5e-6', '--model.lr_final=5e-6', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=/checkpoint/retune/1B5-No-Mask/', '--trainer.logger.init_args.name=Eagle-Retune - 1B5-No-Mask (deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=16', '--model.ctx_len=4096', '--trainer.devices=auto'], args=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-train-no-mask.yaml', '--model.load_model=/workspace/main-models/RWKV-5-World-1B5-v2-20231025-ctx4096.pth', '--model.lr_init=5e-6', '--model.lr_final=5e-6', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=/checkpoint/retune/1B5-No-Mask/', '--trainer.logger.init_args.name=Eagle-Retune - 1B5-No-Mask (deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=16', '--model.ctx_len=4096', '--trainer.devices=auto'].\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 4158660442\n",
      "Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       1024\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         16\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    1024\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "[rank: 0] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-03-07 03:11:45,444] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,519] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,551] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,556] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 03:11:45,598] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[rank: 5] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[rank: 1] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 4158660442\n",
      "[rank: 2] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 4158660442\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[rank: 5] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 4] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 7] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 6] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 3] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 1] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "[rank: 2] Seed set to 4158660442\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-no-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-no-mask/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240307_031217-l4bxp4sg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEagle-Retune - 1B5-No-Mask (deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4/runs/l4bxp4sg\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-06 (5e-06)\n",
      "    - lr_final: 5.000e-06 (5e-06)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05144309997558594 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10137343406677246 seconds\n",
      "Time to load fused_adam op: 0.10144162178039551 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10212945938110352 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10202336311340332 seconds\n",
      "Loading extension module fused_adam...\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10243558883666992 seconds\n",
      "Time to load fused_adam op: 0.10201096534729004 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10181879997253418 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 134 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 134 M \n",
      "--------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,311.018 Total estimated model params size (MB)\n",
      "Epoch 0:  14%|██▌                | 200/1461 [08:02<50:43,  0.41it/s, v_num=p4sg]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|▉| 1460/1461 [54:17<00:02,  0.45it/s, v_num=p4sg, train/tok=6.29e+/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "[rank5]:[2024-03-07 04:07:18,571] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 04:07:18,571] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank5]:[2024-03-07 04:07:18,571] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 04:07:18,572] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 04:07:18,572] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank5]:[2024-03-07 04:07:18,572] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 04:07:18,573] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 04:07:18,573] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank5]:[2024-03-07 04:07:18,573] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 04:07:19,273] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 04:07:19,273] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank6]:[2024-03-07 04:07:19,273] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank6]:[2024-03-07 04:07:19,274] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 04:07:19,483] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 04:07:19,483] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank2]:[2024-03-07 04:07:19,483] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank2]:[2024-03-07 04:07:19,484] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 04:07:19,527] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 04:07:19,527] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank7]:[2024-03-07 04:07:19,527] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank7]:[2024-03-07 04:07:19,528] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 04:07:19,587] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 04:07:19,587] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank4]:[2024-03-07 04:07:19,587] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank4]:[2024-03-07 04:07:19,588] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 04:07:19,694] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 04:07:19,694] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank0]:[2024-03-07 04:07:19,694] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank0]:[2024-03-07 04:07:19,695] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 04:07:20,296] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 04:07:20,296] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank3]:[2024-03-07 04:07:20,296] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank3]:[2024-03-07 04:07:20,297] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 04:07:20,469] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 04:07:20,469] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank1]:[2024-03-07 04:07:20,469] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank1]:[2024-03-07 04:07:20,470] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "Epoch 0: 100%|█| 1461/1461 [55:06<00:00,  0.44it/s, v_num=p4sg, train/tok=6.29e+\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:07<00:00,  0.13it/s]\u001b[A\n",
      "Epoch 1:   0%| | 0/1461 [00:00<?, ?it/s, v_num=p4sg, train/tok=6.29e+8, train/lo\u001b[A[rank6]:[2024-03-07 04:07:51,083] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 04:07:51,083] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank6]:[2024-03-07 04:07:51,083] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 04:07:51,100] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 04:07:51,100] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank7]:[2024-03-07 04:07:51,100] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 04:07:51,120] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 04:07:51,120] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank5]:[2024-03-07 04:07:51,120] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 04:07:51,121] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 04:07:51,121] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank1]:[2024-03-07 04:07:51,121] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 04:07:51,127] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 04:07:51,127] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank2]:[2024-03-07 04:07:51,127] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 04:07:51,150] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 04:07:51,150] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank4]:[2024-03-07 04:07:51,150] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 04:07:51,163] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 04:07:51,163] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank0]:[2024-03-07 04:07:51,163] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 04:07:51,178] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 04:07:51,178] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/ChannelMix.py:34)\n",
      "[rank3]:[2024-03-07 04:07:51,178] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "Epoch 1:   2%| | 29/1461 [01:17<1:03:59,  0.37it/s, v_num=p4sg, train/tok=6.29e+^C\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-26:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-3:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-27:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-31:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-6:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-25:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-22:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-23:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-28:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-20:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-22:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-24:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-14:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-30:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-21:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-26:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-22:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-17:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-28:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-10:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-20:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-24:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-14:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-16:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-15:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\r"
     ]
    }
   ],
   "source": [
    "# The 1B5 model\n",
    "EXPERIMENT_NAME=\"1B5-No-Mask\"\n",
    "\n",
    "# Perform the validation\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_TORCH_COMPILE=1 && \\\n",
    "    export RWKV_NO_CUDA=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/retune-train-no-mask.yaml\" \\\n",
    "        --model.load_model=\"{MODEL_PATH}\" \\\n",
    "        --model.lr_init={LEARNING_RATE} \\\n",
    "        --model.lr_final={LEARNING_RATE} \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"/checkpoint/retune/{EXPERIMENT_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - {EXPERIMENT_NAME} ({DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=1024 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 05:23:45,564] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '/checkpoint/retune/1B5-No-Mask/epoch=0-step=175.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.12.6\n",
      "Reconstructed fp32 state dict with 534 params 1577754624 elements\n",
      "Saving bf16 state dict to /workspace/main-models/R4-retune/R4-1B5-No-Mask.pth\n",
      "-rw-r--r-- 1 nobody root 3.0G Mar  7 05:24 /workspace/main-models/R4-retune/R4-1B5-No-Mask.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "EXPERIMENT_NAME=\"1B5-No-Mask\"\n",
    "CKPT_DIR=\"epoch=0-step=175.ckpt\"\n",
    "\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"/checkpoint/retune/{EXPERIMENT_NAME}/{CKPT_DIR}/\" \"/workspace/main-models/R4-retune/R4-{EXPERIMENT_NAME}.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"/workspace/main-models/R4-retune/R4-{EXPERIMENT_NAME}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 04:11:38,644] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-train-with-mask.yaml', '--model.load_model=/workspace/main-models/RWKV-5-World-1B5-v2-20231025-ctx4096.pth', '--model.lr_init=5e-6', '--model.lr_final=5e-6', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=/checkpoint/retune/1B5-With-Mask/', '--trainer.logger.init_args.name=Eagle-Retune - 1B5-With-Mask (deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=16', '--model.ctx_len=4096', '--trainer.devices=auto'], args=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-train-with-mask.yaml', '--model.load_model=/workspace/main-models/RWKV-5-World-1B5-v2-20231025-ctx4096.pth', '--model.lr_init=5e-6', '--model.lr_final=5e-6', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=/checkpoint/retune/1B5-With-Mask/', '--trainer.logger.init_args.name=Eagle-Retune - 1B5-With-Mask (deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=16', '--model.ctx_len=4096', '--trainer.devices=auto'].\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 984264868\n",
      "Seed set to 984264868\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       1024\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         16\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    1024\n",
      "\n",
      "[rank: 0] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-03-07 04:11:54,428] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,497] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,612] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 04:11:54,647] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.1+cu121'\n",
      "[rank: 4] Seed set to 984264868\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 984264868\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 984264868\n",
      "[rank: 6] Seed set to 984264868\n",
      "[rank: 1] Seed set to 984264868\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 984264868\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 984264868\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[rank: 4] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 2] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 3] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 1] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 6] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 7] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "[rank: 5] Seed set to 984264868\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Loading dataset from data_path:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      ">> Dataset load finished:  /datapath/eval-retune/pack-with-mask/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240307_041224-7uaksxd6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEagle-Retune - 1B5-With-Mask (deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4/runs/7uaksxd6\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /checkpoint/retune/1B5-With-Mask exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-06 (5e-06)\n",
      "    - lr_final: 5.000e-06 (5e-06)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0517115592956543 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10147452354431152 seconds\n",
      "Time to load fused_adam op: 0.10148239135742188 seconds\n",
      "Time to load fused_adam op: 0.10145282745361328 seconds\n",
      "Time to load fused_adam op: 0.10155653953552246 seconds\n",
      "Time to load fused_adam op: 0.10177063941955566 seconds\n",
      "Time to load fused_adam op: 0.1015777587890625 seconds\n",
      "Time to load fused_adam op: 0.10151982307434082 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 134 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 134 M \n",
      "--------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,311.018 Total estimated model params size (MB)\n",
      "Epoch 0:  14%|██▌                | 200/1461 [07:56<50:04,  0.42it/s, v_num=sxd6]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|▉| 1460/1461 [54:25<00:02,  0.45it/s, v_num=sxd6, train/tok=6.29e+/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "[rank3]:[2024-03-07 05:07:30,910] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 05:07:30,910] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank3]:[2024-03-07 05:07:30,910] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank3]:[2024-03-07 05:07:30,911] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 05:07:31,032] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 05:07:31,032] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank2]:[2024-03-07 05:07:31,032] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank2]:[2024-03-07 05:07:31,033] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 05:07:31,401] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 05:07:31,401] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank1]:[2024-03-07 05:07:31,401] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank1]:[2024-03-07 05:07:31,402] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank6]:[2024-03-07 05:07:31,477] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank6]:[2024-03-07 05:07:31,478] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank6]:[2024-03-07 05:07:31,478] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank6]:[2024-03-07 05:07:31,478] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 05:07:31,495] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 05:07:31,495] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank7]:[2024-03-07 05:07:31,495] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank7]:[2024-03-07 05:07:31,496] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 05:07:31,577] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 05:07:31,577] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank4]:[2024-03-07 05:07:31,577] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 05:07:31,578] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 05:07:31,578] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank4]:[2024-03-07 05:07:31,578] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank4]:[2024-03-07 05:07:31,579] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank4]:[2024-03-07 05:07:31,579] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank4]:[2024-03-07 05:07:31,579] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 05:07:31,950] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 05:07:31,950] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank0]:[2024-03-07 05:07:31,950] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 05:07:31,951] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 05:07:31,951] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank0]:[2024-03-07 05:07:31,951] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank0]:[2024-03-07 05:07:31,952] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank0]:[2024-03-07 05:07:31,952] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank0]:[2024-03-07 05:07:31,952] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 05:07:32,228] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 05:07:32,228] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/model.py:103)\n",
      "[rank5]:[2024-03-07 05:07:32,228] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 05:07:32,229] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 05:07:32,229] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:276)\n",
      "[rank5]:[2024-03-07 05:07:32,229] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "[rank5]:[2024-03-07 05:07:32,230] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "[rank5]:[2024-03-07 05:07:32,230] torch._dynamo.convert_frame: [WARNING]    function: '_forward_cuda' (/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5/src/module/TimeMix.py:284)\n",
      "[rank5]:[2024-03-07 05:07:32,230] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "Epoch 0: 100%|█| 1461/1461 [55:10<00:00,  0.44it/s, v_num=sxd6, train/tok=6.29e+\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:06<00:00,  0.15it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1461/1461 [55:17<00:00,  0.44it/s, v_num=sxd6, train/tok=6.29e+`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1461/1461 [55:17<00:00,  0.44it/s, v_num=sxd6, train/tok=6.29e+\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.043 MB of 0.043 MB uploaded (0.004 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          batchidx ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             epoch ▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       global_step ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      perf/cluster/kTokens_per_sec ▁▄▅▆▇▇▇▇▇▇█▇▇███████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/cluster/kTokens_per_sec_step ▄▅▄▄▆▆▆▅▇▂▆▅▄▄▇▄▆▇▅▄▄▄▆▅▆▅▅█▇▃▄▅▆▆▄▅▁▆▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        perf/cluster/kTokens_total ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        perf/kTokens_per_sec.gpu.0 ▁▄▅▆▇▇▇▇▇▇█▇▇███████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_per_sec_step.gpu.0 █▂▃█▇▇█▇▇▃▇█▇▇▇▇▇█▇█▁▇█▃▇▇▇█▇▇▇▇▇▆▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perf/kTokens_total.gpu.0 ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           substep ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/data_ctxlen ▄▆▄▄██▆▅█▃█▅▅▆█▅██▆▄▆▄▆▆█▆▆██▃▅▆█▇▆▆▁█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train/data_loss ▆▃▅▄▄▅▄▇█▅█▄▃▅▅▃▄▅▅▅▃▃▆▄▂▂▁▄▃▁▄▃▂▃▅▄▃▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learn_loss ▆▃▅▄▄▅▄▇█▅█▄▃▅▅▃▄▅▅▅▃▃▆▄▂▂▁▄▃▁▄▃▂▃▅▄▃▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/learn_tokens ▅▄▃▅▅▇▆▅▇▁█▄▅▄▇▅▆▅▇▃▃▄▅▅▆▄▄▇▇▃▂▂▇▅▄▅▂▇▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/loss █▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         train/tok ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            validation/data_ctxlen ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              validation/data_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          batchidx 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       global_step 149.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      perf/cluster/kTokens_per_sec 231.18736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/cluster/kTokens_per_sec_step 11.19994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        perf/cluster/kTokens_total 761867.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        perf/kTokens_per_sec.gpu.0 28.91554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_per_sec_step.gpu.0 1.41149\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          perf/kTokens_total.gpu.0 95289.894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           substep 11680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/data_ctxlen 2793.07031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train/data_loss 2.3418\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learn_loss 2.3418\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/learn_tokens 2743.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        train/loss 2.32079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         train/tok 628621312.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             trainer/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            validation/data_ctxlen 4096.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              validation/data_loss 1.2627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   validation/loss 1.2627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mEagle-Retune - 1B5-With-Mask (deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4/runs/7uaksxd6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-V5-Eagle-2T-R4/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjI1ODU5Ng==/version_details/v1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240307_041224-7uaksxd6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The 1B5 model\n",
    "EXPERIMENT_NAME=\"1B5-With-Mask\"\n",
    "\n",
    "# Perform the validation\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_TORCH_COMPILE=1 && \\\n",
    "    export RWKV_NO_CUDA=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/retune-train-with-mask.yaml\" \\\n",
    "        --model.load_model=\"{MODEL_PATH}\" \\\n",
    "        --model.lr_init={LEARNING_RATE} \\\n",
    "        --model.lr_final={LEARNING_RATE} \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"/checkpoint/retune/{EXPERIMENT_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - {EXPERIMENT_NAME} ({DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=1024 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 05:24:44,220] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '/checkpoint/retune/1B5-With-Mask/epoch=0-step=175.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.12.6\n",
      "Reconstructed fp32 state dict with 534 params 1577754624 elements\n",
      "Saving bf16 state dict to /workspace/main-models/R4-retune/R4-1B5-With-Mask.pth\n",
      "-rw-r--r-- 1 nobody root 3.0G Mar  7 05:25 /workspace/main-models/R4-retune/R4-1B5-With-Mask.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "EXPERIMENT_NAME=\"1B5-With-Mask\"\n",
    "CKPT_DIR=\"epoch=0-step=175.ckpt\"\n",
    "\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"/checkpoint/retune/{EXPERIMENT_NAME}/{CKPT_DIR}/\" \"/workspace/main-models/R4-retune/R4-{EXPERIMENT_NAME}.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"/workspace/main-models/R4-retune/R4-{EXPERIMENT_NAME}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME=\"1B5-No-Mask\"\n",
    "!cd \"/workspace/main-models/R4-retune/\" && \\\n",
    "    huggingface-cli upload rwkv-x-dev/eagle-7b-experiment \"./R4-{EXPERIMENT_NAME}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME=\"1B5-With-Mask\"\n",
    "!cd \"/workspace/main-models/R4-retune/\" && \\\n",
    "    huggingface-cli upload rwkv-x-dev/eagle-7b-experiment \"./R4-{EXPERIMENT_NAME}.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Eagle-Base Validation\"\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "\n",
    "EXPERIMENT_NAME=\"Baseline Validation\"\n",
    "LEARNING_RATE=\"5e-6\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# The model sizing\n",
    "MODEL_PATH=\"/workspace/main-models/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-data-build-with-mask.yaml\n",
      ">> Preparing dataset - index:  0  - name:  lambada-train\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (3/3 shards): 100%|█| 58333/58333 [00:05<00:00, 11324.73 exam\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 126.77 examples/s]\n",
      ">> Preparing dataset - index:  1  - name:  enwiki-train\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (7/7 shards): 100%|█| 124218/124218 [00:10<00:00, 11520.94 ex\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 137.22 examples/s]\n",
      ">> Preparing dataset - index:  2  - name:  balanced-copa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5084.24 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 40.54 examples/s]\n",
      ">> Preparing dataset - index:  3  - name:  balanced-copa-options\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5172.33 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 47.23 examples/s]\n",
      ">> Preparing dataset - index:  4  - name:  MedText-QA\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4383.39 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 48.29 examples/s]\n",
      ">> Preparing dataset - index:  5  - name:  ALMA-prompt-completion\n",
      "Saving the dataset (1/1 shards): 100%|█| 2655/2655 [00:00<00:00, 10088.94 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.14 examples/s]\n",
      ">> Preparing dataset - index:  6  - name:  openbookqa-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3677.74 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 47.12 examples/s]\n",
      ">> Preparing dataset - index:  7  - name:  winogrande-debiased-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3314.15 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.89 examples/s]\n",
      ">> Preparing dataset - index:  8  - name:  winogrande-l-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 320/320 [00:00<00:00, 6636.75 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 48.08 examples/s]\n",
      ">> Preparing dataset - index:  9  - name:  arc_easy-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4255.67 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.26 examples/s]\n",
      ">> Preparing dataset - index:  10  - name:  arc_challenge-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4559.27 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 42.50 examples/s]\n",
      ">> Preparing dataset - index:  11  - name:  piqa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 329/329 [00:00<00:00, 5538.62 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.80 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/eval-retune/pack-with-mask/\n",
      "Saving the dataset (10/10 shards): 100%|█| 186975/186975 [00:19<00:00, 9718.66 e\n",
      "Saving the dataset (1/1 shards): 100%|███| 12/12 [00:00<00:00, 78.55 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 186,975  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 12  samples\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|██████| 186975/186975 [00:26<00:00, 7129.50 examples/s]\n",
      "num_proc must be <= 12. Reducing num_proc to 12 for dataset of size 12.\n",
      "Map (num_proc=12): 100%|█████████████████| 12/12 [00:03<00:00,  3.94 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 761,896,534\n",
      ">> - Valid tokens : 752,459,956\n",
      ">> - Hidden tokens : 9,436,578\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 8,973\n",
      ">> - Valid tokens : 8,373\n",
      ">> - Hidden tokens : 600\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/retune-data-build-with-mask.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-data-build-no-mask.yaml\n",
      ">> Preparing dataset - index:  0  - name:  lambada-train\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (3/3 shards): 100%|█| 58333/58333 [00:05<00:00, 10980.23 exam\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 147.90 examples/s]\n",
      ">> Preparing dataset - index:  1  - name:  enwiki-train\n",
      "Map (num_proc=160): 100%|███| 1000000/1000000 [00:23<00:00, 41789.88 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1000000/1000000 [00:06<00:00, 145873.85 examples/\n",
      "Map (num_proc=160): 100%|█████| 472276/472276 [00:14<00:00, 32941.99 examples/s]\n",
      "Map (num_proc=160): 100%|██████| 124218/124218 [00:14<00:00, 8836.64 examples/s]\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (7/7 shards): 100%|█| 124218/124218 [00:18<00:00, 6767.74 exa\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 147.10 examples/s]\n",
      ">> Preparing dataset - index:  2  - name:  balanced-copa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5194.39 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.37 examples/s]\n",
      ">> Preparing dataset - index:  3  - name:  balanced-copa-options\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5217.77 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.50 examples/s]\n",
      ">> Preparing dataset - index:  4  - name:  MedText-QA\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4315.27 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.01 examples/s]\n",
      ">> Preparing dataset - index:  5  - name:  ALMA-prompt-completion\n",
      "Saving the dataset (1/1 shards): 100%|█| 2655/2655 [00:00<00:00, 10170.66 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.02 examples/s]\n",
      ">> Preparing dataset - index:  6  - name:  openbookqa-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3712.04 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.65 examples/s]\n",
      ">> Preparing dataset - index:  7  - name:  winogrande-debiased-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3443.72 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.23 examples/s]\n",
      ">> Preparing dataset - index:  8  - name:  winogrande-l-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 320/320 [00:00<00:00, 6878.55 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.79 examples/s]\n",
      ">> Preparing dataset - index:  9  - name:  arc_easy-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4188.07 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.60 examples/s]\n",
      ">> Preparing dataset - index:  10  - name:  arc_challenge-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4681.21 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.06 examples/s]\n",
      ">> Preparing dataset - index:  11  - name:  piqa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 329/329 [00:00<00:00, 5461.27 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.44 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/eval-retune/pack-no-mask/\n",
      "Saving the dataset (10/10 shards): 100%|█| 186975/186975 [00:18<00:00, 10043.09 \n",
      "Saving the dataset (1/1 shards): 100%|███| 12/12 [00:00<00:00, 78.26 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 186,975  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 12  samples\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|██████| 186975/186975 [00:27<00:00, 6744.73 examples/s]\n",
      "num_proc must be <= 12. Reducing num_proc to 12 for dataset of size 12.\n",
      "Map (num_proc=12): 100%|█████████████████| 12/12 [00:01<00:00,  6.01 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 761,896,534\n",
      ">> - Valid tokens : 754,446,928\n",
      ">> - Hidden tokens : 7,449,606\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 8,973\n",
      ">> - Valid tokens : 8,651\n",
      ">> - Hidden tokens : 322\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/retune-data-build-no-mask.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-data-build-no-mask-32k.yaml\n",
      ">> Preparing dataset - index:  0  - name:  lambada-train\n",
      "Map (num_proc=160): 100%|███████████| 2662/2662 [00:09<00:00, 294.89 examples/s]\n",
      "Filter (num_proc=160): 100%|████████| 2662/2662 [00:03<00:00, 723.14 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 2661/2661 [00:06<00:00, 436.35 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 7221/7221 [00:06<00:00, 1196.52 examples/s]\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (3/3 shards): 100%|█| 7221/7221 [00:08<00:00, 860.36 examples\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 122.92 examples/s]\n",
      ">> Preparing dataset - index:  1  - name:  enwiki-train\n",
      "Map (num_proc=160): 100%|███| 1000000/1000000 [00:21<00:00, 46232.78 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1000000/1000000 [00:07<00:00, 136515.60 examples/\n",
      "Map (num_proc=160): 100%|█████| 472276/472276 [00:14<00:00, 33143.01 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 15456/15456 [00:12<00:00, 1265.51 examples/s]\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (7/7 shards): 100%|█| 15456/15456 [00:18<00:00, 850.72 exampl\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 108.06 examples/s]\n",
      ">> Preparing dataset - index:  2  - name:  balanced-copa-choices\n",
      "Map (num_proc=160): 100%|███████████| 1000/1000 [00:01<00:00, 944.86 examples/s]\n",
      "Filter (num_proc=160): 100%|████████| 1000/1000 [00:01<00:00, 972.65 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:02<00:00, 414.00 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:01<00:00, 540.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4872.92 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.52 examples/s]\n",
      ">> Preparing dataset - index:  3  - name:  balanced-copa-options\n",
      "Map (num_proc=160): 100%|███████████| 1000/1000 [00:01<00:00, 935.64 examples/s]\n",
      "Filter (num_proc=160): 100%|████████| 1000/1000 [00:01<00:00, 968.80 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:02<00:00, 433.41 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:01<00:00, 528.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5012.50 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.30 examples/s]\n",
      ">> Preparing dataset - index:  4  - name:  MedText-QA\n",
      "Map (num_proc=160): 100%|██████████| 1412/1412 [00:01<00:00, 1370.04 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1412/1412 [00:00<00:00, 1415.80 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1411/1411 [00:02<00:00, 658.51 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1411/1411 [00:01<00:00, 766.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4222.99 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.36 examples/s]\n",
      ">> Preparing dataset - index:  5  - name:  ALMA-prompt-completion\n",
      "Map (num_proc=160): 100%|█████| 117404/117404 [00:01<00:00, 84427.68 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 117404/117404 [00:01<00:00, 103449.36 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 117403/117403 [00:02<00:00, 48025.94 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 117403/117403 [00:01<00:00, 61484.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 411/411 [00:00<00:00, 1180.88 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 42.24 examples/s]\n",
      ">> Preparing dataset - index:  6  - name:  openbookqa-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 4957/4957 [00:01<00:00, 4669.88 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 4957/4957 [00:00<00:00, 5115.47 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 4956/4956 [00:02<00:00, 2154.53 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 4956/4956 [00:01<00:00, 2685.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 1163.91 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.53 examples/s]\n",
      ">> Preparing dataset - index:  7  - name:  winogrande-debiased-choices\n",
      "Map (num_proc=160): 100%|██████████| 9248/9248 [00:01<00:00, 8858.91 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 9248/9248 [00:01<00:00, 9138.30 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 9247/9247 [00:02<00:00, 3961.32 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 9247/9247 [00:01<00:00, 4858.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3421.11 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 24.57 examples/s]\n",
      ">> Preparing dataset - index:  8  - name:  winogrande-l-choices\n",
      "Map (num_proc=160): 100%|███████| 10234/10234 [00:01<00:00, 10099.86 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 10234/10234 [00:00<00:00, 10713.05 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 10233/10233 [00:02<00:00, 4447.36 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 10233/10233 [00:01<00:00, 5208.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3172.52 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 38.39 examples/s]\n",
      ">> Preparing dataset - index:  9  - name:  arc_easy-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 2251/2251 [00:01<00:00, 2023.89 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 2251/2251 [00:00<00:00, 2316.50 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 2250/2250 [00:02<00:00, 997.01 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 2250/2250 [00:01<00:00, 1159.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4271.29 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.69 examples/s]\n",
      ">> Preparing dataset - index:  10  - name:  arc_challenge-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 1119/1119 [00:01<00:00, 1016.30 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1119/1119 [00:01<00:00, 1080.18 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1118/1118 [00:02<00:00, 458.11 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1118/1118 [00:01<00:00, 602.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4528.08 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.31 examples/s]\n",
      ">> Preparing dataset - index:  11  - name:  piqa-choices\n",
      "Map (num_proc=160): 100%|███████| 16113/16113 [00:01<00:00, 14850.29 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 16113/16113 [00:01<00:00, 15600.36 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 16112/16112 [00:02<00:00, 7100.73 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 16112/16112 [00:01<00:00, 8771.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 884.08 examples/s\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 42.64 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/eval-retune/pack-no-mask-32k/\n",
      "Saving the dataset (10/10 shards): 100%|█| 24528/24528 [00:19<00:00, 1269.30 exa\n",
      "Saving the dataset (1/1 shards): 100%|███| 12/12 [00:00<00:00, 77.37 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 24,528  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 12  samples\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|█████████| 24528/24528 [00:31<00:00, 767.98 examples/s]\n",
      "num_proc must be <= 12. Reducing num_proc to 12 for dataset of size 12.\n",
      "Map (num_proc=12): 100%|█████████████████| 12/12 [00:03<00:00,  3.63 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 757,250,147\n",
      ">> - Valid tokens : 749,800,541\n",
      ">> - Hidden tokens : 7,449,606\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 66,317\n",
      ">> - Valid tokens : 65,995\n",
      ">> - Hidden tokens : 322\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/retune-data-build-no-mask-32k.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune-data-build-no-mask-no-text.yaml\n",
      ">> Preparing dataset - index:  0  - name:  balanced-copa-choices\n",
      "Map (num_proc=160): 100%|███████████| 1000/1000 [00:01<00:00, 749.89 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1000/1000 [00:00<00:00, 1110.68 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:02<00:00, 452.39 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:01<00:00, 589.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5295.46 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.34 examples/s]\n",
      ">> Preparing dataset - index:  1  - name:  balanced-copa-options\n",
      "Map (num_proc=160): 100%|███████████| 1000/1000 [00:01<00:00, 949.99 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1000/1000 [00:00<00:00, 1047.64 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:02<00:00, 462.29 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 999/999 [00:01<00:00, 590.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5193.26 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.34 examples/s]\n",
      ">> Preparing dataset - index:  2  - name:  MedText-QA\n",
      "Map (num_proc=160): 100%|██████████| 1412/1412 [00:00<00:00, 1440.98 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1412/1412 [00:00<00:00, 1467.08 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1411/1411 [00:02<00:00, 684.53 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1411/1411 [00:01<00:00, 815.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4376.36 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.90 examples/s]\n",
      ">> Preparing dataset - index:  3  - name:  ALMA-prompt-completion\n",
      "Map (num_proc=160): 100%|█████| 117404/117404 [00:01<00:00, 96403.92 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 117404/117404 [00:01<00:00, 114128.31 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 117403/117403 [00:02<00:00, 55460.76 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 117403/117403 [00:01<00:00, 63525.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 2655/2655 [00:00<00:00, 10049.60 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 40.57 examples/s]\n",
      ">> Preparing dataset - index:  4  - name:  openbookqa-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 4957/4957 [00:00<00:00, 5020.61 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 4957/4957 [00:00<00:00, 5386.22 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 4956/4956 [00:02<00:00, 2307.23 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 4956/4956 [00:01<00:00, 2834.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 2867.67 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.20 examples/s]\n",
      ">> Preparing dataset - index:  5  - name:  winogrande-debiased-choices\n",
      "Map (num_proc=160): 100%|██████████| 9248/9248 [00:00<00:00, 9789.46 examples/s]\n",
      "Filter (num_proc=160): 100%|██████| 9248/9248 [00:00<00:00, 10080.53 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 9247/9247 [00:02<00:00, 4182.93 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 9247/9247 [00:01<00:00, 5169.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 2348.11 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.32 examples/s]\n",
      ">> Preparing dataset - index:  6  - name:  winogrande-l-choices\n",
      "Map (num_proc=160): 100%|███████| 10234/10234 [00:00<00:00, 10300.11 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 10234/10234 [00:00<00:00, 10846.91 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 10233/10233 [00:02<00:00, 4792.37 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 10233/10233 [00:01<00:00, 5768.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 320/320 [00:00<00:00, 5209.75 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.06 examples/s]\n",
      ">> Preparing dataset - index:  7  - name:  arc_easy-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 2251/2251 [00:00<00:00, 2320.81 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 2251/2251 [00:00<00:00, 2281.46 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 2250/2250 [00:02<00:00, 1034.23 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 2250/2250 [00:01<00:00, 1319.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3744.25 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.58 examples/s]\n",
      ">> Preparing dataset - index:  8  - name:  arc_challenge-answer-choice\n",
      "Map (num_proc=160): 100%|██████████| 1119/1119 [00:01<00:00, 1100.39 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 1119/1119 [00:00<00:00, 1173.84 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1118/1118 [00:02<00:00, 515.15 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1118/1118 [00:01<00:00, 626.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4203.71 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.67 examples/s]\n",
      ">> Preparing dataset - index:  9  - name:  piqa-choices\n",
      "Map (num_proc=160): 100%|███████| 16113/16113 [00:00<00:00, 16423.87 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 16113/16113 [00:00<00:00, 18913.24 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 16112/16112 [00:02<00:00, 7504.01 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 16112/16112 [00:01<00:00, 9118.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 329/329 [00:00<00:00, 4519.52 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 42.57 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/eval-retune/pack-no-mask-no-text/\n",
      "Saving the dataset (1/1 shards): 100%|█| 4424/4424 [00:00<00:00, 5063.70 example\n",
      "Saving the dataset (1/1 shards): 100%|███| 10/10 [00:00<00:00, 64.43 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 4,424  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 10  samples\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|███████████| 4424/4424 [00:22<00:00, 197.19 examples/s]\n",
      "num_proc must be <= 10. Reducing num_proc to 10 for dataset of size 10.\n",
      "Map (num_proc=10): 100%|█████████████████| 10/10 [00:02<00:00,  4.14 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 14,167,638\n",
      ">> - Valid tokens : 6,718,032\n",
      ">> - Hidden tokens : 7,449,606\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 781\n",
      ">> - Valid tokens : 459\n",
      ">> - Hidden tokens : 322\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/retune-data-build-no-mask-no-text.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-retune/retune5-data-build.yaml\n",
      ">> Preparing dataset - index:  0  - name:  lambada-train\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (3/3 shards): 100%|█| 58333/58333 [00:05<00:00, 10642.25 exam\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 96.40 examples/s]\n",
      ">> Preparing dataset - index:  1  - name:  enwiki-train\n",
      "Warning: packing_enable=true, with text rechunking (either auto, or forced) - packing_enable will be treated as false\n",
      "Saving the dataset (7/7 shards): 100%|█| 124218/124218 [00:11<00:00, 11074.44 ex\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 120.13 examples/s]\n",
      ">> Preparing dataset - index:  2  - name:  balanced-copa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4993.37 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 46.95 examples/s]\n",
      ">> Preparing dataset - index:  3  - name:  balanced-copa-options\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5056.73 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.42 examples/s]\n",
      ">> Preparing dataset - index:  4  - name:  MedText-QA\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3783.83 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.22 examples/s]\n",
      ">> Preparing dataset - index:  5  - name:  ALMA-prompt-completion\n",
      "Saving the dataset (1/1 shards): 100%|█| 2655/2655 [00:00<00:00, 10179.92 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 32.20 examples/s]\n",
      ">> Preparing dataset - index:  6  - name:  openbookqa-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3284.31 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.81 examples/s]\n",
      ">> Preparing dataset - index:  7  - name:  winogrande-debiased-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3196.16 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.81 examples/s]\n",
      ">> Preparing dataset - index:  8  - name:  winogrande-l-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 320/320 [00:00<00:00, 5800.67 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 41.56 examples/s]\n",
      ">> Preparing dataset - index:  9  - name:  logiqa-options\n",
      "Saving the dataset (1/1 shards): 100%|█| 480/480 [00:00<00:00, 5771.96 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 37.33 examples/s]\n",
      ">> Preparing dataset - index:  10  - name:  arc_easy-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4189.54 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 45.12 examples/s]\n",
      ">> Preparing dataset - index:  11  - name:  arc_challenge-answer-choice\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4675.01 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 44.19 examples/s]\n",
      ">> Preparing dataset - index:  12  - name:  piqa-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 329/329 [00:00<00:00, 5608.34 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.35 examples/s]\n",
      ">> Preparing dataset - index:  13  - name:  boolq-choices\n",
      "Saving the dataset (1/1 shards): 100%|█| 480/480 [00:00<00:00, 7115.70 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 41.09 examples/s]\n",
      ">> Preparing dataset - index:  14  - name:  mmlu-choices\n",
      "Map (num_proc=160): 100%|█████████████| 285/285 [00:01<00:00, 276.45 examples/s]\n",
      "Filter (num_proc=160): 100%|██████████| 285/285 [00:01<00:00, 274.18 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 284/284 [00:01<00:00, 145.31 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 284/284 [00:01<00:00, 176.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 5182.07 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█████| 1/1 [00:00<00:00, 43.49 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/eval-retune/pack-5-no-mask/\n",
      "Saving the dataset (10/10 shards): 100%|█| 188095/188095 [00:21<00:00, 8880.70 e\n",
      "Saving the dataset (1/1 shards): 100%|███| 15/15 [00:00<00:00, 72.47 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 188,095  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 15  samples\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|██████| 188095/188095 [00:30<00:00, 6192.88 examples/s]\n",
      "num_proc must be <= 15. Reducing num_proc to 15 for dataset of size 15.\n",
      "Map (num_proc=15): 100%|█████████████████| 15/15 [00:02<00:00,  5.65 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 764,990,976\n",
      ">> - Valid tokens : 757,024,325\n",
      ">> - Hidden tokens : 7,966,651\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 9,469\n",
      ">> - Valid tokens : 9,055\n",
      ">> - Hidden tokens : 414\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/retune-extd-data-build.yaml\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

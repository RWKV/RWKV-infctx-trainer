#
# Custom multiple datasource, built as a single datapack
#
datapack:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # If using relative path, this should be relative to the trainer script path
  data_path: /datapath/world/Eagle-x-Instruct/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Mixing mode to use, this is used to alternate between datasets
  #
  # - concat  : Keep It Simple Silly, lets just concat the datasets together
  # - shuffle : Dataset is mixed on a per sample level
  #
  # (@TODO: Advance operations)
  # - batch   : Meaning one dataset worth per batch, partial batches are discarded
  mixing_mode: "shuffle"

#
# Default settings used across all datasets in the datapack
# These settings can be overriden by the dataset specific settings
#
default:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # Datapath here is entirely optional, and only used if you intend to save each individual dataset
  # seperately (makes it easier to tweak and rebuild the datapack if it crash mid-way)
  #
  # The dataset index will be appended to the default value, if set
  # ---
  data_path: /datapath/world/Eagle-x-instruct-partial/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Additional source dataset params, used to grab subsets of the dataset
  # ---
  # source_dataset_params:
  #   language: en

  # Sort the dataset by length, useful to reduce gpu waiting time (also useful for RWKV long context coherence)
  # ---
  # sort_by_length: false
  # sort_asc: true # Sort in ascending order, true = shortest first, false = longest first

  # Limit the document count, to an offset/length limit
  # If an int value is used, it is interprated as document count
  # If a floating value (<1.0) is used, it is interprated as a percentage of the dataset
  # ---
  # dataset_offset: -1
  # dataset_length: -1

  # Use data_dir, if you are using source=text/json/etc
  # If using relative path, this should be relative to the trainer script path
  # source_data_dir: ../dataset-text/

  # After loading the dataset, split out test data used for validation, 
  # This process is skipped if the dataset includes a test split
  #
  # If given a float value, a percentage of the dataset is used (1.0 being 100%)
  # If given an int value, the number of data sample is used.
  #
  # Due to the limitaitons in the trainer process, there is always a minimum of 1 test sample
  test_split: 0.01 # Intentionally set to a low sample for test, cause the real eval is humans
  test_split_shuffle: true

  # Tokenizer to use, use either the inbuilt 'neox', or 'world' tokenizer
  # If using a custom tokenizer, provide the HF tokenizer name/path
  # ---
  tokenizer: world

  # Minimum / Maximum token size of the dataset to use
  # useful for filtering out small noisy data samples from large datasets
  # (eg. removal of small articles of less then 1024 tokens from wikipedia)
  #
  # This is ignored, if set to -1
  # ---
  min_token_size: -1
  max_token_size: 32768

  # Custom text column to use, useful for dataset with alternative training columns labels
  # This is checked before multi column merging, default is null (disabled)
  # eg: 'code'
  # ---
  # custom_text_key: 'code'

  # Multi Column merging process, default setting is used to support and merge
  # "instruction", "input", "output", datasets. To disable set multi_column_keys to []
  #
  # A minimum of 2 columns is required, with non empty data, for the merge to occur
  # If no match is found, this will fallback to the default prompt/completion or text column, 
  # or throw an error if the default fallback is not found
  #
  # IMPORTANT NOTE: as newlines are commonly used for multi_column_suffix, etc. 
  #                 you should use single quotes to ensure such values dun get escaped.
  #                 eg. multi_column_suffix: ['\n\n']
  #
  # See: https://github.com/RWKV/RWKV-infctx-trainer/issues/34
  # Need to use " or the new lines won't be tokenized properly
  # ---
  # multi_column_keys: ["instruction", "input", "output"]
  # multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
  # multi_column_suffix: ["\n\n", "\n\n", "\n\n"]
  # multi_column_train_mask: [true, false, true]
  # multi_column_separator: "\n\n"
  
  # Conversation merging process
  # useful for merging full conversational datasets, into single documents
  # default is off, (or set conversation_key to [])
  # conversation_formatting supports "iopairs" or "sender" for now.
  # ---
  # conversation_format: 'iopairs'
  # conversation_key: 'conversation'
  # conversation_end_of_conversation: "\n\nUser:"

  # Iopairs specific config
  # This means that every object in the conversation object is a pair of input output.
  # In future it will also support a format where one of the keys dictates the format style
  # if conversation_key is set to null, it will use the root object as the conversation object
  # ---
  # conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
  # conversation_input_key_mask: {'input': false, 'output': true}
  # conversation_sender_suffix: {'input': "", 'output': ""}

  # Sender specific config
  # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
  # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
  # conversation_input_key_map: {'message': "\n\n{sender}: ", 'context': ''}
  # conversation_sender_key: 'sender'
  # conversation_sender_value_map: {'user': 'User', 'assistant': 'Assistant', 'system': 'System'}
  # conversation_sender_mask: {'user': false, 'assistant': true, 'system': false}
  # conversation_sender_suffix: {'user': "", 'assistant': "", 'system': ""}

  # If processing prompt/completion jsonl pairs, the prompt is masked by default
  # use this flag to disable this default behaviour
  # ---
  # disable_prompt_completion_mask: false

  # ----------------------------
  # Rechunking support
  # ----------------------------

  # Rechunking of text dataset, this is done only when source is set as 'text'
  # and will merge the various sentencees, into larger chunks up to the target size
  #
  # Defaults to 2048
  #
  # This is ignored, if source is not set as text (unless text_rechunk_force)
  # This is ignored, if set to zero / -1
  # ---
  text_rechunk_size: 32768

  # Apply text rechunk to the dataset, even if its not a 'text' source
  # This is done only after dataset filtering, and if source is not 'text'
  # ---
  text_rechunk_force: False

  # Used to disable the automated text rechunkin for text files, if set as false
  # ---
  text_rechunk_auto: True

  # ----------------------------
  # Dataset packing support
  # Recommended to be used with mixed documents sized finetuning
  # For foundation model "from scratch", rechunking is typically used instead
  # ----------------------------

  # Boolean flag to enable / disable dataset packing
  packing_enable: True

  # Used to ensure all training samples wihin this batch size is the same length
  # Ideally this should align exactly with your real "batch size"
  #
  # Uses, `8 * (3 * 4 * 5 * 6 * 7) = 20160` for default, as it should align across
  # a large number of batch size combinations. This helps reduce the amount of
  # misaligned batches, and thus reduce the amount of wasted training time.
  #
  # This is tagged to datapack.batchsize, unless overriden here or on a dataset level
  # ---
  # packing_batchsize: 20160

  # Chunking size to align within each batch, this ideally should be equal to
  # the training context length used.
  packing_chunksize: 4096

  # Minimum size to pack up to, this should be a multiple of packing_chunksize
  # defautls to -1, which equals to packing_chunksize
  packing_min_ctx_len: 32768

  # Pack the data sequentially if possible, in accordance to the dataset sequence
  # this can be used together with sort_by_length, otherwise a shuffle will be done
  packing_in_sequence: False

  # ----------------------------
  # Specal use caes flags
  # ----------------------------

  # Reverse the training dataset order before saving, this is useful for,
  # optimizing dataset packing process, when using packing_in_sequence
  # and sort_by_length desc order together
  reverse_train_dataset_before_save: False

#
# The dataset specific settings
# 
dataset:
  
  # ---
  # Each dataset should be unique 
  # ---

  - # Question answer pair medical text
    # https://huggingface.co/datasets/BI55/MedText
    source: "BI55/MedText"   
    name: "MedText"

    # Question / Answer pairings
    multi_column_keys: ["Prompt", "Completion"]
    multi_column_prefix: ["Question:\n", "Answer:\n"]
    multi_column_suffix: ["", ""]
    multi_column_train_mask: [false, true]
    multi_column_separator: "\n\n"
  
  - # Language translation prompt/completion
    # https://huggingface.co/datasets/kristaller486/ALMA-prompt-completion
    source: "kristaller486/ALMA-prompt-completion"
    name: "ALMA-prompt-completion"
    # Prompt completion, nothing else else

  - # Instruct, input, output format
    # https://huggingface.co/datasets/teknium/openhermes
    source: "Open-Orca/OpenOrca"
    name: "OpenOrca"

    multi_column_keys: ["system_prompt", "question", "response"]
    multi_column_prefix: ["Instruction:\n", "", ""]
    multi_column_suffix: ["\n\n", "\n\n", "\n\n"]
    multi_column_train_mask: [false, false, true]
    multi_column_separator: ""
    
  - # Instruct, input, output format
    # https://huggingface.co/datasets/teknium/openhermes
    source: "teknium/openhermes"
    name: "openhermes-1-instruct"

    multi_column_keys: ["instruction", "input", "output"]
    multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
    multi_column_suffix: ["", "", ""]
    multi_column_train_mask: [false, false, true]
    multi_column_separator: "\n\n"
    
  - # Conversation format
    # https://huggingface.co/datasets/teknium/OpenHermes-2.5
    source: "teknium/OpenHermes-2.5"
    name: "openhermes-2-convo"
      
    # Conversation merging process
    # useful for merging full conversational datasets, into single documents
    # default is off, (or set conversation_key to [])
    # conversation_formatting supports "iopairs" or "sender" for now.
    # ---
    conversation_format: 'sender'
    conversation_key: 'conversations'
    conversation_end_of_conversation: "\n\nUser: "

    # Sender specific config
    # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
    # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
    # ---
    conversation_input_key_map: {'value': "\n\n{sender}: "}
    conversation_sender_key: 'from'
    conversation_sender_value_map: {'user': 'User', 'human': 'User', 'assistant': 'Assistant', 'gpt': 'Assistant', 'system': 'System'}
    conversation_sender_mask: {'user': false, 'human': false, 'assistant': true, 'gpt': true, 'system': false}
    conversation_sender_suffix: {'user': "", 'human': "", 'assistant': "", 'gpt': "", 'system': ""}

  - # Conversation format
    # https://huggingface.co/datasets/teknium/OpenHermes-2.5
    source: "LDJnr/Capybara"
    name: "Capybara-chat"
      
    # Conversation merging process=
    # ---
    conversation_format: 'iopairs'
    conversation_key: 'conversation'
    conversation_end_of_conversation: "\n\nUser: "

    # Iopairs specific config
    # ---
    conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
    conversation_input_key_mask: {'input': false, 'output': true}
    conversation_sender_suffix: {'input': "", 'output': ""}

  - # Conversation format
    # https://huggingface.co/datasets/teknium/OpenHermes-2.5
    source: "LDJnr/Pure-Dove"
    name: "Pure-Dove"
      
    # Conversation merging process=
    # ---
    conversation_format: 'iopairs'
    conversation_key: 'conversation'
    conversation_end_of_conversation: "\n\nUser: "

    # Iopairs specific config
    # ---
    conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
    conversation_input_key_mask: {'input': false, 'output': true}
    conversation_sender_suffix: {'input': "", 'output': ""}

  - # Instruct, input, output format
    # With the instruction format changed, to fix the formatting
    # https://huggingface.co/datasets/Darok/Lamini-instructions-to-french
    source: "Darok/Lamini-instructions-to-french"
    name: "Lamini-instructions-to-french"

    multi_column_keys: ["Input", "Response"]
    multi_column_prefix: ["### Instruction:\nPlease translate the next sentence into French\n\n### Input:\n", "### Output:\n"]
    multi_column_suffix: ["", ""]
    multi_column_train_mask: [false, true]
    multi_column_separator: "\n\n"
    
  - # Long range instruction format
    # https://huggingface.co/datasets/THUDM/LongAlign-10k/
    source: "THUDM/LongAlign-10k"
    name: "LongAlign-10k"

    # Conversation merging process
    # useful for merging full conversational datasets, into single documents
    # default is off, (or set conversation_key to [])
    # conversation_formatting supports "iopairs" or "sender" for now.
    # ---
    conversation_format: 'sender'
    conversation_key: 'messages'
    conversation_end_of_conversation: "\n\nUser: "

    # Sender specific config
    # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
    # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
    # ---
    conversation_input_key_map: {'content': "\n\n{sender}: "}
    conversation_sender_key: 'role'
    conversation_sender_value_map: {'user': 'User', 'human': 'User', 'assistant': 'Assistant', 'gpt': 'Assistant', 'system': 'System'}
    conversation_sender_mask: {'user': false, 'human': false, 'assistant': true, 'gpt': true, 'system': false}
    conversation_sender_suffix: {'user': "", 'human': "", 'assistant': "", 'gpt': "", 'system': ""}

  ######################################################
  # Note: You can probably throw in enwiki if you want
  ######################################################
  # - # Text book is all you need
  #   # https://huggingface.co/datasets/TanvirOnHF/muse_textbooks
  #   source: "teven/enwiki_100k" 

  #   # Optional, provide a name for the dataset
  #   name: "enwiki_100k"

  #   # Minimum / Maximum token size of the dataset to use
  #   min_token_size: 1024
  #   max_token_size: -1

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: true
  #   packing_enable: False
  #   max_token_size: -1

  # - # SuperWiki (Multi-lingual)
  #   # https://huggingface.co/datasets/RyokoExtra/SuperWIKI-Cleaned
  #   source: "RyokoExtra/SuperWIKI-Cleaned" 

  #   # Optional, provide a name for the dataset
  #   name: "super_wiki"

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: true
  #   packing_enable: False
  #   max_token_size: -1

  #   source_dataset_split: lang25

  #   # Custom text column to use, useful for dataset with alternative training columns labels
  #   # This is checked before multi column merging, default is null (disabled)
  #   # If set this takes priority
  #   # eg: 'code'
  #   # ---
  #   custom_text_key: 'text'

  #   # All other settings found in default can be overriden here
  #   # ---
  #   # ...

  ######################################################
  # Note: We found the ML generated textbooks
  # too low in perplexity that it hurts the model
  # so we are using the original enwiki_100k & superwiki
  ######################################################
  # - # Text book is all you need
  #   # https://huggingface.co/datasets/TanvirOnHF/muse_textbooks
  #   source: "TanvirOnHF/muse_textbooks" 

  #   # Optional, provide a name for the dataset
  #   name: "muse_textbooks"

  #   # Various over write settings
  #   # ---
  #   text_rechunk_size: 32768
  #   text_rechunk_force: true
  #   packing_enable: False
  #   max_token_size: -1
  ######################################################

      


    


{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory model from scratch)\n",
    "This attempts to build the memory model in stages, from scratch\n",
    "(Instead of previous attempts in doing enwiki foundation + gpt4all + etc)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    "\n",
    "## Insights from previous failed attempt at training from scratch / training from enwiki\n",
    "\n",
    "The following are insights that was found after multiple back and forth (3 weeks+ of experiments). This insights in partcular was derived from the failure to tune enwiki model, but success in doing so after limited gpt4all tuning, in limited capacity.\n",
    "\n",
    "What prevented training from scratch, was the lack of unmasked \"training instruction data\", as the original finetune went straight to fully masked instruction+input, and unmasked outputs. This worked on raven models, because they have already be pretrained for memory recall. But is unable to teach the model on its own otherwise.\n",
    "\n",
    "On the other hand if we were to train with the instruction unmasked, even with the \"input\" document masked (what needs to be memorized), the model end up thinking it should be somewhat RNG-ing a specific set of words, given an instruction. And fail to fully learn the memorization task. But at the very least, it will learn the \"instruction statement\" trigger. \n",
    "\n",
    "Finally, we used the original limited word list for the bulk of the training / validation. Instead of the full 400k+ larger word list. Another issue that was faced was that the model training / memory gets completely blindsided (bad loss) when it encounter a set of words it has never seen before. This is possible even after 100k samples, due to how large the word list was. Preventing / slowing down the training process.\n",
    "\n",
    "While the enwiki finetune, resolve these issues in stages, since our goal is to be able to replicate this experiment across multiple models config rapidly. The training from scratch model, is an attempt to remove the enwiki+gpt4all steps required in the process (hopefully) - by ensuring a good mix of all 3 of the above data - when training from scratch (instead of previous attempts at them one-by-one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch - Stage 1\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/segmented-word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 10000 samples - at ./dataset/limited-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/limited-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/full-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/limited-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/segmented-word-15-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/limited-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/full-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 10000 samples - at ./dataset/full-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 20000 samples - at ./dataset/segmented-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/segmented-word-10-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 20000 samples - at ./dataset/segmented-word-40-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 10000 samples - at ./dataset/limited-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 10000 samples - at ./dataset/full-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/full-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 20000 samples - at ./dataset/segmented-word-20-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ./dataset/limited-masked-word-100-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 10000 samples - at ./dataset/segmented-word-100-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 10000 samples - at ./dataset/limited-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 10000 samples - at ./dataset/full-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 10000 samples - at ./dataset/limited-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 10000 samples - at ./dataset/full-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/segmented-word-80-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 10000 samples - at ./dataset/segmented-word-200-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ./dataset/limited-masked-word-200-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# For the first stage of < 512 tokens, we form a strong bias for <= 100 words\n",
    "# to focus training with smaller datasets in the inital stages\n",
    "\n",
    "# Segmented JSONL, was designed to be only masking the input document. \n",
    "# While it failed to teach memorization properly, it teaches the model how to understand the instruction triggers.\n",
    "#\n",
    "# One theory, is that it loosely teach memory, but the model is confused thinking maybe these words should be randomly\n",
    "# generated when seeing a certain instruction. Which is not the case.\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-5-count.jsonl  5  20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-10-count.jsonl 10 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-15-count.jsonl 10 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-20-count.jsonl 20 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-40-count.jsonl 40 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-80-count.jsonl 80 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-100-count.jsonl 100 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-200-count.jsonl 200 10000 &\n",
    "\n",
    "# Prompt completion pairs, are fully masked instruction and input, with unmasked outputs\n",
    "# This is required to actually teach the model how to memorize the input, but on its own, \n",
    "# its unable to actually teach the model how to trigger this behavior (as the instruct is masked)\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-5-count.jsonl  5  10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-10-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-15-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-20-count.jsonl 20 10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-40-count.jsonl 40 10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-80-count.jsonl 80 10000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-200-count.jsonl 200 5000 &\n",
    "\n",
    "# Prompt completion pairs, with the full word list. Due to the size of the full word list, it \n",
    "# was possible to be stuck training the model just to recognize new words / tokens, and not perform the memorization task\n",
    "# this greatly slowed down the memorization learning process. As the model was constantly learning new words. \n",
    "# With 400k+ words total, even after 100k worth of document samples, new words can appear (due to how RNG works)\n",
    "#\n",
    "# We still include a mix of the data, in an attempt to reduce overtraining the model to only a fixed token set.\n",
    "# which was one of the weakness faced in the original training / benchmark (but technically not an issue for measuring memory)\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-5-count.jsonl  5  10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-10-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-15-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-20-count.jsonl 20 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-40-count.jsonl 40 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-80-count.jsonl 80 10000 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory-scratch\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo\n"
     ]
    }
   ],
   "source": [
    "# Configure your preferred options\n",
    "\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Low word count memory training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 38850.54it/s]\n",
      "Downloading and preparing dataset json/default to /home/picocreator/.cache/huggingface/datasets/json/default-9d3acc0155290f3e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 4686.37it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 122.88it/s]\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/json/default-9d3acc0155290f3e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.98it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-scratch-stage-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-10 11:53:45,272] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3901155180\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3901155180\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_115347-c8mf73xy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEcho-A-1B5 - Mem-Train-Stage-1 (bs=64, train-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/c8mf73xy\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 307.48it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4d0d02db630cf0a0_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c6cc97c105981bb4_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 3901155180                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-10 11:54:02,032] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 11:54:02,033] [WARNING] [deepspeed.py:638:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3330750465393066 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%| | 63/133000 [00:51<30:21:44,  1.22it/s, v_num=73xy, train/loss=10/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1828: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "Epoch 0:   9%| | 12572/133000 [2:28:59<23:47:08,  1.41it/s, v_num=73xy, train/lo^C\n"
     ]
    }
   ],
   "source": [
    "# Start the memory model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_TORCH_COMPILE=0 && \\\n",
    "    export RWKV_JIT_ON=1 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Scratch-Stage-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    && python export_checkpoint.py \"../checkpoint/Echo-A-1B5-scratch-stage-1/last.ckpt\" \"../model/Echo-A-1B5-Scratch-Stage-1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Echo-A-1B5-Scratch-Stage-1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && python3 dragon_test.py \"../model/Echo-A-1B5-Scratch-Stage-1.pth\" \"cuda fp32\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 Baseline C\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the same memory training process as TokenShift, but using the v5 code\n",
    "\n",
    "See `./notes.md` for how the init model was initilaized.\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "# ninja-build is required for the new trainer\n",
    "sudo apt-get install ninja-build\n",
    "\n",
    "# Update conda & its package listings\n",
    "# Conda install script can be found here : \n",
    "# https://docs.anaconda.com/free/anaconda/install/linux/#installation\n",
    "conda update conda\n",
    "\n",
    "# Virtual env, with python 3.10\n",
    "# python 3.11 have issues with torch.compile / h100s\n",
    "# and if you want to use 3.11, you will need to do a nightly build install\n",
    "conda create -n rwkv-infctx python=3.11 pip\n",
    "conda activate rwkv-infctx\n",
    "\n",
    "# Install pytorch (>=2.0.1)\n",
    "conda install -y pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Verify your pytorch version \n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "\n",
    "# We use python -m pip, instead of pip directly, as it resolve issues with venv not loading the right pip\n",
    "python -m pip install datasets transformers \n",
    "python -m pip install lightning==2.0.4 deepspeed==0.9.5\n",
    "python -m pip install ninja numexpr jsonargparse 'jsonargparse[signatures]'\n",
    "python -m pip install lm-dataformat ftfy sentencepiece tokenizers wandb\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-neox-v5-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# Init the model\n",
    "!cd ../../../../RWKV-v5 && python3 ./init_model.py --n_layer 24 --n_embd 2048 --vocab_size neox --skip-if-exists ../model/L24-D2048-neox-v5-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"BaseV5-R2-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 85.20it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/BaseV5-C-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-enwiki.yaml', '--trainer.logger.init_args.name=BaseV5-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-enwiki.yaml', '--trainer.logger.init_args.name=BaseV5-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3835032983\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3835032983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230806_043020-88gaxcg5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseV5-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/88gaxcg5\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 84.03it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "Saving the dataset (0/5 shards):   0%|         | 0/81487 [00:00<?, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):   5%| | 4000/81487 [00:00<00:02, 27322.94 exampSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):  17%|â–| 14000/81487 [00:00<00:01, 37882.48 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/5 shards):  20%|â–| 16298/81487 [00:00<00:01, 37882.48 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/5 shards):  55%|â–Œ| 44596/81487 [00:01<00:00, 38015.76 exam[rank: 7] Global seed set to 3835032983\n",
      "[rank: 4] Global seed set to 3835032983\n",
      "[rank: 6] Global seed set to 3835032983\n",
      "[rank: 2] Global seed set to 3835032983\n",
      "[rank: 5] Global seed set to 3835032983\n",
      "[rank: 1] Global seed set to 3835032983\n",
      "[rank: 3] Global seed set to 3835032983\n",
      "[rank: 0] Global seed set to 3835032983                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-06 04:30:37,651] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-06 04:30:53,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-06 04:30:53,760] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-06 04:30:53,814] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-06 04:30:53,837] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3835032983\n",
      "[rank: 3] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-06 04:30:53,839] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-06 04:30:53,840] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3835032983\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-06 04:30:53,847] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 20.32833433151245 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 20.421996355056763 seconds\n",
      "Time to load fused_adam op: 20.422023057937622 seconds\n",
      "Time to load fused_adam op: 20.42237687110901 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 20.423731803894043 seconds\n",
      "Time to load fused_adam op: 20.4226553440094 seconds\n",
      "Time to load fused_adam op: 20.422996520996094 seconds\n",
      "Time to load fused_adam op: 20.42310404777527 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.69725775718689 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.713019371032715 seconds\n",
      "Time to load utils op: 10.71351408958435 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.713855743408203 seconds\n",
      "Time to load utils op: 10.712512254714966 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.713116884231567 seconds\n",
      "Time to load utils op: 10.712001085281372 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.713200569152832 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002727508544921875 seconds\n",
      "Time to load utils op: 0.000270843505859375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002713203430175781 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023031234741210938 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00022649765014648438 seconds\n",
      "Time to load utils op: 0.00022649765014648438 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00024390220642089844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005331039428710938 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 1000/10186 [41:26<6:20:38,  2.49s/it, v_num=xcg5, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:05:02<00:00,  2.50s/it, v_num=xcg5, train/loss=3\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–Ž                  | 1/52 [00:00<00:27,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                  | 2/52 [00:01<00:25,  1.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 3/52 [00:01<00:24,  1.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 4/52 [00:02<00:24,  1.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–Š                 | 5/52 [00:02<00:23,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 6/52 [00:03<00:23,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–Œ                | 7/52 [00:03<00:22,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 8/52 [00:03<00:21,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–ˆâ–Ž               | 9/52 [00:04<00:21,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–              | 10/52 [00:04<00:20,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–Š              | 11/52 [00:05<00:20,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–             | 12/52 [00:05<00:19,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 13/52 [00:06<00:19,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 14/52 [00:06<00:18,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 15/52 [00:07<00:18,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 16/52 [00:07<00:17,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 17/52 [00:08<00:17,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 18/52 [00:08<00:16,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 19/52 [00:09<00:16,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 20/52 [00:09<00:15,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 21/52 [00:10<00:15,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 22/52 [00:10<00:14,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 23/52 [00:11<00:14,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 24/52 [00:11<00:13,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 25/52 [00:12<00:13,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 26/52 [00:12<00:12,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 27/52 [00:13<00:12,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 28/52 [00:13<00:11,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 29/52 [00:14<00:11,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 30/52 [00:14<00:10,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 31/52 [00:15<00:10,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 32/52 [00:15<00:09,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 33/52 [00:16<00:09,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 34/52 [00:16<00:08,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 35/52 [00:17<00:08,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 36/52 [00:17<00:07,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 37/52 [00:18<00:07,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 38/52 [00:18<00:06,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/52 [00:19<00:06,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 40/52 [00:19<00:05,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/52 [00:20<00:05,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/52 [00:20<00:04,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 43/52 [00:21<00:04,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/52 [00:21<00:03,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/52 [00:22<00:03,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 46/52 [00:22<00:02,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 47/52 [00:23<00:02,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 48/52 [00:23<00:01,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/52 [00:24<00:01,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/52 [00:24<00:00,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 51/52 [00:25<00:00,  2.04it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:05:34<00:00,  2.51s/it, v_num=xcg5, train/loss=3\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:05:34<00:00,  2.51s/it, v_num=xcg5, train/loss=3\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:05:45<00:00,  2.51s/it, v_num=xcg5, train/loss=3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–†â–‡â–†â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.97378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mBaseV5-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/88gaxcg5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230806_043020-88gaxcg5/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/BaseV5-C-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/BaseV5-C-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/BaseV5-C-Stage1.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  6 11:38 ../model/BaseV5-C-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/BaseV5-C-enwiki/last.ckpt\" \"../model/BaseV5-C-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/BaseV5-C-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "Middle Ages\n",
      "\n",
      "The Magyars lived in the area between 1120 and 692. However, in later times, however, the Alsatian cult of Ptolemais, a native of Jupiter, has given him the powers of sin and the promise of divine revelation. He is as benevolent and virtuous. The rich princess, which he had destroyed, is a corruption of the slave-cat. This place was originally in a small town called Tali Shary (also called Karu Ghalib) is one of the seven most influential civilizations of the modern world, with the period leading to the 13th century. According to the archeological record, the city of Bingen was known as GÄ™sadÃ¤ki. The town was named after Saint Croix, who ruled over the East Slavs, and was called The Persian Language, the language of the Slavs.\n",
      "\n",
      "The war against the Peloponnese took place\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/BaseV5-C-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/BaseV5-C-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.79k/7.79k [00:00<00:00, 18.0MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/7.80M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 53.2k/7.80M [00:00<00:26, 294kB/s]\u001b[A\n",
      "Downloading data:   4%|â–Š                    | 297k/7.80M [00:00<00:06, 1.08MB/s]\u001b[A\n",
      "Downloading data:   8%|â–ˆâ–‹                   | 645k/7.80M [00:00<00:04, 1.70MB/s]\u001b[A\n",
      "Downloading data:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 1.78M/7.80M [00:00<00:01, 4.68MB/s]\u001b[A\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 3.98M/7.80M [00:00<00:00, 9.93MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.80M/7.80M [00:00<00:00, 10.3MB/s]\u001b[A\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2236.96it/s]\n",
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 746.98it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/BaseV5-C-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-instruct.yaml', '--trainer.logger.init_args.name=BaseV5-C - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-instruct.yaml', '--trainer.logger.init_args.name=BaseV5-C - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2920952832\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2920952832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230806_113914-ffzxdgc0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseV5-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ffzxdgc0\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 748.98it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "[rank: 0] Global seed set to 2920952832                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-06 11:39:29,305] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 2920952832\n",
      "[rank: 1] Global seed set to 2920952832\n",
      "[rank: 7] Global seed set to 2920952832\n",
      "[rank: 3] Global seed set to 2920952832\n",
      "[rank: 5] Global seed set to 2920952832\n",
      "[rank: 6] Global seed set to 2920952832\n",
      "[rank: 4] Global seed set to 2920952832\n",
      "[rank: 3] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-06 11:39:48,181] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-06 11:39:48,917] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-06 11:39:48,954] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-06 11:39:49,068] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[rank: 7] Global seed set to 2920952832\n",
      "[2023-08-06 11:39:49,080] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-06 11:39:49,081] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2920952832\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-06 11:39:49,083] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06715822219848633 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1015310287475586 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10122084617614746 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1014559268951416 seconds\n",
      "Time to load fused_adam op: 0.10149192810058594 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10142374038696289 seconds\n",
      "Time to load fused_adam op: 0.10160398483276367 seconds\n",
      "Time to load fused_adam op: 0.10207343101501465 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0684347152709961 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10201859474182129 seconds\n",
      "Time to load utils op: 0.10170221328735352 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10190701484680176 seconds\n",
      "Time to load utils op: 0.10109972953796387 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10163164138793945 seconds\n",
      "Time to load utils op: 0.1020817756652832 seconds\n",
      "Time to load utils op: 0.10191130638122559 seconds\n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189388288, False), (96, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00026226043701171875 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002639293670654297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002543926239013672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003437995910644531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00021719932556152344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003342628479003906 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003075599670410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005059242248535156 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|â–Œ| 1000/1867 [18:14<15:49,  1.09s/it, v_num=dgc0, train/loss=5.120/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [33:43<00:00,  1.08s/it, v_num=dgc0, train/loss=2.480\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 1/10 [00:00<00:03,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–Š               | 2/10 [00:00<00:02,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3/10 [00:00<00:01,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 4/10 [00:00<00:01,  4.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 5/10 [00:01<00:01,  4.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 6/10 [00:01<00:00,  4.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7/10 [00:01<00:00,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/10 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9/10 [00:01<00:00,  4.80it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [33:51<00:00,  1.09s/it, v_num=dgc0, train/loss=2.480\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [33:51<00:00,  1.09s/it, v_num=dgc0, train/loss=2.480\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [34:03<00:00,  1.09s/it, v_num=dgc0, train/loss=2.480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–„â–ˆâ–â–â–â–â–â–‚â–ƒâ–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–†â–„â–‡â–‡â–…â–ˆâ–…â–‡â–‡â–â–‡â–ƒâ–ƒâ–†â–…â–ƒâ–…â–„â–ƒâ–…â–â–‚â–ƒâ–†â–†â–‡â–ƒâ–…â–„â–ƒâ–…â–ƒâ–„â–…â–…â–†â–…â–†â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.45312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mBaseV5-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ffzxdgc0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230806_113914-ffzxdgc0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/BaseV5-C-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/BaseV5-C-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/BaseV5-C-Stage2.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  6 12:14 ../model/BaseV5-C-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/BaseV5-C-instruct/last.ckpt\" \"../model/BaseV5-C-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/BaseV5-C-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. However, the common people, known as humans and, are naturally humans. When the humans are, they are very kind. They are all types of insects. Polar bears are found in California, United States, and Canada. Its popular breakfast foods are vegetables, dairy, pasta, cheese, and beef.  The typical restaurant and rice are fruits and vegetables. Dogs are all found in Europe, Asia, North America, and North America. The day's main event is the iconic Hindi television series created by Tessa Thompson, released in 1998. It stars the voices of Katarina Whitoff, Adam Scott, Linkin Park, Marcissa, Manchester United, England.# Answer:\\nThe three men's national team is called the Ballet and the Rivalry, the PGA, and the Eurodance. Each are given to different cast members and one of them is a staff who might not be with the team. The only one is the player's home and level.\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/BaseV5-C-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/BaseV5-C-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 Wavenet C - memory finetune\n",
    "\n",
    "This continues off from `./WaveV5-C-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "This is done generally across the following stages\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set.\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens.\n",
    "- Tune 3: Mid ctx size (1024), stage 2, scaled up to 1024 context sizes.\n",
    "- Tune 4: Mid ctx size (4096), stage 3, scaled up to 4096 context sizes.\n",
    "\n",
    "In all cases, the input tokens is always masked. And we intentionally use the limited word set for memory training, which matches the same wordset used in the original memory evaluation of raven pretrained models. This is intentional to serve as both consistent comparision between experiments, and resonable training time.\n",
    "\n",
    "One of the issue faced previously with an excessive large word set, is that the model would be required to see \"new words\" atleast a few time before being able to train the memory process. This drastically slowed down the process as the large word list meant the model was constantly spending time learning new words (instead of memory training).\n",
    "\n",
    "If we want to increase the number / type of words the model can handle for memory training, that can be done later as a stage 4 memory tune if needed. But that exceeds the current requirements for the memory experiment process.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File â€˜WaveV5-C-Stage2.pthâ€™ already there; not retrieving.\n",
      "\n",
      "-rw-r--r-- 1 root root 5.7G Aug  8 05:12 ../../../../model/WaveV5-C-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Init required dirs\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# Download the Stage2.pth file\n",
    "!cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/v5-Wave/WaveV5-C-Stage2.pth\n",
    "!ls -alh ../../../../model/WaveV5-C-Stage2.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5wavenet\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5wavenet\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"WaveV5-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ../dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ../dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ../dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ../dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ../dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ../dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x  2 root root  330 Aug  8 07:32 .\n",
      "drwxr-xr-x 17 root root  323 Aug  8 07:24 ..\n",
      "-rw-r--r--  1 root root 612K Aug  8 07:32 word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Aug  8 07:32 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 718K Aug  8 07:32 word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 838K Aug  8 07:32 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root 853K Aug  8 07:32 word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Aug  8 07:32 word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 971K Aug  8 07:32 word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug  8 07:32 word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 969K Aug  8 07:32 word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Aug  8 07:32 word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug  8 07:32 word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug  8 07:32 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-5-count.jsonl  5  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-10-count.jsonl 10 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-15-count.jsonl 15 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-20-count.jsonl 20 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-25-count.jsonl 25 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-40-count.jsonl 40 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-50-count.jsonl 50 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-60-count.jsonl 80 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-100-count.jsonl 100 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2878.73it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 206.79it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 204.79it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/WaveV5-C-mem-finetune-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-1.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=512'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-1.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=512'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2175051473\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2175051473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230808_073240-yedr2hnk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWaveV5-C - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/yedr2hnk\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 64\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 303.36it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19db7b7b83c94358_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6eb0d0be9b1e255b_*_of_00032.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f075c372e2cca1f0.arrow and /root/.cache/huggingface/datasets/json/default-1f241d8a9c98d7ad/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bf9c8e932e4ebe73.arrow\n",
      "[rank: 0] Global seed set to 2175051473                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-08 07:32:56,767] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 2175051473\n",
      "[rank: 1] Global seed set to 2175051473\n",
      "[rank: 2] Global seed set to 2175051473\n",
      "[rank: 2] Global seed set to 2175051473\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-08 07:33:18,189] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2175051473\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-08 07:33:18,253] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2175051473\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-08 07:33:18,275] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.765018224716187 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.834813833236694 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.836856365203857 seconds\n",
      "Time to load fused_adam op: 26.836355924606323 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.433202981948853 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.520097017288208 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.519661903381348 seconds\n",
      "Time to load utils op: 14.518839120864868 seconds\n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030684471130371094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003075599670410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00024271011352539062 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005447864532470703 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|â–| 1600/8742 [06:04<27:08,  4.39it/s, v_num=2hnk, train/loss=4.030/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 8742/8742 [33:45<00:00,  4.32it/s, v_num=2hnk, train/loss=0.922\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–ˆâ–                 | 1/9 [00:00<00:01,  4.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–               | 2/9 [00:00<00:01,  4.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3/9 [00:00<00:01,  4.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 4/9 [00:00<00:01,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 5/9 [00:01<00:00,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 6/9 [00:01<00:00,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 7/9 [00:01<00:00,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 8/9 [00:01<00:00,  4.62it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 8742/8742 [33:49<00:00,  4.31it/s, v_num=2hnk, train/loss=0.922\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 8742/8742 [33:49<00:00,  4.31it/s, v_num=2hnk, train/loss=0.922\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 8742/8742 [34:02<00:00,  4.28it/s, v_num=2hnk, train/loss=0.922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–„â–‚â–â–â–â–‚â–â–‚â–â–â–â–…â–…â–‚â–ƒâ–ƒâ–â–‚â–‚â–„â–„â–„â–â–ƒâ–„â–„â–‚â–â–‚â–…â–‚â–ƒâ–‚â–â–ˆâ–‚â–‚â–â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–†â–„â–ƒâ–„â–„â–„â–ƒâ–‚â–…â–…â–â–ƒâ–„â–…â–‚â–ƒâ–‚â–…â–„â–…â–â–„â–…â–ƒâ–â–â–‚â–ƒâ–â–â–‚â–â–‚â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.85938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.51912\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mWaveV5-C - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/yedr2hnk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_073240-yedr2hnk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/WaveV5-C-mem-finetune-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/WaveV5-C-Tune1.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  8 08:08 ../model/WaveV5-C-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/WaveV5-C-mem-finetune-1/last.ckpt\" \\\n",
    "        \"../model/WaveV5-C-Tune1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/WaveV5-C-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a memory eval\n",
    "# #\n",
    "# # Note that the expected performance \"is not that great\", as the model seems to be only loosely\n",
    "# # learning the memorization task, and the instruction propmt. And is seem to be acting more\n",
    "# # like an RNG based on the instruct. (Instead of the actual memorization task)\n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/WaveV5-C-Tune1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 3557 samples (20 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 3197 samples (30 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 678 samples (50 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 5224 samples (20 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 1326 samples (50 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 1776 samples (50 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 2629 samples (50 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 79M\n",
      "drwxr-xr-x  2 root root  4.0K Aug  8 08:08 .\n",
      "drwxr-xr-x 17 root root   323 Aug  8 07:24 ..\n",
      "-rw-r--r--  1 root root  979K Aug  8 08:08 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug  8 08:08 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug  8 08:08 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug  8 08:08 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug  8 08:08 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug  8 08:08 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug  8 08:08 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug  8 08:08 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  728K Aug  8 08:08 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug  8 08:08 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug  8 08:08 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug  8 08:08 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug  8 08:08 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug  8 08:08 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug  8 08:08 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug  8 08:08 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug  8 08:08 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug  8 08:08 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug  8 08:08 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 1022K Aug  8 08:08 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug  8 08:08 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  854K Aug  8 08:08 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug  8 08:08 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug  8 08:08 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug  8 08:08 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug  8 08:08 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug  8 08:08 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  599K Aug  8 08:08 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root   10M Aug  8 08:08 word-200-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-100-count.jsonl 100 5000 &\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 20536.01it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1591.16it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 109.81it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 104.41it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/WaveV5-C-mem-finetune-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-2.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=512'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-2.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=512'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3804140867\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3804140867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230808_080913-9nq4hot0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWaveV5-C - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/9nq4hot0\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 64\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 95399.86it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 103.74it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-07088bc907bf0919_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b7e6f03d8075128a_*_of_00032.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a828ae7de0b00e49.arrow and /root/.cache/huggingface/datasets/json/default-eeaa17c0390edd62/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7fdb807cb3b3361.arrow\n",
      "[rank: 0] Global seed set to 3804140867                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-08 08:09:31,011] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 3804140867\n",
      "[rank: 2] Global seed set to 3804140867\n",
      "[rank: 1] Global seed set to 3804140867\n",
      "[rank: 3] Global seed set to 3804140867\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-08 08:09:51,470] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3804140867\n",
      "[rank: 2] Global seed set to 3804140867\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-08 08:09:51,553] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-08 08:09:51,554] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07755303382873535 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10101819038391113 seconds\n",
      "Time to load fused_adam op: 0.1014862060546875 seconds\n",
      "Time to load fused_adam op: 0.10166263580322266 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0831294059753418 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10218071937561035 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10112285614013672 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10291218757629395 seconds\n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00029087066650390625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0003020763397216797 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003001689910888672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005540847778320312 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 1600/32065 [06:13<1:58:37,  4.28it/s, v_num=hot0, train/loss=1./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 32065/32065 [2:05:00<00:00,  4.28it/s, v_num=hot0, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–Œ                  | 1/33 [00:00<00:10,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆâ–                 | 2/33 [00:00<00:08,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–‹                 | 3/33 [00:00<00:08,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–Ž                | 4/33 [00:01<00:07,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 5/33 [00:01<00:07,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 6/33 [00:01<00:06,  4.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–ˆ               | 7/33 [00:01<00:06,  4.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 8/33 [00:01<00:06,  4.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 9/33 [00:02<00:05,  4.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 10/33 [00:02<00:05,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 11/33 [00:02<00:05,  4.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 12/33 [00:02<00:05,  4.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 13/33 [00:03<00:04,  4.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 14/33 [00:03<00:04,  4.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 15/33 [00:03<00:04,  4.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 16/33 [00:03<00:04,  4.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 17/33 [00:04<00:03,  4.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 18/33 [00:04<00:03,  4.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 19/33 [00:04<00:03,  4.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 20/33 [00:04<00:03,  4.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 21/33 [00:04<00:02,  4.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 22/33 [00:05<00:02,  4.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/33 [00:05<00:02,  4.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/33 [00:05<00:02,  4.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25/33 [00:05<00:01,  4.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/33 [00:06<00:01,  4.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 27/33 [00:06<00:01,  4.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/33 [00:06<00:01,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/33 [00:06<00:00,  4.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 30/33 [00:07<00:00,  4.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 31/33 [00:07<00:00,  4.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/33 [00:07<00:00,  4.29it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 32065/32065 [2:05:10<00:00,  4.27it/s, v_num=hot0, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 32065/32065 [2:05:10<00:00,  4.27it/s, v_num=hot0, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 32065/32065 [2:05:32<00:00,  4.26it/s, v_num=hot0, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–ƒâ–ƒâ–‚â–ƒâ–„â–â–â–„â–‚â–‚â–„â–…â–â–ˆâ–ƒâ–†â–‚â–‚â–ƒâ–â–ƒâ–â–ƒâ–„â–ƒâ–…â–‚â–â–â–„â–â–‚â–„â–‚â–â–„â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.02258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.02234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mWaveV5-C - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/9nq4hot0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_080913-9nq4hot0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/WaveV5-C-mem-finetune-2/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/WaveV5-C-Tune2.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  8 10:16 ../model/WaveV5-C-Tune2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/WaveV5-C-mem-finetune-2/last.ckpt\" \\\n",
    "        \"../model/WaveV5-C-Tune2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/WaveV5-C-Tune2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a memory eval \n",
    "# #\n",
    "# # While not at its full potential, its memory ability should start emerging\n",
    "# #\n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/WaveV5-C-Tune2.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Ramping up the ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Mid ctx size (1024), same as tune 2, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 1000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 653 samples (10 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 755 samples (10 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 1000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 1000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 1000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 1786 samples (10 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 2615 samples (10 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 1000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 1304 samples (10 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 1000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 368 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 812 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 1062 samples (10 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 1000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 877 samples (10 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 1000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 582 samples (10 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 295 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 281 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 622 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 881 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 667 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 749 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 359 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 319 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 537 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 564 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 1000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 394 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 379 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 190 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 281 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 271 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 220 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 203 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 198 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 415 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 964 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 337 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 179 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 191 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 299 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 286 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 270 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 211 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 1061 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 190 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 585 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 5581 samples (10 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 713 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 312 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 349 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 274 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 144 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "## Done ##\n",
      "total 450M\n",
      "drwxr-xr-x  2 root root 8.0K Aug  8 10:16 .\n",
      "drwxr-xr-x 17 root root  323 Aug  8 07:24 ..\n",
      "-rw-r--r--  1 root root 196K Aug  8 10:16 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.1M Aug  8 10:16 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Aug  8 10:16 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug  8 10:16 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Aug  8 10:16 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 2.5M Aug  8 10:16 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 2.6M Aug  8 10:16 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 2.7M Aug  8 10:16 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Aug  8 10:16 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Aug  8 10:16 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 3.0M Aug  8 10:16 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 240K Aug  8 10:16 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Aug  8 10:16 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 3.2M Aug  8 10:16 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 3.3M Aug  8 10:16 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 3.4M Aug  8 10:16 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Aug  8 10:16 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Aug  8 10:16 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 3.7M Aug  8 10:16 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 3.7M Aug  8 10:16 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Aug  8 10:16 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 3.9M Aug  8 10:16 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 293K Aug  8 10:16 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Aug  8 10:16 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Aug  8 10:16 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 4.2M Aug  8 10:16 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Aug  8 10:16 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 4.4M Aug  8 10:16 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Aug  8 10:16 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 4.6M Aug  8 10:16 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 4.7M Aug  8 10:16 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Aug  8 10:16 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 4.9M Aug  8 10:16 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root 341K Aug  8 10:16 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Aug  8 10:16 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 5.1M Aug  8 10:16 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Aug  8 10:16 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 5.3M Aug  8 10:16 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 5.4M Aug  8 10:16 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Aug  8 10:16 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 5.6M Aug  8 10:16 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 5.6M Aug  8 10:16 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 5.7M Aug  8 10:16 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 5.8M Aug  8 10:16 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root 392K Aug  8 10:16 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 5.9M Aug  8 10:16 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 6.0M Aug  8 10:16 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 6.1M Aug  8 10:16 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 6.2M Aug  8 10:16 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 6.3M Aug  8 10:16 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 6.4M Aug  8 10:16 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 6.5M Aug  8 10:16 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 6.6M Aug  8 10:16 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 6.7M Aug  8 10:16 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 6.8M Aug  8 10:16 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root 434K Aug  8 10:16 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 6.9M Aug  8 10:16 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 7.0M Aug  8 10:16 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 7.1M Aug  8 10:16 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Aug  8 10:16 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 7.3M Aug  8 10:16 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 7.3M Aug  8 10:16 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 7.5M Aug  8 10:16 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 7.6M Aug  8 10:16 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 7.6M Aug  8 10:16 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Aug  8 10:16 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root 488K Aug  8 10:16 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 7.8M Aug  8 10:16 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 8.0M Aug  8 10:16 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 8.0M Aug  8 10:16 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 8.2M Aug  8 10:16 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 8.2M Aug  8 10:16 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 8.3M Aug  8 10:16 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 8.4M Aug  8 10:16 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 8.5M Aug  8 10:16 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 8.6M Aug  8 10:16 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 8.7M Aug  8 10:16 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 10:16 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 8.8M Aug  8 10:16 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 144K Aug  8 10:16 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Aug  8 10:16 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 1.3M Aug  8 10:16 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug  8 10:16 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Aug  8 10:16 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Aug  8 10:16 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Aug  8 10:16 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Aug  8 10:16 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Aug  8 10:16 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Aug  8 10:16 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Aug  8 10:16 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 508K Aug  8 10:16 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 568K Aug  8 10:16 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 553K Aug  8 10:16 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 554K Aug  8 10:16 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 553K Aug  8 10:16 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 553K Aug  8 10:16 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Aug  8 10:16 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 547K Aug  8 10:16 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 554K Aug  8 10:16 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Aug  8 10:16 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Aug  8 10:16 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 428K Aug  8 10:16 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Aug  8 10:16 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 540K Aug  8 10:16 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Aug  8 10:16 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 546K Aug  8 10:16 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 540K Aug  8 10:16 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 540K Aug  8 10:16 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 545K Aug  8 10:16 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 10:16 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 541K Aug  8 10:16 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Aug  8 10:16 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 379K Aug  8 10:16 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 546K Aug  8 10:16 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 541K Aug  8 10:16 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 10:16 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 10:16 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Aug  8 10:16 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 10:16 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 10:16 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 538K Aug  8 10:16 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 10:16 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root 358K Aug  8 10:16 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 10:16 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 10:16 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 10:16 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 10:16 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 10:16 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 10:16 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 10:16 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 10:16 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Aug  8 10:16 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root 341K Aug  8 10:16 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 10:16 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 10:16 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 10:16 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 10:16 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 10:16 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root 335K Aug  8 10:16 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 10:16 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 10:16 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 10:16 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 10:16 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 10:16 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root 317K Aug  8 10:16 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 10:16 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 10:16 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 10:16 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 10:16 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 10:16 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 10:16 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 10:16 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 10:16 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 10:16 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 10:16 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root 311K Aug  8 10:16 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 10:16 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 827K Aug  8 10:16 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 613K Aug  8 10:16 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 611K Aug  8 10:16 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 594K Aug  8 10:16 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 597K Aug  8 10:16 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 585K Aug  8 10:16 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 586K Aug  8 10:16 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 581K Aug  8 10:16 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 579K Aug  8 10:16 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 568K Aug  8 10:16 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 567K Aug  8 10:16 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 120K Aug  8 10:16 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {50..450..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<00:00, 38511.09it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 527.78it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.54it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 88.75it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-3.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/WaveV5-C-mem-finetune-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-3.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=1024'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-3.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=1024'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1872781226\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1872781226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230808_101701-148bw638\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWaveV5-C - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/148bw638\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 64\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<00:00, 70970.27it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 109.85it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ae9bd9a30ebfb5d8_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-89d0ceddc0ebd8fb_*_of_00032.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c797ff997b790987.arrow and /root/.cache/huggingface/datasets/json/default-5e17b9ef5955c7b4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-90c60cb6469a0853.arrow\n",
      "[rank: 0] Global seed set to 1872781226                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-08 10:17:20,173] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 1872781226\n",
      "[rank: 3] Global seed set to 1872781226\n",
      "[rank: 2] Global seed set to 1872781226\n",
      "[rank: 3] Global seed set to 1872781226\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-08 10:17:39,431] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1872781226\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-08 10:17:39,432] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1872781226\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-08 10:17:39,445] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.08126163482666016 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10234189033508301 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.1024022102355957 seconds\n",
      "Time to load fused_adam op: 0.1024022102355957 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08007407188415527 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10226941108703613 seconds\n",
      "Time to load utils op: 0.10198402404785156 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10248160362243652 seconds\n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003094673156738281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002849102020263672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003018379211425781 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005648136138916016 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 1600/52337 [07:37<4:02:00,  3.49it/s, v_num=w638, train/loss=0./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 52337/52337 [4:06:41<00:00,  3.54it/s, v_num=w638, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–Ž                  | 1/53 [00:00<00:15,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                  | 2/53 [00:00<00:12,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 3/53 [00:00<00:12,  4.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 4/53 [00:00<00:11,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–Š                 | 5/53 [00:01<00:10,  4.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–ˆâ–                | 6/53 [00:01<00:10,  4.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–Œ                | 7/53 [00:01<00:10,  4.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–Š                | 8/53 [00:01<00:09,  4.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–ˆâ–               | 9/53 [00:01<00:09,  4.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–              | 10/53 [00:02<00:09,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‹              | 11/53 [00:02<00:09,  4.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆ              | 12/53 [00:02<00:08,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–             | 13/53 [00:02<00:08,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 14/53 [00:03<00:08,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 15/53 [00:03<00:08,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 16/53 [00:03<00:08,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 17/53 [00:03<00:07,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 18/53 [00:03<00:07,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 19/53 [00:04<00:07,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 20/53 [00:04<00:07,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 21/53 [00:04<00:06,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 22/53 [00:04<00:06,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 23/53 [00:04<00:06,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 24/53 [00:05<00:06,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 25/53 [00:05<00:06,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 26/53 [00:05<00:05,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 27/53 [00:05<00:05,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 28/53 [00:06<00:05,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 29/53 [00:06<00:05,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 30/53 [00:06<00:04,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 31/53 [00:06<00:04,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 32/53 [00:06<00:04,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 33/53 [00:07<00:04,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 34/53 [00:07<00:04,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 35/53 [00:07<00:03,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 36/53 [00:07<00:03,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 37/53 [00:07<00:03,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 38/53 [00:08<00:03,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 39/53 [00:08<00:03,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 40/53 [00:08<00:02,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 41/53 [00:08<00:02,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/53 [00:09<00:02,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 43/53 [00:09<00:02,  4.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/53 [00:09<00:01,  4.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 45/53 [00:09<00:01,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 46/53 [00:09<00:01,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/53 [00:10<00:01,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 48/53 [00:10<00:01,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 49/53 [00:10<00:00,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 50/53 [00:10<00:00,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/53 [00:10<00:00,  4.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/53 [00:11<00:00,  4.67it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 52337/52337 [4:06:58<00:00,  3.53it/s, v_num=w638, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 52337/52337 [4:06:58<00:00,  3.53it/s, v_num=w638, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 52337/52337 [4:07:12<00:00,  3.53it/s, v_num=w638, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–„â–„â–‚â–‡â–â–„â–„â–ƒâ–â–…â–…â–…â–‚â–‚â–ƒâ–‚â–â–ƒâ–„â–‚â–‚â–â–‡â–„â–…â–‡â–…â–…â–ˆâ–†â–…â–„â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–‚â–†â–…â–â–‚â–ƒâ–ƒâ–‚â–â–â–ˆâ–â–â–„â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mWaveV5-C - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/148bw638\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_101701-148bw638/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-3.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-3 (bs=256, train-ctx=1024, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/WaveV5-C-mem-finetune-3/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/WaveV5-C-Tune3.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  8 14:25 ../model/WaveV5-C-Tune3.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/WaveV5-C-mem-finetune-3/last.ckpt\" \\\n",
    "        \"../model/WaveV5-C-Tune3.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/WaveV5-C-Tune3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a memory eval \n",
    "# #\n",
    "# # We should start approaching the full potential of the model, unless its able to exceed 250 tokens of memory\n",
    "# #\n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/WaveV5-C-Tune3.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 4 : Ramping up the ctx size (4096), memory training\n",
    "\n",
    "- Tune 4: Mid ctx size (4096), same as tune 4, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 76 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 106 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 90 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 132 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated a single JSONL file with 180 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 568 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 266 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 966 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 274 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 268 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 302 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 538 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 711 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 368 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 308 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 394 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 197 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 1036 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 312 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 590 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 193 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 763 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 219 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 287 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 144 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 144 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 202 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 291 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 271 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 195 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 204 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 348 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 358 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 623 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 339 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 210 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 655 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 411 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 825 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 382 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 268 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1035 max words - at ../dataset/shuffle-word-1035-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated a single JSONL file with 284 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 549 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 882 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1095 max words - at ../dataset/shuffle-word-1095-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1105 max words - at ../dataset/shuffle-word-1105-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1005 max words - at ../dataset/shuffle-word-1005-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1015 max words - at ../dataset/shuffle-word-1015-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1745 max words - at ../dataset/shuffle-word-1745-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1285 max words - at ../dataset/shuffle-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (20 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1235 max words - at ../dataset/shuffle-word-1235-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1025 max words - at ../dataset/shuffle-word-1025-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (20 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1725 max words - at ../dataset/shuffle-word-1725-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1155 max words - at ../dataset/shuffle-word-1155-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1435 max words - at ../dataset/shuffle-word-1435-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1580 max words - at ../dataset/shuffle-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1135 max words - at ../dataset/shuffle-word-1135-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1205 max words - at ../dataset/shuffle-word-1205-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1685 max words - at ../dataset/shuffle-word-1685-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1615 max words - at ../dataset/shuffle-word-1615-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1690 max words - at ../dataset/shuffle-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1055 max words - at ../dataset/shuffle-word-1055-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1525 max words - at ../dataset/shuffle-word-1525-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1545 max words - at ../dataset/shuffle-word-1545-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1510 max words - at ../dataset/shuffle-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1645 max words - at ../dataset/shuffle-word-1645-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1215 max words - at ../dataset/shuffle-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1245 max words - at ../dataset/shuffle-word-1245-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1405 max words - at ../dataset/shuffle-word-1405-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1085 max words - at ../dataset/shuffle-word-1085-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1585 max words - at ../dataset/shuffle-word-1585-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1590 max words - at ../dataset/shuffle-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1660 max words - at ../dataset/shuffle-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (20 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1680 max words - at ../dataset/shuffle-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1650 max words - at ../dataset/shuffle-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1065 max words - at ../dataset/shuffle-word-1065-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1465 max words - at ../dataset/shuffle-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 55 samples (20 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1875 max words - at ../dataset/shuffle-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1855 max words - at ../dataset/shuffle-word-1855-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1305 max words - at ../dataset/shuffle-word-1305-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1045 max words - at ../dataset/shuffle-word-1045-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1675 max words - at ../dataset/shuffle-word-1675-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1550 max words - at ../dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1145 max words - at ../dataset/shuffle-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 45 samples (20 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1820 max words - at ../dataset/shuffle-word-1820-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1845 max words - at ../dataset/shuffle-word-1845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1455 max words - at ../dataset/shuffle-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1760 max words - at ../dataset/shuffle-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1990 max words - at ../dataset/shuffle-word-1990-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1530 max words - at ../dataset/shuffle-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1785 max words - at ../dataset/shuffle-word-1785-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1515 max words - at ../dataset/shuffle-word-1515-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1850 max words - at ../dataset/shuffle-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1275 max words - at ../dataset/shuffle-word-1275-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1950 max words - at ../dataset/shuffle-word-1950-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1780 max words - at ../dataset/shuffle-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1475 max words - at ../dataset/shuffle-word-1475-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1315 max words - at ../dataset/shuffle-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1880 max words - at ../dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1770 max words - at ../dataset/shuffle-word-1770-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1445 max words - at ../dataset/shuffle-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1355 max words - at ../dataset/shuffle-word-1355-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1345 max words - at ../dataset/shuffle-word-1345-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1830 max words - at ../dataset/shuffle-word-1830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1840 max words - at ../dataset/shuffle-word-1840-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1960 max words - at ../dataset/shuffle-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1795 max words - at ../dataset/shuffle-word-1795-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1810 max words - at ../dataset/shuffle-word-1810-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1195 max words - at ../dataset/shuffle-word-1195-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1870 max words - at ../dataset/shuffle-word-1870-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1165 max words - at ../dataset/shuffle-word-1165-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1415 max words - at ../dataset/shuffle-word-1415-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1935 max words - at ../dataset/shuffle-word-1935-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1930 max words - at ../dataset/shuffle-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1720 max words - at ../dataset/shuffle-word-1720-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1365 max words - at ../dataset/shuffle-word-1365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1665 max words - at ../dataset/shuffle-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1125 max words - at ../dataset/shuffle-word-1125-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1485 max words - at ../dataset/shuffle-word-1485-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1630 max words - at ../dataset/shuffle-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1980 max words - at ../dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1620 max words - at ../dataset/shuffle-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1075 max words - at ../dataset/shuffle-word-1075-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1115 max words - at ../dataset/shuffle-word-1115-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1755 max words - at ../dataset/shuffle-word-1755-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1605 max words - at ../dataset/shuffle-word-1605-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1925 max words - at ../dataset/shuffle-word-1925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1915 max words - at ../dataset/shuffle-word-1915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1735 max words - at ../dataset/shuffle-word-1735-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1375 max words - at ../dataset/shuffle-word-1375-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1185 max words - at ../dataset/shuffle-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1560 max words - at ../dataset/shuffle-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1520 max words - at ../dataset/shuffle-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1565 max words - at ../dataset/shuffle-word-1565-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1385 max words - at ../dataset/shuffle-word-1385-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1715 max words - at ../dataset/shuffle-word-1715-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1885 max words - at ../dataset/shuffle-word-1885-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1910 max words - at ../dataset/shuffle-word-1910-count.jsonl\n",
      "Generated a single JSONL file with 45 samples (20 token repeat) - 1255 max words - at ../dataset/shuffle-word-1255-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1695 max words - at ../dataset/shuffle-word-1695-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1335 max words - at ../dataset/shuffle-word-1335-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1740 max words - at ../dataset/shuffle-word-1740-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1920 max words - at ../dataset/shuffle-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1265 max words - at ../dataset/shuffle-word-1265-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1955 max words - at ../dataset/shuffle-word-1955-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1640 max words - at ../dataset/shuffle-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1705 max words - at ../dataset/shuffle-word-1705-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1995 max words - at ../dataset/shuffle-word-1995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1775 max words - at ../dataset/shuffle-word-1775-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1890 max words - at ../dataset/shuffle-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1975 max words - at ../dataset/shuffle-word-1975-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1625 max words - at ../dataset/shuffle-word-1625-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1730 max words - at ../dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1325 max words - at ../dataset/shuffle-word-1325-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1555 max words - at ../dataset/shuffle-word-1555-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1825 max words - at ../dataset/shuffle-word-1825-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1635 max words - at ../dataset/shuffle-word-1635-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1985 max words - at ../dataset/shuffle-word-1985-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1595 max words - at ../dataset/shuffle-word-1595-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1710 max words - at ../dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1225 max words - at ../dataset/shuffle-word-1225-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1860 max words - at ../dataset/shuffle-word-1860-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1815 max words - at ../dataset/shuffle-word-1815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1940 max words - at ../dataset/shuffle-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1765 max words - at ../dataset/shuffle-word-1765-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1965 max words - at ../dataset/shuffle-word-1965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1865 max words - at ../dataset/shuffle-word-1865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1610 max words - at ../dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1505 max words - at ../dataset/shuffle-word-1505-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1835 max words - at ../dataset/shuffle-word-1835-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1945 max words - at ../dataset/shuffle-word-1945-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1895 max words - at ../dataset/shuffle-word-1895-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1540 max words - at ../dataset/shuffle-word-1540-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1295 max words - at ../dataset/shuffle-word-1295-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1750 max words - at ../dataset/shuffle-word-1750-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1535 max words - at ../dataset/shuffle-word-1535-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1575 max words - at ../dataset/shuffle-word-1575-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1790 max words - at ../dataset/shuffle-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1395 max words - at ../dataset/shuffle-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1175 max words - at ../dataset/shuffle-word-1175-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1425 max words - at ../dataset/shuffle-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1670 max words - at ../dataset/shuffle-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1905 max words - at ../dataset/shuffle-word-1905-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1805 max words - at ../dataset/shuffle-word-1805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1655 max words - at ../dataset/shuffle-word-1655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1570 max words - at ../dataset/shuffle-word-1570-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1970 max words - at ../dataset/shuffle-word-1970-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1495 max words - at ../dataset/shuffle-word-1495-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 1025 max words, 2000 samples - at ../dataset/gen-word-1025-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 2000 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 2000 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 1315 max words, 2000 samples - at ../dataset/gen-word-1315-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 2000 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 2000 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 2000 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 2000 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 2000 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 2000 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 2000 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 1015 max words, 2000 samples - at ../dataset/gen-word-1015-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 2000 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 1055 max words, 2000 samples - at ../dataset/gen-word-1055-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 2000 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 2000 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 2000 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated JSONL file with - 1205 max words, 2000 samples - at ../dataset/gen-word-1205-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 2000 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 2000 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 1505 max words, 2000 samples - at ../dataset/gen-word-1505-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 2000 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 2000 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 1575 max words, 2000 samples - at ../dataset/gen-word-1575-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 2000 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 1695 max words, 2000 samples - at ../dataset/gen-word-1695-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 2000 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 2000 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated JSONL file with - 1115 max words, 2000 samples - at ../dataset/gen-word-1115-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 2000 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 2000 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 2000 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 2000 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 2000 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 2000 samples - at ../dataset/gen-word-1620-count.jsonl\n",
      "Generated JSONL file with - 1395 max words, 2000 samples - at ../dataset/gen-word-1395-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 1905 max words, 2000 samples - at ../dataset/gen-word-1905-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 2000 samples - at ../dataset/gen-word-1780-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 2000 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 2000 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 2000 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 2000 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 2000 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 1475 max words, 2000 samples - at ../dataset/gen-word-1475-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 2000 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 2000 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 2000 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 2000 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 2000 samples - at ../dataset/gen-word-1750-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 2000 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 2000 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 1155 max words, 2000 samples - at ../dataset/gen-word-1155-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 2000 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 2000 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 2000 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 2000 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 2000 samples - at ../dataset/gen-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 2000 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 2000 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 2000 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 2000 samples - at ../dataset/gen-word-1640-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 2000 samples - at ../dataset/gen-word-1690-count.jsonl\n",
      "Generated JSONL file with - 1325 max words, 2000 samples - at ../dataset/gen-word-1325-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 2000 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 2000 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 2000 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 1145 max words, 2000 samples - at ../dataset/gen-word-1145-count.jsonl\n",
      "Generated JSONL file with - 1685 max words, 2000 samples - at ../dataset/gen-word-1685-count.jsonl\n",
      "Generated JSONL file with - 1185 max words, 2000 samples - at ../dataset/gen-word-1185-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 2000 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 2000 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 2000 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 2000 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 2000 samples - at ../dataset/gen-word-1660-count.jsonl\n",
      "Generated JSONL file with - 1045 max words, 2000 samples - at ../dataset/gen-word-1045-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 2000 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 1705 max words, 2000 samples - at ../dataset/gen-word-1705-count.jsonl\n",
      "Generated JSONL file with - 1795 max words, 2000 samples - at ../dataset/gen-word-1795-count.jsonl\n",
      "Generated JSONL file with - 1175 max words, 2000 samples - at ../dataset/gen-word-1175-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 2000 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated JSONL file with - 1265 max words, 2000 samples - at ../dataset/gen-word-1265-count.jsonl\n",
      "Generated JSONL file with - 1625 max words, 2000 samples - at ../dataset/gen-word-1625-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 2000 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 2000 samples - at ../dataset/gen-word-1970-count.jsonl\n",
      "Generated JSONL file with - 1535 max words, 2000 samples - at ../dataset/gen-word-1535-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 2000 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 2000 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 2000 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated JSONL file with - 1745 max words, 2000 samples - at ../dataset/gen-word-1745-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 2000 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 1775 max words, 2000 samples - at ../dataset/gen-word-1775-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 2000 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 2000 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 2000 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 2000 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 2000 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 2000 samples - at ../dataset/gen-word-1930-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 2000 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 2000 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 2000 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 1485 max words, 2000 samples - at ../dataset/gen-word-1485-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 2000 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 2000 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated JSONL file with - 1075 max words, 2000 samples - at ../dataset/gen-word-1075-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 2000 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 2000 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 2000 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 1455 max words, 2000 samples - at ../dataset/gen-word-1455-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 2000 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 2000 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 1065 max words, 2000 samples - at ../dataset/gen-word-1065-count.jsonl\n",
      "Generated JSONL file with - 1095 max words, 2000 samples - at ../dataset/gen-word-1095-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 2000 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 2000 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 2000 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 2000 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 2000 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 2000 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 1225 max words, 2000 samples - at ../dataset/gen-word-1225-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 2000 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 2000 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 2000 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 2000 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 2000 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 1245 max words, 2000 samples - at ../dataset/gen-word-1245-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 2000 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 2000 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 2000 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 2000 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 2000 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 2000 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 2000 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated JSONL file with - 1615 max words, 2000 samples - at ../dataset/gen-word-1615-count.jsonl\n",
      "Generated JSONL file with - 1215 max words, 2000 samples - at ../dataset/gen-word-1215-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 2000 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 2000 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 2000 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 2000 samples - at ../dataset/gen-word-1550-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 2000 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 2000 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 2000 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 2000 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 1865 max words, 2000 samples - at ../dataset/gen-word-1865-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 2000 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 1385 max words, 2000 samples - at ../dataset/gen-word-1385-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 2000 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated JSONL file with - 1285 max words, 2000 samples - at ../dataset/gen-word-1285-count.jsonl\n",
      "Generated JSONL file with - 1345 max words, 2000 samples - at ../dataset/gen-word-1345-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 2000 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 2000 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 2000 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 1445 max words, 2000 samples - at ../dataset/gen-word-1445-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 2000 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated JSONL file with - 1085 max words, 2000 samples - at ../dataset/gen-word-1085-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 2000 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated JSONL file with - 1425 max words, 2000 samples - at ../dataset/gen-word-1425-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 2000 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 2000 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 1305 max words, 2000 samples - at ../dataset/gen-word-1305-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 2000 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 2000 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated JSONL file with - 1755 max words, 2000 samples - at ../dataset/gen-word-1755-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 2000 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 2000 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 2000 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 2000 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 2000 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1875 max words, 2000 samples - at ../dataset/gen-word-1875-count.jsonl\n",
      "Generated JSONL file with - 1645 max words, 2000 samples - at ../dataset/gen-word-1645-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 2000 samples - at ../dataset/gen-word-1830-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 2000 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 2000 samples - at ../dataset/gen-word-1730-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 2000 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 2000 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 2000 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 2000 samples - at ../dataset/gen-word-1540-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 2000 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 1125 max words, 2000 samples - at ../dataset/gen-word-1125-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 2000 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 2000 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 2000 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 1195 max words, 2000 samples - at ../dataset/gen-word-1195-count.jsonl\n",
      "Generated JSONL file with - 1995 max words, 2000 samples - at ../dataset/gen-word-1995-count.jsonl\n",
      "Generated JSONL file with - 1565 max words, 2000 samples - at ../dataset/gen-word-1565-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 2000 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 2000 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 2000 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 2000 samples - at ../dataset/gen-word-1680-count.jsonl\n",
      "Generated JSONL file with - 1135 max words, 2000 samples - at ../dataset/gen-word-1135-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 2000 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated JSONL file with - 1355 max words, 2000 samples - at ../dataset/gen-word-1355-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 2000 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 2000 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 2000 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 2000 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated JSONL file with - 1105 max words, 2000 samples - at ../dataset/gen-word-1105-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 2000 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 2000 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 2000 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 2000 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 2000 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 2000 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated JSONL file with - 1035 max words, 2000 samples - at ../dataset/gen-word-1035-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 2000 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated JSONL file with - 1295 max words, 2000 samples - at ../dataset/gen-word-1295-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 2000 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated JSONL file with - 1415 max words, 2000 samples - at ../dataset/gen-word-1415-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 2000 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated JSONL file with - 1735 max words, 2000 samples - at ../dataset/gen-word-1735-count.jsonl\n",
      "Generated JSONL file with - 1945 max words, 2000 samples - at ../dataset/gen-word-1945-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 2000 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 2000 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated JSONL file with - 1975 max words, 2000 samples - at ../dataset/gen-word-1975-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 2000 samples - at ../dataset/gen-word-1520-count.jsonl\n",
      "Generated JSONL file with - 1605 max words, 2000 samples - at ../dataset/gen-word-1605-count.jsonl\n",
      "Generated JSONL file with - 1005 max words, 2000 samples - at ../dataset/gen-word-1005-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 2000 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 2000 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 2000 samples - at ../dataset/gen-word-1670-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 2000 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1955 max words, 2000 samples - at ../dataset/gen-word-1955-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 2000 samples - at ../dataset/gen-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1715 max words, 2000 samples - at ../dataset/gen-word-1715-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 2000 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 2000 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 2000 samples - at ../dataset/gen-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1545 max words, 2000 samples - at ../dataset/gen-word-1545-count.jsonl\n",
      "Generated JSONL file with - 1595 max words, 2000 samples - at ../dataset/gen-word-1595-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 2000 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1935 max words, 2000 samples - at ../dataset/gen-word-1935-count.jsonl\n",
      "Generated JSONL file with - 1465 max words, 2000 samples - at ../dataset/gen-word-1465-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 2000 samples - at ../dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 2000 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 2000 samples - at ../dataset/gen-word-1960-count.jsonl\n",
      "Generated JSONL file with - 1235 max words, 2000 samples - at ../dataset/gen-word-1235-count.jsonl\n",
      "Generated JSONL file with - 1435 max words, 2000 samples - at ../dataset/gen-word-1435-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 2000 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 2000 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 2000 samples - at ../dataset/gen-word-1840-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 2000 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1855 max words, 2000 samples - at ../dataset/gen-word-1855-count.jsonl\n",
      "Generated JSONL file with - 1665 max words, 2000 samples - at ../dataset/gen-word-1665-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 2000 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated JSONL file with - 1515 max words, 2000 samples - at ../dataset/gen-word-1515-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 2000 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 2000 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 2000 samples - at ../dataset/gen-word-1650-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 2000 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1585 max words, 2000 samples - at ../dataset/gen-word-1585-count.jsonl\n",
      "Generated JSONL file with - 1165 max words, 2000 samples - at ../dataset/gen-word-1165-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 2000 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1635 max words, 2000 samples - at ../dataset/gen-word-1635-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 2000 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 1825 max words, 2000 samples - at ../dataset/gen-word-1825-count.jsonl\n",
      "Generated JSONL file with - 1405 max words, 2000 samples - at ../dataset/gen-word-1405-count.jsonl\n",
      "Generated JSONL file with - 1725 max words, 2000 samples - at ../dataset/gen-word-1725-count.jsonl\n",
      "Generated JSONL file with - 1555 max words, 2000 samples - at ../dataset/gen-word-1555-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 2000 samples - at ../dataset/gen-word-1590-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 2000 samples - at ../dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 2000 samples - at ../dataset/gen-word-1740-count.jsonl\n",
      "Generated JSONL file with - 1835 max words, 2000 samples - at ../dataset/gen-word-1835-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 2000 samples - at ../dataset/gen-word-1910-count.jsonl\n",
      "Generated JSONL file with - 1275 max words, 2000 samples - at ../dataset/gen-word-1275-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 2000 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 1255 max words, 2000 samples - at ../dataset/gen-word-1255-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 2000 samples - at ../dataset/gen-word-1710-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 2000 samples - at ../dataset/gen-word-1880-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 2000 samples - at ../dataset/gen-word-1850-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 2000 samples - at ../dataset/gen-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 2000 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated JSONL file with - 1985 max words, 2000 samples - at ../dataset/gen-word-1985-count.jsonl\n",
      "Generated JSONL file with - 1335 max words, 2000 samples - at ../dataset/gen-word-1335-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 2000 samples - at ../dataset/gen-word-1810-count.jsonl\n",
      "Generated JSONL file with - 1495 max words, 2000 samples - at ../dataset/gen-word-1495-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 2000 samples - at ../dataset/gen-word-1510-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 2000 samples - at ../dataset/gen-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1885 max words, 2000 samples - at ../dataset/gen-word-1885-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 2000 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 2000 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 2000 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 2000 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1525 max words, 2000 samples - at ../dataset/gen-word-1525-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 2000 samples - at ../dataset/gen-word-1580-count.jsonl\n",
      "Generated JSONL file with - 1915 max words, 2000 samples - at ../dataset/gen-word-1915-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 2000 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1365 max words, 2000 samples - at ../dataset/gen-word-1365-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 2000 samples - at ../dataset/gen-word-1890-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 2000 samples - at ../dataset/gen-word-1530-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 2000 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 2000 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 1375 max words, 2000 samples - at ../dataset/gen-word-1375-count.jsonl\n",
      "Generated JSONL file with - 1675 max words, 2000 samples - at ../dataset/gen-word-1675-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 2000 samples - at ../dataset/gen-word-1560-count.jsonl\n",
      "Generated JSONL file with - 1965 max words, 2000 samples - at ../dataset/gen-word-1965-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 2000 samples - at ../dataset/gen-word-1820-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 2000 samples - at ../dataset/gen-word-1610-count.jsonl\n",
      "Generated JSONL file with - 1655 max words, 2000 samples - at ../dataset/gen-word-1655-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 2000 samples - at ../dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 2000 samples - at ../dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1845 max words, 2000 samples - at ../dataset/gen-word-1845-count.jsonl\n",
      "Generated JSONL file with - 1805 max words, 2000 samples - at ../dataset/gen-word-1805-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 2000 samples - at ../dataset/gen-word-1760-count.jsonl\n",
      "Generated JSONL file with - 1895 max words, 2000 samples - at ../dataset/gen-word-1895-count.jsonl\n",
      "Generated JSONL file with - 1785 max words, 2000 samples - at ../dataset/gen-word-1785-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 2000 samples - at ../dataset/gen-word-1920-count.jsonl\n",
      "Generated JSONL file with - 1765 max words, 2000 samples - at ../dataset/gen-word-1765-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 2000 samples - at ../dataset/gen-word-1940-count.jsonl\n",
      "Generated JSONL file with - 1815 max words, 2000 samples - at ../dataset/gen-word-1815-count.jsonl\n",
      "Generated JSONL file with - 1925 max words, 2000 samples - at ../dataset/gen-word-1925-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 2000 samples - at ../dataset/gen-word-1950-count.jsonl\n",
      "## Done ##\n",
      "total 7.8G\n",
      "drwxr-xr-x  2 root root  36K Aug  8 14:25 .\n",
      "drwxr-xr-x 17 root root  323 Aug  8 07:24 ..\n",
      "-rw-r--r--  1 root root  20K Aug  8 14:25 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.1M Aug  8 14:25 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:25 gen-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:25 gen-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:25 gen-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:25 gen-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:25 gen-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Aug  8 14:25 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:25 gen-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Aug  8 14:26 gen-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug  8 14:25 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:25 gen-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:25 gen-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:26 gen-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Aug  8 14:25 gen-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Aug  8 14:25 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:25 gen-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:25 gen-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:25 gen-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:25 gen-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root 2.5M Aug  8 14:25 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Aug  8 14:26 gen-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:25 gen-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:25 gen-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:25 gen-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root 2.6M Aug  8 14:25 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Aug  8 14:26 gen-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:25 gen-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:25 gen-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root 2.7M Aug  8 14:25 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Aug  8 14:26 gen-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:25 gen-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:25 gen-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Aug  8 14:25 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Aug  8 14:26 gen-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:25 gen-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:25 gen-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Aug  8 14:25 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:25 gen-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Aug  8 14:26 gen-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root 3.0M Aug  8 14:25 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Aug  8 14:26 gen-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:25 gen-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:25 gen-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root  25K Aug  8 14:25 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Aug  8 14:25 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:25 gen-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Aug  8 14:26 gen-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root 3.2M Aug  8 14:25 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Aug  8 14:26 gen-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:25 gen-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root 3.3M Aug  8 14:25 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Aug  8 14:26 gen-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:25 gen-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:25 gen-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:25 gen-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root 3.4M Aug  8 14:25 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:25 gen-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Aug  8 14:26 gen-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:25 gen-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:25 gen-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:25 gen-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Aug  8 14:25 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:25 gen-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Aug  8 14:26 gen-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root 3.6M Aug  8 14:25 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:25 gen-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Aug  8 14:26 gen-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:25 gen-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:25 gen-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root 3.7M Aug  8 14:25 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Aug  8 14:26 gen-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Aug  8 14:25 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:25 gen-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Aug  8 14:26 gen-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Aug  8 14:25 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:25 gen-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:25 gen-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Aug  8 14:26 gen-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Aug  8 14:25 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Aug  8 14:26 gen-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  39M Aug  8 14:26 gen-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root  30K Aug  8 14:25 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Aug  8 14:25 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  39M Aug  8 14:26 gen-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Aug  8 14:25 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 4.2M Aug  8 14:25 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Aug  8 14:25 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 4.4M Aug  8 14:25 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Aug  8 14:25 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 4.6M Aug  8 14:25 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 4.7M Aug  8 14:25 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Aug  8 14:25 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 4.9M Aug  8 14:25 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Aug  8 14:25 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Aug  8 14:25 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 5.1M Aug  8 14:25 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Aug  8 14:25 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Aug  8 14:25 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 5.4M Aug  8 14:25 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Aug  8 14:25 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 5.6M Aug  8 14:25 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 5.7M Aug  8 14:25 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 5.7M Aug  8 14:25 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 5.8M Aug  8 14:25 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  39K Aug  8 14:25 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 5.9M Aug  8 14:25 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 6.0M Aug  8 14:25 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 6.1M Aug  8 14:25 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 6.2M Aug  8 14:26 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 6.3M Aug  8 14:25 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 6.4M Aug  8 14:25 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 6.5M Aug  8 14:25 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 6.6M Aug  8 14:25 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 6.7M Aug  8 14:26 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 6.8M Aug  8 14:25 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  44K Aug  8 14:25 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 6.9M Aug  8 14:26 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 7.0M Aug  8 14:25 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 7.1M Aug  8 14:25 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Aug  8 14:25 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 7.3M Aug  8 14:25 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 7.4M Aug  8 14:25 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 7.4M Aug  8 14:25 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 7.5M Aug  8 14:25 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Aug  8 14:25 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Aug  8 14:25 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  50K Aug  8 14:25 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 7.9M Aug  8 14:26 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 7.9M Aug  8 14:25 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 8.0M Aug  8 14:25 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 8.1M Aug  8 14:25 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 8.2M Aug  8 14:25 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 8.3M Aug  8 14:25 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 8.4M Aug  8 14:25 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 8.5M Aug  8 14:25 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 8.6M Aug  8 14:25 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 8.7M Aug  8 14:25 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  51K Aug  8 14:25 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 8.8M Aug  8 14:25 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 8.9M Aug  8 14:25 gen-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root 9.0M Aug  8 14:25 gen-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root 9.1M Aug  8 14:25 gen-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root 9.1M Aug  8 14:25 gen-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root 9.2M Aug  8 14:25 gen-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root 9.3M Aug  8 14:25 gen-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root 9.4M Aug  8 14:25 gen-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root 9.6M Aug  8 14:25 gen-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root 9.6M Aug  8 14:25 gen-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root  15K Aug  8 14:25 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Aug  8 14:25 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 9.7M Aug  8 14:26 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 9.8M Aug  8 14:25 gen-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 9.9M Aug  8 14:26 gen-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  10M Aug  8 14:25 gen-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:25 gen-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:25 gen-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root 1.3M Aug  8 14:25 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:25 gen-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:25 gen-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Aug  8 14:26 gen-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:25 gen-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:25 gen-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:26 gen-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:25 gen-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:26 gen-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug  8 14:25 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:26 gen-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:25 gen-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:26 gen-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:26 gen-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Aug  8 14:25 gen-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:25 gen-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Aug  8 14:25 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:25 gen-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:25 gen-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Aug  8 14:26 gen-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:25 gen-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:25 gen-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Aug  8 14:25 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:25 gen-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:25 gen-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:25 gen-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Aug  8 14:26 gen-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:25 gen-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Aug  8 14:25 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:25 gen-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Aug  8 14:26 gen-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:25 gen-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Aug  8 14:25 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:25 gen-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:25 gen-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:26 gen-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Aug  8 14:25 gen-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Aug  8 14:25 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:26 gen-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Aug  8 14:25 gen-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:25 gen-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Aug  8 14:25 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:25 gen-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:25 gen-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:25 gen-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Aug  8 14:26 gen-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:25 gen-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Aug  8 14:25 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:25 gen-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:25 gen-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Aug  8 14:26 gen-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Aug  8 14:26 gen-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root  55K Aug  8 14:25 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 568K Aug  8 14:25 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root 561K Aug  8 14:25 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root 556K Aug  8 14:25 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root 552K Aug  8 14:25 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Aug  8 14:25 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Aug  8 14:25 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root 547K Aug  8 14:25 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Aug  8 14:25 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root 546K Aug  8 14:25 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Aug  8 14:25 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root 537K Aug  8 14:25 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Aug  8 14:25 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root  43K Aug  8 14:25 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Aug  8 14:25 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root 547K Aug  8 14:25 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root 548K Aug  8 14:25 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Aug  8 14:25 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Aug  8 14:25 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Aug  8 14:25 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Aug  8 14:25 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Aug  8 14:25 shuffle-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Aug  8 14:25 shuffle-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Aug  8 14:25 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Aug  8 14:25 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Aug  8 14:25 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Aug  8 14:25 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root  40K Aug  8 14:25 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Aug  8 14:25 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Aug  8 14:25 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root 540K Aug  8 14:25 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 14:25 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 14:25 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 14:25 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Aug  8 14:25 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Aug  8 14:25 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 14:25 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Aug  8 14:25 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Aug  8 14:25 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  38K Aug  8 14:25 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 14:25 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 538K Aug  8 14:25 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 14:25 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 14:25 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Aug  8 14:25 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 538K Aug  8 14:25 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 538K Aug  8 14:25 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 14:25 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Aug  8 14:25 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Aug  8 14:25 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 14:25 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Aug  8 14:25 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 14:25 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 14:25 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  35K Aug  8 14:25 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 14:25 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  33K Aug  8 14:25 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Aug  8 14:25 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Aug  8 14:25 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Aug  8 14:25 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  33K Aug  8 14:25 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Aug  8 14:25 shuffle-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Aug  8 14:25 shuffle-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root  78K Aug  8 14:25 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 605K Aug  8 14:25 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Aug  8 14:25 shuffle-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root 599K Aug  8 14:25 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root 590K Aug  8 14:25 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root 590K Aug  8 14:25 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root 586K Aug  8 14:25 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root 580K Aug  8 14:25 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Aug  8 14:25 shuffle-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Aug  8 14:25 shuffle-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root 572K Aug  8 14:25 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root 576K Aug  8 14:25 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root 570K Aug  8 14:25 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root 568K Aug  8 14:25 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Aug  8 14:25 shuffle-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Aug  8 14:25 shuffle-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Aug  8 14:25 shuffle-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Aug  8 14:25 shuffle-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Aug  8 14:25 shuffle-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Aug  8 14:25 shuffle-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root 119K Aug  8 14:25 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 2000 words dataset\n",
    "# \n",
    "for i in {50..2000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 21965.02it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.54it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.18it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-4.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/WaveV5-C-mem-finetune-4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-4.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/WaveV5-C/WaveV5-C-mem-finetune-4.yaml', '--trainer.logger.init_args.name=WaveV5-C - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 125379871\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 125379871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230808_174617-8ustmchv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mWaveV5-C - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/8ustmchv\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 64\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 17127.11it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.65it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1352fda467ad199a_*_of_00032.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d7b30f33d8960038_*_of_00032.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8af12fd602ce2516.arrow and /root/.cache/huggingface/datasets/json/default-69ee389b229bd2e9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5a4dbf8554ed833f.arrow\n",
      "Saving the dataset (1/16 shards):   7%| | 55551/824806 [00:01<00:17, 43774.16 exSetting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/16 shards):   8%| | 63551/824806 [00:01<00:15, 49688.08 exSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/16 shards):  11%| | 87551/824806 [00:01<00:11, 61497.94 ex[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/16 shards):  12%| | 95551/824806 [00:01<00:11, 64513.05 ex[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/16 shards):  18%|â–| 151102/824806 [00:02<00:09, 68881.91 e[rank: 1] Global seed set to 125379871\n",
      "[rank: 3] Global seed set to 125379871\n",
      "[rank: 2] Global seed set to 125379871\n",
      "[rank: 0] Global seed set to 125379871                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-08 17:46:48,848] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 125379871\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-08 17:46:54,240] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 125379871\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-08 17:46:54,357] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 125379871\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-08 17:46:54,559] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0821537971496582 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10131955146789551 seconds\n",
      "Time to load fused_adam op: 0.10172057151794434 seconds\n",
      "Time to load fused_adam op: 0.10212159156799316 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.08530807495117188 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10223555564880371 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10164594650268555 seconds\n",
      "Time to load utils op: 0.10154533386230469 seconds\n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002779960632324219 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003066062927246094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002872943878173828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005936622619628906 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   1%| | 1600/206202 [16:40<35:31:36,  1.60it/s, v_num=mchv, train/loss=/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 206202/206202 [36:03:00<00:00,  1.59it/s, v_num=mchv, train/los\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/207 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/207 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                  | 1/207 [00:00<01:14,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|â–                 | 2/207 [00:00<01:17,  2.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|â–Ž                 | 3/207 [00:01<01:13,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–Ž                 | 4/207 [00:01<01:14,  2.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–                 | 5/207 [00:01<01:11,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–Œ                 | 6/207 [00:02<01:11,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–Œ                 | 7/207 [00:02<01:07,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                 | 8/207 [00:02<01:09,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–Š                 | 9/207 [00:03<01:07,  2.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–Š                | 10/207 [00:03<01:05,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                | 11/207 [00:03<01:03,  3.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–‰                | 12/207 [00:04<01:05,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                | 13/207 [00:04<01:03,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|â–ˆâ–               | 14/207 [00:04<01:03,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|â–ˆâ–               | 15/207 [00:05<01:04,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–Ž               | 16/207 [00:05<01:02,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–               | 17/207 [00:05<01:03,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–               | 18/207 [00:05<01:02,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–Œ               | 19/207 [00:06<01:02,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‹               | 20/207 [00:06<01:01,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‹               | 21/207 [00:06<01:00,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–Š               | 22/207 [00:07<01:00,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–‰               | 23/207 [00:07<00:59,  3.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–‰               | 24/207 [00:07<00:58,  3.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆ               | 25/207 [00:08<00:58,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–              | 26/207 [00:08<00:58,  3.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–              | 27/207 [00:08<00:58,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|â–ˆâ–ˆâ–Ž              | 28/207 [00:09<00:58,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|â–ˆâ–ˆâ–              | 29/207 [00:09<00:58,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|â–ˆâ–ˆâ–              | 30/207 [00:09<00:57,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–Œ              | 31/207 [00:10<00:57,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‹              | 32/207 [00:10<00:57,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|â–ˆâ–ˆâ–‹              | 33/207 [00:10<00:57,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|â–ˆâ–ˆâ–Š              | 34/207 [00:11<00:56,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–Š              | 35/207 [00:11<00:56,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–‰              | 36/207 [00:11<00:55,  3.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆ              | 37/207 [00:12<00:55,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆ              | 38/207 [00:12<00:55,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–             | 39/207 [00:12<00:54,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–Ž             | 40/207 [00:12<00:54,  3.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–Ž             | 41/207 [00:13<00:53,  3.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–             | 42/207 [00:13<00:54,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–Œ             | 43/207 [00:14<00:53,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–Œ             | 44/207 [00:14<00:54,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|â–ˆâ–ˆâ–ˆâ–‹             | 45/207 [00:14<00:53,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|â–ˆâ–ˆâ–ˆâ–Š             | 46/207 [00:15<00:53,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–Š             | 47/207 [00:15<00:52,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–‰             | 48/207 [00:15<00:52,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆ             | 49/207 [00:16<00:52,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆ             | 50/207 [00:16<00:51,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–            | 51/207 [00:16<00:51,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 52/207 [00:17<00:50,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 53/207 [00:17<00:50,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–            | 54/207 [00:17<00:50,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 55/207 [00:18<00:50,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 56/207 [00:18<00:49,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 57/207 [00:18<00:49,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 58/207 [00:19<00:48,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 59/207 [00:19<00:48,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 60/207 [00:19<00:48,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 61/207 [00:20<00:48,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 62/207 [00:20<00:48,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 63/207 [00:20<00:47,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 64/207 [00:21<00:47,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 65/207 [00:21<00:47,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 66/207 [00:21<00:46,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 67/207 [00:22<00:46,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 68/207 [00:22<00:45,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 69/207 [00:22<00:45,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 70/207 [00:23<00:45,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 71/207 [00:23<00:45,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 72/207 [00:23<00:44,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 73/207 [00:24<00:44,  3.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 74/207 [00:24<00:43,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 75/207 [00:24<00:43,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 76/207 [00:25<00:43,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 77/207 [00:25<00:43,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 78/207 [00:26<00:43,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 79/207 [00:26<00:42,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 80/207 [00:26<00:42,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 81/207 [00:27<00:42,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 82/207 [00:27<00:41,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 83/207 [00:27<00:41,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 84/207 [00:28<00:41,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 85/207 [00:28<00:40,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 86/207 [00:28<00:40,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 87/207 [00:28<00:39,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 88/207 [00:29<00:39,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 89/207 [00:29<00:39,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 90/207 [00:29<00:38,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 91/207 [00:30<00:38,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 92/207 [00:30<00:38,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 93/207 [00:31<00:38,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 94/207 [00:31<00:37,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 95/207 [00:31<00:37,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 96/207 [00:32<00:37,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 97/207 [00:32<00:36,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 98/207 [00:32<00:36,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 99/207 [00:33<00:36,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 100/207 [00:33<00:35,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 101/207 [00:33<00:35,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 102/207 [00:34<00:35,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 103/207 [00:34<00:34,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 104/207 [00:35<00:34,  2.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 105/207 [00:35<00:34,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 106/207 [00:35<00:33,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 107/207 [00:35<00:33,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 108/207 [00:36<00:33,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 109/207 [00:36<00:32,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 110/207 [00:36<00:32,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 111/207 [00:37<00:32,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 112/207 [00:37<00:31,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 113/207 [00:37<00:31,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 114/207 [00:38<00:31,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 115/207 [00:38<00:30,  2.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 116/207 [00:38<00:30,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 117/207 [00:39<00:30,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 118/207 [00:39<00:29,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 119/207 [00:39<00:29,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 120/207 [00:39<00:28,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 121/207 [00:40<00:28,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 122/207 [00:40<00:28,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 123/207 [00:40<00:27,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 124/207 [00:41<00:27,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 125/207 [00:41<00:27,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 126/207 [00:42<00:27,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 127/207 [00:42<00:26,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/207 [00:42<00:26,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 129/207 [00:43<00:26,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 130/207 [00:43<00:25,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 131/207 [00:43<00:25,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 132/207 [00:44<00:25,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 133/207 [00:44<00:24,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 134/207 [00:44<00:24,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 135/207 [00:44<00:23,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 136/207 [00:45<00:23,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/207 [00:45<00:23,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 138/207 [00:46<00:23,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 139/207 [00:46<00:22,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 140/207 [00:46<00:22,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 141/207 [00:47<00:22,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 142/207 [00:47<00:21,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 143/207 [00:47<00:21,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 144/207 [00:47<00:20,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 145/207 [00:48<00:20,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/207 [00:48<00:20,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/207 [00:49<00:20,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/207 [00:49<00:19,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/207 [00:49<00:19,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/207 [00:50<00:19,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 151/207 [00:50<00:18,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/207 [00:50<00:18,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 153/207 [00:51<00:18,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 154/207 [00:51<00:17,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 155/207 [00:51<00:17,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 156/207 [00:52<00:17,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 157/207 [00:52<00:16,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 158/207 [00:52<00:16,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 159/207 [00:53<00:16,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 160/207 [00:53<00:15,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 161/207 [00:53<00:15,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 162/207 [00:53<00:14,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 163/207 [00:54<00:14,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 164/207 [00:54<00:14,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 165/207 [00:55<00:14,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 166/207 [00:55<00:13,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 167/207 [00:55<00:13,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 168/207 [00:56<00:13,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/207 [00:56<00:12,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 170/207 [00:56<00:12,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/207 [00:57<00:12,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 172/207 [00:57<00:11,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 173/207 [00:57<00:11,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 174/207 [00:58<00:11,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 175/207 [00:58<00:10,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 176/207 [00:58<00:10,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 177/207 [00:59<00:10,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 178/207 [00:59<00:09,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 179/207 [00:59<00:09,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 180/207 [00:59<00:08,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 181/207 [01:00<00:08,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 182/207 [01:00<00:08,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 183/207 [01:00<00:07,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 184/207 [01:01<00:07,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 185/207 [01:01<00:07,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 186/207 [01:02<00:07,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 187/207 [01:02<00:06,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 188/207 [01:02<00:06,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 189/207 [01:03<00:06,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 190/207 [01:03<00:05,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 191/207 [01:03<00:05,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 192/207 [01:04<00:05,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 193/207 [01:04<00:04,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 194/207 [01:04<00:04,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 195/207 [01:05<00:04,  2.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 196/207 [01:05<00:03,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 197/207 [01:05<00:03,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 198/207 [01:05<00:02,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 199/207 [01:06<00:02,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 200/207 [01:06<00:02,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 201/207 [01:06<00:01,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 202/207 [01:07<00:01,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 203/207 [01:07<00:01,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 204/207 [01:07<00:00,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 205/207 [01:08<00:00,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 206/207 [01:08<00:00,  3.01it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 206202/206202 [36:04:11<00:00,  1.59it/s, v_num=mchv, train/los\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 206202/206202 [36:04:11<00:00,  1.59it/s, v_num=mchv, train/los\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 206202/206202 [36:04:28<00:00,  1.59it/s, v_num=mchv, train/los\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.071 MB of 0.071 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ˆâ–‚â–‚â–ƒâ–…â–‚â–â–‡â–†â–‚â–‚â–…â–…â–ƒâ–â–†â–„â–‚â–â–‡â–ƒâ–â–ƒâ–„â–„â–„â–ˆâ–â–â–„â–‡â–‚â–ƒâ–‚â–‚â–ˆâ–â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–â–‚â–‚â–‡â–â–‡â–â–â–‚â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.01331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.01854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mWaveV5-C - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/8ustmchv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230808_174617-8ustmchv/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/WaveV5-C-mem-finetune-4.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-4 (bs=256, train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/WaveV5-C-mem-finetune-4/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/WaveV5-C-Tune4.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 10 05:52 ../model/WaveV5-C-Tune4.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/WaveV5-C-mem-finetune-4/last.ckpt\" \\\n",
    "        \"../model/WaveV5-C-Tune4.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/WaveV5-C-Tune4.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a memory eval \n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/WaveV5-C-Tune4.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

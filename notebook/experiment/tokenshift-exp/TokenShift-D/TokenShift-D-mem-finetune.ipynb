{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Token Shift Experiment (Memory Finetune)\n",
    "This continues off from `./TokenShift-D-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "This is done generally in 3 Tune stages\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set.\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens.\n",
    "- Tune 3: Mid ctx size (1024), stage 2, scaled up to 1024 context sizes.\n",
    "- Tune 4: Mid ctx size (4096), stage 3, scaled up to 4096 context sizes.\n",
    "\n",
    "In all cases, the input tokens is always masked. And we intentionally use the limited word set for memory training, which matches the same wordset used in the original memory evaluation of raven pretrained models. This is intentional to serve as both consistent comparision between experiments, and resonable training time.\n",
    "\n",
    "One of the issue faced previously with an excessive large word set, is that the model would be required to see \"new words\" atleast a few time before being able to train the memory process. This drastically slowed down the process as the large word list meant the model was constantly spending time learning new words (instead of memory training).\n",
    "\n",
    "If we want to increase the number / type of words the model can handle for memory training, that can be done later as a stage 4 memory tune if needed. But that exceeds the current requirements for the memory experiment process.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init required dirs\n",
    "# !mkdir -p ../../../../model/\n",
    "# !mkdir -p ../../../../datapath/\n",
    "# !mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# # Download the Stage2.pth file\n",
    "# !rm -rf ../../../../model/TokenShift-D-Stage2.pth\n",
    "# !cd ../../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-D-Stage2.pth\n",
    "# !ls -alh ../../../../model/TokenShift-D-Stage2.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/TokenShift-D\n",
      "TRAINER_DIR: /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet\n",
      "PROJECT_DIR: /root/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"TokenShift-D\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ../dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ../dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ../dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ../dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ../dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ../dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 20M\n",
      "drwxr-xr-x  2 root root  330 Jul 25 00:39 .\n",
      "drwxr-xr-x 13 root root  248 Jul 24 13:07 ..\n",
      "-rw-r--r--  1 root root 612K Jul 25 00:39 word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Jul 25 00:39 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 724K Jul 25 00:39 word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 836K Jul 25 00:39 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root 856K Jul 25 00:39 word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 5.1M Jul 25 00:39 word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 963K Jul 25 00:39 word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Jul 25 00:39 word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 966K Jul 25 00:39 word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Jul 25 00:39 word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Jul 25 00:39 word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Jul 25 00:39 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-5-count.jsonl  5  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-10-count.jsonl 10 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-15-count.jsonl 15 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-20-count.jsonl 20 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-25-count.jsonl 25 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-40-count.jsonl 40 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-50-count.jsonl 50 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-60-count.jsonl 80 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-100-count.jsonl 100 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5f88d0a1db77f214/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 6278.90it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 342.92it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5f88d0a1db77f214/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 205.45it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-D-mem-finetune-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 126229482\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 126229482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230725_003931-envps9kr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTokenShift-D - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/envps9kr\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5f88d0a1db77f214/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 352.52it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5f88d0a1db77f214/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-be0a03bc4a0b9119_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5f88d0a1db77f214/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a057223c0cab7471_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 126229482                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-25 00:39:45,694] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 126229482\n",
      "[rank: 2] Global seed set to 126229482\n",
      "[rank: 5] Global seed set to 126229482\n",
      "[rank: 1] Global seed set to 126229482\n",
      "[rank: 7] Global seed set to 126229482\n",
      "[rank: 6] Global seed set to 126229482\n",
      "[rank: 4] Global seed set to 126229482\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 2] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-25 00:40:01,887] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-25 00:40:11,958] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-25 00:40:18,109] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-25 00:40:18,462] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-25 00:40:18,475] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-25 00:40:18,491] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 126229482\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-25 00:40:18,501] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06258130073547363 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10151481628417969 seconds\n",
      "Time to load fused_adam op: 0.1014401912689209 seconds\n",
      "Time to load fused_adam op: 0.10141992568969727 seconds\n",
      "Time to load fused_adam op: 0.10162806510925293 seconds\n",
      "Time to load fused_adam op: 0.10138678550720215 seconds\n",
      "Time to load fused_adam op: 0.10143136978149414 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10151910781860352 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.05994701385498047 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1018378734588623 seconds\n",
      "Time to load utils op: 0.1019587516784668 seconds\n",
      "Time to load utils op: 0.10151171684265137 seconds\n",
      "Time to load utils op: 0.10225129127502441 seconds\n",
      "Time to load utils op: 0.1019432544708252 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10168242454528809 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10180783271789551 seconds\n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002675056457519531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002689361572265625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002636909484863281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027060508728027344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002770423889160156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002810955047607422 seconds\n",
      "Time to load utils op: 0.0002865791320800781 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005040168762207031 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|â–| 800/4371 [07:23<32:58,  1.81it/s, v_num=s9kr, train/loss=4.620]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [40:39<00:00,  1.79it/s, v_num=s9kr, train/loss=0.305\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆ                | 1/5 [00:00<00:01,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 2/5 [00:00<00:00,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 3/5 [00:00<00:00,  4.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4/5 [00:00<00:00,  4.49it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [40:47<00:00,  1.79it/s, v_num=s9kr, train/loss=0.305\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [40:47<00:00,  1.79it/s, v_num=s9kr, train/loss=0.305\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [41:00<00:00,  1.78it/s, v_num=s9kr, train/loss=0.305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–‚â–â–„â–â–‚â–ƒâ–ƒâ–„â–‚â–†â–…â–ƒâ–ƒâ–ƒâ–â–„â–„â–ƒâ–â–‚â–‚â–â–â–â–‡â–‚â–â–â–ˆâ–â–â–â–ƒâ–‚â–‚â–â–â–â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–‡â–…â–ƒâ–ˆâ–†â–‡â–‡â–‚â–â–‚â–†â–ƒâ–…â–…â–â–‚â–â–‡â–†â–‚â–†â–„â–â–â–‡â–â–â–â–„â–‡â–â–ƒâ–‡â–â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 1.86719\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 1.38816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mTokenShift-D - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/envps9kr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230725_003931-envps9kr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-D-mem-finetune-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-D-Tune1.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 25 01:22 ../model/TokenShift-D-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-D-mem-finetune-1/last.ckpt\" \\\n",
    "        \"../model/TokenShift-D-Tune1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-D-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 85.0% similarity, with 17 matched token, and 3 token mismatch\n",
      "## Model validation for 25 tokens : 60.0% similarity, with 15 matched token, and 10 token mismatch\n",
      "## Model validation for 30 tokens : 53.333333333333336% similarity, with 16 matched token, and 14 token mismatch\n",
      "## Model validation for 35 tokens : 51.42857142857142% similarity, with 18 matched token, and 17 token mismatch\n",
      "## Model validation for 40 tokens : 47.5% similarity, with 19 matched token, and 21 token mismatch\n",
      "## Model validation for 45 tokens : 40.0% similarity, with 18 matched token, and 27 token mismatch\n",
      "## Model validation for 50 tokens : 42.0% similarity, with 21 matched token, and 29 token mismatch\n",
      "## Model validation for 55 tokens : 40.0% similarity, with 22 matched token, and 33 token mismatch\n",
      "## Model validation for 60 tokens : 36.666666666666664% similarity, with 22 matched token, and 38 token mismatch\n",
      "## Model validation for 65 tokens : 35.38461538461539% similarity, with 23 matched token, and 42 token mismatch\n",
      "## Model validation for 70 tokens : 31.428571428571427% similarity, with 22 matched token, and 48 token mismatch\n",
      "## Model validation for 75 tokens : 30.666666666666664% similarity, with 23 matched token, and 52 token mismatch\n",
      "## Model validation for 80 tokens : 28.749999999999996% similarity, with 23 matched token, and 57 token mismatch\n",
      "## Model validation for 85 tokens : 23.52941176470588% similarity, with 20 matched token, and 65 token mismatch\n",
      "## Model validation for 90 tokens : 25.555555555555554% similarity, with 23 matched token, and 67 token mismatch\n",
      "## Model validation for 95 tokens : 23.157894736842106% similarity, with 22 matched token, and 73 token mismatch\n",
      "## Model validation for 100 tokens : 24.0% similarity, with 24 matched token, and 76 token mismatch\n",
      "## Model validation for 105 tokens : 22.857142857142858% similarity, with 24 matched token, and 81 token mismatch\n",
      "## Model validation for 110 tokens : 18.181818181818183% similarity, with 20 matched token, and 90 token mismatch\n",
      "## Model validation for 115 tokens : 18.26086956521739% similarity, with 21 matched token, and 94 token mismatch\n",
      "## Model validation for 120 tokens : 16.666666666666664% similarity, with 20 matched token, and 100 token mismatch\n",
      "## Model validation for 125 tokens : 16.8% similarity, with 21 matched token, and 104 token mismatch\n",
      "## Model validation for 130 tokens : 16.153846153846153% similarity, with 21 matched token, and 109 token mismatch\n",
      "## Model validation for 135 tokens : 16.296296296296298% similarity, with 22 matched token, and 113 token mismatch\n",
      "## Model validation for 140 tokens : 14.285714285714285% similarity, with 20 matched token, and 120 token mismatch\n",
      "## Model validation for 145 tokens : 16.551724137931036% similarity, with 24 matched token, and 121 token mismatch\n",
      "## Model validation for 150 tokens : 14.666666666666666% similarity, with 22 matched token, and 128 token mismatch\n",
      "## Model validation for 160 tokens : 11.875% similarity, with 19 matched token, and 141 token mismatch\n",
      "## Model validation for 170 tokens : 11.76470588235294% similarity, with 20 matched token, and 150 token mismatch\n",
      "## Model validation for 180 tokens : 10.0% similarity, with 18 matched token, and 162 token mismatch\n",
      "## Model validation for 190 tokens : 10.0% similarity, with 19 matched token, and 171 token mismatch\n",
      "## Model validation for 200 tokens : 9.0% similarity, with 18 matched token, and 182 token mismatch\n",
      "## Model validation for 210 tokens : 9.047619047619047% similarity, with 19 matched token, and 191 token mismatch\n",
      "## Model validation for 220 tokens : 8.636363636363637% similarity, with 19 matched token, and 201 token mismatch\n",
      "## Model validation for 230 tokens : 9.130434782608695% similarity, with 21 matched token, and 209 token mismatch\n",
      "## Model validation for 240 tokens : 8.333333333333332% similarity, with 20 matched token, and 220 token mismatch\n",
      "## Model validation for 250 tokens : 8.4% similarity, with 21 matched token, and 229 token mismatch\n",
      "## Model validation for 260 tokens : 8.076923076923077% similarity, with 21 matched token, and 239 token mismatch\n",
      "## Model validation for 270 tokens : 7.777777777777778% similarity, with 21 matched token, and 249 token mismatch\n",
      "## Model validation for 280 tokens : 7.142857142857142% similarity, with 20 matched token, and 260 token mismatch\n",
      "## Model validation for 290 tokens : 7.241379310344828% similarity, with 21 matched token, and 269 token mismatch\n",
      "## Model validation for 300 tokens : 6.666666666666667% similarity, with 20 matched token, and 280 token mismatch\n",
      "## Model validation for 325 tokens : 6.153846153846154% similarity, with 20 matched token, and 305 token mismatch\n",
      "## Model validation for 350 tokens : 6.0% similarity, with 21 matched token, and 329 token mismatch\n",
      "## Model validation for 375 tokens : 5.6000000000000005% similarity, with 21 matched token, and 354 token mismatch\n",
      "## Model validation for 400 tokens : 4.75% similarity, with 19 matched token, and 381 token mismatch\n",
      "## Model validation for 425 tokens : 4.705882352941177% similarity, with 20 matched token, and 405 token mismatch\n",
      "## Model validation for 450 tokens : 4.0% similarity, with 18 matched token, and 432 token mismatch\n",
      "## Model validation for 475 tokens : 3.7894736842105265% similarity, with 18 matched token, and 457 token mismatch\n",
      "## Model validation for 500 tokens : 3.8% similarity, with 19 matched token, and 481 token mismatch\n",
      "## Model validation for 525 tokens : 3.428571428571429% similarity, with 18 matched token, and 507 token mismatch\n",
      "## Model validation for 550 tokens : 3.272727272727273% similarity, with 18 matched token, and 532 token mismatch\n",
      "## Model validation for 575 tokens : 2.9565217391304346% similarity, with 17 matched token, and 558 token mismatch\n",
      "## Model validation for 600 tokens : 2.833333333333333% similarity, with 17 matched token, and 583 token mismatch\n",
      "## Model validation for 625 tokens : 2.88% similarity, with 18 matched token, and 607 token mismatch\n",
      "## Model validation for 650 tokens : 2.769230769230769% similarity, with 18 matched token, and 632 token mismatch\n",
      "## Model validation for 675 tokens : 2.666666666666667% similarity, with 18 matched token, and 657 token mismatch\n",
      "## Model validation for 700 tokens : 2.571428571428571% similarity, with 18 matched token, and 682 token mismatch\n",
      "## Model validation for 750 tokens : 2.533333333333333% similarity, with 19 matched token, and 731 token mismatch\n",
      "## Model validation for 800 tokens : 2.375% similarity, with 19 matched token, and 781 token mismatch\n",
      "## Model validation for 850 tokens : 2.3529411764705883% similarity, with 20 matched token, and 830 token mismatch\n",
      "## Model validation for 900 tokens : 2.3333333333333335% similarity, with 21 matched token, and 879 token mismatch\n",
      "## Model validation for 950 tokens : 2.4210526315789473% similarity, with 23 matched token, and 927 token mismatch\n",
      "## Model validation for 1000 tokens : 2.4% similarity, with 24 matched token, and 976 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval\n",
    "#\n",
    "# Note that the expected performance \"is not that great\", as the model seems to be only loosely\n",
    "# learning the memorization task, and the instruction propmt. And is seem to be acting more\n",
    "# like an RNG based on the instruct. (Instead of the actual memorization task)\n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-D-Tune1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 3189 samples (30 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 683 samples (50 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 1328 samples (50 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 5238 samples (20 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 2619 samples (50 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 3557 samples (20 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 1769 samples (50 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 79M\n",
      "drwxr-xr-x  2 root root 4.0K Jul 25 01:38 .\n",
      "drwxr-xr-x 13 root root  248 Jul 24 13:07 ..\n",
      "-rw-r--r--  1 root root 988K Jul 25 01:38 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Jul 25 01:38 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Jul 25 01:38 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Jul 25 01:38 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Jul 25 01:38 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Jul 25 01:38 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Jul 25 01:38 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 2.6M Jul 25 01:38 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 726K Jul 25 01:38 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Jul 25 01:38 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Jul 25 01:38 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 3.4M Jul 25 01:38 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 3.6M Jul 25 01:38 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Jul 25 01:38 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Jul 25 01:38 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Jul 25 01:38 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Jul 25 01:38 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Jul 25 01:38 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Jul 25 01:38 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 1.1M Jul 25 01:38 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Jul 25 01:38 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 850K Jul 25 01:38 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Jul 25 01:38 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 1.1M Jul 25 01:38 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Jul 25 01:38 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Jul 25 01:38 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Jul 25 01:38 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 599K Jul 25 01:38 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root  10M Jul 25 01:38 word-200-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-100-count.jsonl 100 5000 &\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 59421.01it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a5ca6a42f9a28314/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2082.57it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 128.07it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a5ca6a42f9a28314/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 197.41it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-D-mem-finetune-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 579741080\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 579741080\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230725_013840-6zdb6gci\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTokenShift-D - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/6zdb6gci\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 65926.73it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a5ca6a42f9a28314/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 135.29it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a5ca6a42f9a28314/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-89e7c9db683714a6_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a5ca6a42f9a28314/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b5dd54595a888f5e_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 579741080                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-25 01:38:55,409] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 579741080\n",
      "[rank: 1] Global seed set to 579741080\n",
      "[rank: 6] Global seed set to 579741080\n",
      "[rank: 4] Global seed set to 579741080\n",
      "[rank: 3] Global seed set to 579741080\n",
      "[rank: 2] Global seed set to 579741080\n",
      "[rank: 7] Global seed set to 579741080\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 5] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-25 01:39:14,532] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-25 01:39:21,973] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-25 01:39:28,190] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-25 01:39:28,301] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-25 01:39:28,305] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-25 01:39:28,310] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 579741080\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-25 01:39:28,318] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06371188163757324 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10134434700012207 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10144495964050293 seconds\n",
      "Time to load fused_adam op: 0.10126948356628418 seconds\n",
      "Time to load fused_adam op: 0.10127544403076172 seconds\n",
      "Time to load fused_adam op: 0.10126042366027832 seconds\n",
      "Time to load fused_adam op: 0.10149955749511719 seconds\n",
      "Time to load fused_adam op: 0.10162782669067383 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0689387321472168 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10149312019348145 seconds\n",
      "Time to load utils op: 0.10169577598571777 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10181546211242676 seconds\n",
      "Time to load utils op: 0.10176658630371094 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10195088386535645 seconds\n",
      "Time to load utils op: 0.10189175605773926 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1021573543548584 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00028395652770996094 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002696514129638672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026607513427734375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002841949462890625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002090930938720703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00029087066650390625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003707408905029297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004987716674804688 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/16032 [07:24<2:20:56,  1.80it/s, v_num=6gci, train/loss=0.8/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 16032/16032 [2:29:38<00:00,  1.79it/s, v_num=6gci, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 1/17 [00:00<00:04,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 2/17 [00:00<00:03,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–Ž               | 3/17 [00:00<00:03,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 4/17 [00:00<00:02,  4.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 5/17 [00:01<00:02,  4.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 6/17 [00:01<00:02,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 7/17 [00:01<00:02,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 8/17 [00:01<00:01,  4.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 9/17 [00:01<00:01,  4.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 10/17 [00:02<00:01,  4.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 11/17 [00:02<00:01,  4.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12/17 [00:02<00:01,  4.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13/17 [00:02<00:00,  4.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 14/17 [00:02<00:00,  4.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/17 [00:03<00:00,  4.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/17 [00:03<00:00,  4.78it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16032/16032 [2:29:48<00:00,  1.78it/s, v_num=6gci, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16032/16032 [2:29:48<00:00,  1.78it/s, v_num=6gci, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 16032/16032 [2:30:07<00:00,  1.78it/s, v_num=6gci, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–ƒâ–ƒâ–‚â–â–‚â–„â–‚â–â–‚â–ˆâ–‚â–â–‚â–‚â–ƒâ–â–‚â–â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–ƒâ–†â–†â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.08594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mTokenShift-D - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/6zdb6gci\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230725_013840-6zdb6gci/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-D-mem-finetune-2/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-D-Tune2.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 25 04:36 ../model/TokenShift-D-Tune2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-D-mem-finetune-2/last.ckpt\" \\\n",
    "        \"../model/TokenShift-D-Tune2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-D-Tune2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 97.14285714285714% similarity, with 68 matched token, and 2 token mismatch\n",
      "## Model validation for 75 tokens : 97.33333333333334% similarity, with 73 matched token, and 2 token mismatch\n",
      "## Model validation for 80 tokens : 95.0% similarity, with 76 matched token, and 4 token mismatch\n",
      "## Model validation for 85 tokens : 95.29411764705881% similarity, with 81 matched token, and 4 token mismatch\n",
      "## Model validation for 90 tokens : 94.44444444444444% similarity, with 85 matched token, and 5 token mismatch\n",
      "## Model validation for 95 tokens : 92.63157894736842% similarity, with 88 matched token, and 7 token mismatch\n",
      "## Model validation for 100 tokens : 90.0% similarity, with 90 matched token, and 10 token mismatch\n",
      "## Model validation for 105 tokens : 88.57142857142857% similarity, with 93 matched token, and 12 token mismatch\n",
      "## Model validation for 110 tokens : 80.9090909090909% similarity, with 89 matched token, and 21 token mismatch\n",
      "## Model validation for 115 tokens : 80.0% similarity, with 92 matched token, and 23 token mismatch\n",
      "## Model validation for 120 tokens : 77.5% similarity, with 93 matched token, and 27 token mismatch\n",
      "## Model validation for 125 tokens : 76.0% similarity, with 95 matched token, and 30 token mismatch\n",
      "## Model validation for 130 tokens : 75.38461538461539% similarity, with 98 matched token, and 32 token mismatch\n",
      "## Model validation for 135 tokens : 71.11111111111111% similarity, with 96 matched token, and 39 token mismatch\n",
      "## Model validation for 140 tokens : 70.0% similarity, with 98 matched token, and 42 token mismatch\n",
      "## Model validation for 145 tokens : 66.89655172413794% similarity, with 97 matched token, and 48 token mismatch\n",
      "## Model validation for 150 tokens : 67.33333333333333% similarity, with 101 matched token, and 49 token mismatch\n",
      "## Model validation for 160 tokens : 65.625% similarity, with 105 matched token, and 55 token mismatch\n",
      "## Model validation for 170 tokens : 62.94117647058823% similarity, with 107 matched token, and 63 token mismatch\n",
      "## Model validation for 180 tokens : 56.666666666666664% similarity, with 102 matched token, and 78 token mismatch\n",
      "## Model validation for 190 tokens : 54.21052631578947% similarity, with 103 matched token, and 87 token mismatch\n",
      "## Model validation for 200 tokens : 42.5% similarity, with 85 matched token, and 115 token mismatch\n",
      "## Model validation for 210 tokens : 38.095238095238095% similarity, with 80 matched token, and 130 token mismatch\n",
      "## Model validation for 220 tokens : 36.81818181818181% similarity, with 81 matched token, and 139 token mismatch\n",
      "## Model validation for 230 tokens : 33.91304347826087% similarity, with 78 matched token, and 152 token mismatch\n",
      "## Model validation for 240 tokens : 32.083333333333336% similarity, with 77 matched token, and 163 token mismatch\n",
      "## Model validation for 250 tokens : 29.599999999999998% similarity, with 74 matched token, and 176 token mismatch\n",
      "## Model validation for 260 tokens : 26.923076923076923% similarity, with 70 matched token, and 190 token mismatch\n",
      "## Model validation for 270 tokens : 24.814814814814813% similarity, with 67 matched token, and 203 token mismatch\n",
      "## Model validation for 280 tokens : 24.642857142857146% similarity, with 69 matched token, and 211 token mismatch\n",
      "## Model validation for 290 tokens : 22.758620689655174% similarity, with 66 matched token, and 224 token mismatch\n",
      "## Model validation for 300 tokens : 20.0% similarity, with 60 matched token, and 240 token mismatch\n",
      "## Model validation for 325 tokens : 16.923076923076923% similarity, with 55 matched token, and 270 token mismatch\n",
      "## Model validation for 350 tokens : 14.857142857142858% similarity, with 52 matched token, and 298 token mismatch\n",
      "## Model validation for 375 tokens : 14.399999999999999% similarity, with 54 matched token, and 321 token mismatch\n",
      "## Model validation for 400 tokens : 13.0% similarity, with 52 matched token, and 348 token mismatch\n",
      "## Model validation for 425 tokens : 12.0% similarity, with 51 matched token, and 374 token mismatch\n",
      "## Model validation for 450 tokens : 11.11111111111111% similarity, with 50 matched token, and 400 token mismatch\n",
      "## Model validation for 475 tokens : 10.105263157894736% similarity, with 48 matched token, and 427 token mismatch\n",
      "## Model validation for 500 tokens : 9.4% similarity, with 47 matched token, and 453 token mismatch\n",
      "## Model validation for 525 tokens : 8.761904761904763% similarity, with 46 matched token, and 479 token mismatch\n",
      "## Model validation for 550 tokens : 8.363636363636363% similarity, with 46 matched token, and 504 token mismatch\n",
      "## Model validation for 575 tokens : 7.652173913043478% similarity, with 44 matched token, and 531 token mismatch\n",
      "## Model validation for 600 tokens : 7.166666666666667% similarity, with 43 matched token, and 557 token mismatch\n",
      "## Model validation for 625 tokens : 6.88% similarity, with 43 matched token, and 582 token mismatch\n",
      "## Model validation for 650 tokens : 6.615384615384616% similarity, with 43 matched token, and 607 token mismatch\n",
      "## Model validation for 675 tokens : 6.222222222222222% similarity, with 42 matched token, and 633 token mismatch\n",
      "## Model validation for 700 tokens : 5.857142857142858% similarity, with 41 matched token, and 659 token mismatch\n",
      "## Model validation for 750 tokens : 5.6000000000000005% similarity, with 42 matched token, and 708 token mismatch\n",
      "## Model validation for 800 tokens : 5.25% similarity, with 42 matched token, and 758 token mismatch\n",
      "## Model validation for 850 tokens : 4.705882352941177% similarity, with 40 matched token, and 810 token mismatch\n",
      "## Model validation for 900 tokens : 4.444444444444445% similarity, with 40 matched token, and 860 token mismatch\n",
      "## Model validation for 950 tokens : 3.8947368421052633% similarity, with 37 matched token, and 913 token mismatch\n",
      "## Model validation for 1000 tokens : 3.5999999999999996% similarity, with 36 matched token, and 964 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# While not at its full potential, its memory ability should start emerging\n",
    "#\n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-D-Tune2.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Ramping up the ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Mid ctx size (1024), same as tune 2, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 5 max words, 1000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 648 samples (10 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 1000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 1000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 1000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 1000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 314 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 1307 samples (10 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 879 samples (10 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 1000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 345 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 582 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 821 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 755 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 766 samples (10 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 1057 samples (10 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 593 samples (10 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 1778 samples (10 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 660 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 5555 samples (10 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 214 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 562 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 1065 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 1000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated a single JSONL file with 628 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 1000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 1000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 2596 samples (10 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 524 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 879 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 204 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 290 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 369 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 957 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 301 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 707 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 399 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 195 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 381 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 335 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 191 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 211 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 191 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 406 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 114 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 353 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 287 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 280 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated a single JSONL file with 190 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 284 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 306 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 202 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "## Done ##\n",
      "total 450M\n",
      "drwxr-xr-x  2 root root 8.0K Jul 25 05:43 .\n",
      "drwxr-xr-x 13 root root  248 Jul 24 13:07 ..\n",
      "-rw-r--r--  1 root root 197K Jul 25 05:43 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.1M Jul 25 05:43 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Jul 25 05:43 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Jul 25 05:43 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Jul 25 05:43 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 2.5M Jul 25 05:43 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 2.6M Jul 25 05:43 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 2.7M Jul 25 05:43 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Jul 25 05:43 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Jul 25 05:43 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 3.0M Jul 25 05:43 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 242K Jul 25 05:43 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Jul 25 05:43 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 3.2M Jul 25 05:43 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 3.3M Jul 25 05:43 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 3.4M Jul 25 05:43 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Jul 25 05:43 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Jul 25 05:43 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 3.7M Jul 25 05:43 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 3.7M Jul 25 05:43 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Jul 25 05:43 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 3.9M Jul 25 05:43 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 295K Jul 25 05:43 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Jul 25 05:43 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Jul 25 05:43 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 4.2M Jul 25 05:43 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Jul 25 05:43 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 4.4M Jul 25 05:43 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Jul 25 05:43 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 4.6M Jul 25 05:43 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 4.7M Jul 25 05:43 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Jul 25 05:43 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 4.9M Jul 25 05:43 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root 337K Jul 25 05:43 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Jul 25 05:43 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 5.1M Jul 25 05:43 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Jul 25 05:43 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 5.3M Jul 25 05:43 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 5.4M Jul 25 05:43 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Jul 25 05:43 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Jul 25 05:43 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 5.6M Jul 25 05:43 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 5.7M Jul 25 05:43 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 5.8M Jul 25 05:43 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root 390K Jul 25 05:43 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 5.9M Jul 25 05:43 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 6.0M Jul 25 05:43 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 6.1M Jul 25 05:43 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 6.2M Jul 25 05:43 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 6.3M Jul 25 05:43 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 6.4M Jul 25 05:43 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 6.5M Jul 25 05:43 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 6.6M Jul 25 05:43 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 6.7M Jul 25 05:43 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 6.8M Jul 25 05:43 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root 434K Jul 25 05:43 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 6.9M Jul 25 05:43 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 7.0M Jul 25 05:43 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 7.1M Jul 25 05:43 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Jul 25 05:43 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Jul 25 05:43 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 7.3M Jul 25 05:43 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 7.5M Jul 25 05:43 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 7.5M Jul 25 05:43 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 7.6M Jul 25 05:43 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Jul 25 05:43 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root 484K Jul 25 05:43 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 7.8M Jul 25 05:43 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 7.9M Jul 25 05:43 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 8.0M Jul 25 05:43 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 8.1M Jul 25 05:43 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 8.2M Jul 25 05:43 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 8.3M Jul 25 05:43 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 8.4M Jul 25 05:43 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 8.5M Jul 25 05:43 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 8.6M Jul 25 05:43 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 8.7M Jul 25 05:43 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 8.8M Jul 25 05:43 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 146K Jul 25 05:43 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Jul 25 05:43 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 1.3M Jul 25 05:43 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Jul 25 05:43 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Jul 25 05:43 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Jul 25 05:43 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Jul 25 05:43 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Jul 25 05:43 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Jul 25 05:43 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Jul 25 05:43 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Jul 25 05:43 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 05:43 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 569K Jul 25 05:43 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 555K Jul 25 05:43 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 556K Jul 25 05:43 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 550K Jul 25 05:43 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 552K Jul 25 05:43 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Jul 25 05:43 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 549K Jul 25 05:43 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 05:43 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Jul 25 05:43 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 546K Jul 25 05:43 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 433K Jul 25 05:43 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Jul 25 05:43 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 550K Jul 25 05:43 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Jul 25 05:43 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Jul 25 05:43 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 05:43 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Jul 25 05:43 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 05:43 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Jul 25 05:43 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 05:43 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 548K Jul 25 05:43 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 384K Jul 25 05:43 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 05:43 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Jul 25 05:43 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 537K Jul 25 05:43 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 537K Jul 25 05:43 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 538K Jul 25 05:43 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 05:43 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Jul 25 05:43 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Jul 25 05:43 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 05:43 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root 353K Jul 25 05:43 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 05:43 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Jul 25 05:43 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 05:43 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 05:43 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Jul 25 05:43 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 05:43 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Jul 25 05:43 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root 347K Jul 25 05:43 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 05:43 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 05:43 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 05:43 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 05:43 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 05:43 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 05:43 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root 330K Jul 25 05:43 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 05:43 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 05:43 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 05:43 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 05:43 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 05:43 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 05:43 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 05:43 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root 315K Jul 25 05:43 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 05:43 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 05:43 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 05:43 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 05:43 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 05:43 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 05:43 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 05:43 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 05:43 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root 311K Jul 25 05:43 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 05:43 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 826K Jul 25 05:43 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 614K Jul 25 05:43 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 612K Jul 25 05:43 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 591K Jul 25 05:43 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 597K Jul 25 05:43 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 593K Jul 25 05:43 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 579K Jul 25 05:43 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 574K Jul 25 05:43 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 573K Jul 25 05:43 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 574K Jul 25 05:43 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 568K Jul 25 05:43 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 120K Jul 25 05:43 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {50..450..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<00:00, 47394.74it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d1a304ae1026187a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 625.27it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.54it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d1a304ae1026187a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 83.80it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-3.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-D-mem-finetune-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1289647921\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1289647921\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230725_054404-ur4a8zsu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTokenShift-D - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ur4a8zsu\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<00:00, 257782.35it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d1a304ae1026187a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 106.73it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d1a304ae1026187a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-17f53e4e2d3d7550_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d1a304ae1026187a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f88f77ba946a5c76_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 1289647921                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-25 05:44:20,275] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 1289647921\n",
      "[rank: 1] Global seed set to 1289647921\n",
      "[rank: 5] Global seed set to 1289647921\n",
      "[rank: 6] Global seed set to 1289647921\n",
      "[rank: 7] Global seed set to 1289647921\n",
      "[rank: 4] Global seed set to 1289647921\n",
      "[rank: 3] Global seed set to 1289647921\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 5] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-25 05:44:51,716] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-25 05:44:55,481] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-25 05:44:55,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-25 05:44:55,717] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1289647921\n",
      "[rank: 6] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-25 05:44:55,856] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-25 05:44:55,857] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1289647921\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-25 05:44:55,863] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05971956253051758 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10137629508972168 seconds\n",
      "Time to load fused_adam op: 0.10159850120544434 seconds\n",
      "Time to load fused_adam op: 0.10156488418579102 seconds\n",
      "Time to load fused_adam op: 0.1018056869506836 seconds\n",
      "Time to load fused_adam op: 0.10160589218139648 seconds\n",
      "Time to load fused_adam op: 0.10170125961303711 seconds\n",
      "Time to load fused_adam op: 0.10155415534973145 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06690621376037598 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10188579559326172 seconds\n",
      "Time to load utils op: 0.10149407386779785 seconds\n",
      "Time to load utils op: 0.10141158103942871 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10211730003356934 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10188055038452148 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10190916061401367 seconds\n",
      "Time to load utils op: 0.1018517017364502 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028777122497558594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000286102294921875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002155303955078125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000274658203125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002732276916503906 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027179718017578125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004975795745849609 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 800/26158 [07:33<3:59:46,  1.76it/s, v_num=8zsu, train/loss=0.3/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 26158/26158 [4:09:41<00:00,  1.75it/s, v_num=8zsu, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                  | 1/27 [00:00<00:06,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|â–ˆâ–                 | 2/27 [00:00<00:06,  4.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–ˆ                 | 3/27 [00:00<00:05,  4.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–Š                | 4/27 [00:00<00:05,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–Œ               | 5/27 [00:01<00:04,  4.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 6/27 [00:01<00:04,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 7/27 [00:01<00:04,  4.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 8/27 [00:01<00:04,  4.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 9/27 [00:02<00:04,  4.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 10/27 [00:02<00:03,  4.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 11/27 [00:02<00:03,  4.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 12/27 [00:02<00:03,  4.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/27 [00:02<00:03,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 14/27 [00:03<00:02,  4.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 15/27 [00:03<00:02,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 16/27 [00:03<00:02,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 17/27 [00:03<00:02,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 18/27 [00:03<00:01,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/27 [00:04<00:01,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/27 [00:04<00:01,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/27 [00:04<00:01,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 22/27 [00:04<00:01,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 23/27 [00:04<00:00,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/27 [00:05<00:00,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 25/27 [00:05<00:00,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 26/27 [00:05<00:00,  4.65it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 26158/26158 [4:09:54<00:00,  1.74it/s, v_num=8zsu, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 26158/26158 [4:09:54<00:00,  1.74it/s, v_num=8zsu, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 26158/26158 [4:10:06<00:00,  1.74it/s, v_num=8zsu, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–†â–‡â–†â–‡â–„â–‚â–â–†â–„â–…â–ƒâ–…â–…â–…â–ˆâ–†â–„â–‚â–‡â–„â–ƒâ–ƒâ–ˆâ–…â–ƒâ–†â–‡â–‡â–„â–‚â–â–‚â–„â–ƒâ–ˆâ–ˆâ–‚â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–‡â–‡â–â–â–†â–â–â–„â–â–„â–ƒâ–‚â–‚â–‚â–â–‚â–â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.08045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mTokenShift-D - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ur4a8zsu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230725_054404-ur4a8zsu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-3.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-3 (bs=256, train-ctx=1024, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-D-mem-finetune-3/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-D-Tune3.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 25 09:56 ../model/TokenShift-D-Tune3.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-D-mem-finetune-3/last.ckpt\" \\\n",
    "        \"../model/TokenShift-D-Tune3.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-D-Tune3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 99.16666666666667% similarity, with 119 matched token, and 1 token mismatch\n",
      "## Model validation for 125 tokens : 99.2% similarity, with 124 matched token, and 1 token mismatch\n",
      "## Model validation for 130 tokens : 99.23076923076923% similarity, with 129 matched token, and 1 token mismatch\n",
      "## Model validation for 135 tokens : 99.25925925925925% similarity, with 134 matched token, and 1 token mismatch\n",
      "## Model validation for 140 tokens : 99.28571428571429% similarity, with 139 matched token, and 1 token mismatch\n",
      "## Model validation for 145 tokens : 99.3103448275862% similarity, with 144 matched token, and 1 token mismatch\n",
      "## Model validation for 150 tokens : 99.33333333333333% similarity, with 149 matched token, and 1 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.94736842105263% similarity, with 188 matched token, and 2 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 99.0909090909091% similarity, with 218 matched token, and 2 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 98.75% similarity, with 237 matched token, and 3 token mismatch\n",
      "## Model validation for 250 tokens : 98.8% similarity, with 247 matched token, and 3 token mismatch\n",
      "## Model validation for 260 tokens : 98.84615384615385% similarity, with 257 matched token, and 3 token mismatch\n",
      "## Model validation for 270 tokens : 98.51851851851852% similarity, with 266 matched token, and 4 token mismatch\n",
      "## Model validation for 280 tokens : 97.85714285714285% similarity, with 274 matched token, and 6 token mismatch\n",
      "## Model validation for 290 tokens : 97.24137931034483% similarity, with 282 matched token, and 8 token mismatch\n",
      "## Model validation for 300 tokens : 96.66666666666667% similarity, with 290 matched token, and 10 token mismatch\n",
      "## Model validation for 325 tokens : 94.15384615384616% similarity, with 306 matched token, and 19 token mismatch\n",
      "## Model validation for 350 tokens : 90.0% similarity, with 315 matched token, and 35 token mismatch\n",
      "## Model validation for 375 tokens : 83.2% similarity, with 312 matched token, and 63 token mismatch\n",
      "## Model validation for 400 tokens : 76.25% similarity, with 305 matched token, and 95 token mismatch\n",
      "## Model validation for 425 tokens : 71.52941176470588% similarity, with 304 matched token, and 121 token mismatch\n",
      "## Model validation for 450 tokens : 63.33333333333333% similarity, with 285 matched token, and 165 token mismatch\n",
      "## Model validation for 475 tokens : 56.21052631578948% similarity, with 267 matched token, and 208 token mismatch\n",
      "## Model validation for 500 tokens : 48.4% similarity, with 242 matched token, and 258 token mismatch\n",
      "## Model validation for 525 tokens : 42.47619047619048% similarity, with 223 matched token, and 302 token mismatch\n",
      "## Model validation for 550 tokens : 38.72727272727273% similarity, with 213 matched token, and 337 token mismatch\n",
      "## Model validation for 575 tokens : 34.43478260869565% similarity, with 198 matched token, and 377 token mismatch\n",
      "## Model validation for 600 tokens : 30.666666666666664% similarity, with 184 matched token, and 416 token mismatch\n",
      "## Model validation for 625 tokens : 27.200000000000003% similarity, with 170 matched token, and 455 token mismatch\n",
      "## Model validation for 650 tokens : 23.846153846153847% similarity, with 155 matched token, and 495 token mismatch\n",
      "## Model validation for 675 tokens : 21.48148148148148% similarity, with 145 matched token, and 530 token mismatch\n",
      "## Model validation for 700 tokens : 18.857142857142858% similarity, with 132 matched token, and 568 token mismatch\n",
      "## Model validation for 750 tokens : 15.866666666666667% similarity, with 119 matched token, and 631 token mismatch\n",
      "## Model validation for 800 tokens : 13.0% similarity, with 104 matched token, and 696 token mismatch\n",
      "## Model validation for 850 tokens : 10.705882352941176% similarity, with 91 matched token, and 759 token mismatch\n",
      "## Model validation for 900 tokens : 9.11111111111111% similarity, with 82 matched token, and 818 token mismatch\n",
      "## Model validation for 950 tokens : 8.0% similarity, with 76 matched token, and 874 token mismatch\n",
      "## Model validation for 1000 tokens : 7.3999999999999995% similarity, with 74 matched token, and 926 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# We should start approaching the full potential of the model, unless its able to exceed 250 tokens of memory\n",
    "#\n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-D-Tune3.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 4 : Ramping up the ctx size (4096), memory training\n",
    "\n",
    "- Tune 4: Mid ctx size (4096), same as tune 4, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits (supposingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 175 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 56 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 130 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 259 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 72 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 104 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 549 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 408 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 88 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 711 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 661 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 961 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 285 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 1068 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 555 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 211 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 752 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 271 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 817 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 527 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 889 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 591 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 299 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 333 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 381 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 310 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 623 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 370 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 138 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 198 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 398 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1550 max words - at ../dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1445 max words - at ../dataset/shuffle-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1580 max words - at ../dataset/shuffle-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1025 max words - at ../dataset/shuffle-word-1025-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1215 max words - at ../dataset/shuffle-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 318 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1225 max words - at ../dataset/shuffle-word-1225-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 355 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1165 max words - at ../dataset/shuffle-word-1165-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1015 max words - at ../dataset/shuffle-word-1015-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 83 samples (20 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 205 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1035 max words - at ../dataset/shuffle-word-1035-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1595 max words - at ../dataset/shuffle-word-1595-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1565 max words - at ../dataset/shuffle-word-1565-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1085 max words - at ../dataset/shuffle-word-1085-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1095 max words - at ../dataset/shuffle-word-1095-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated a single JSONL file with 279 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1635 max words - at ../dataset/shuffle-word-1635-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1005 max words - at ../dataset/shuffle-word-1005-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1685 max words - at ../dataset/shuffle-word-1685-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 114 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 83 samples (20 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1815 max words - at ../dataset/shuffle-word-1815-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 194 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 343 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 282 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1525 max words - at ../dataset/shuffle-word-1525-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1945 max words - at ../dataset/shuffle-word-1945-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1475 max words - at ../dataset/shuffle-word-1475-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1245 max words - at ../dataset/shuffle-word-1245-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1455 max words - at ../dataset/shuffle-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1395 max words - at ../dataset/shuffle-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1385 max words - at ../dataset/shuffle-word-1385-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1275 max words - at ../dataset/shuffle-word-1275-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1810 max words - at ../dataset/shuffle-word-1810-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1855 max words - at ../dataset/shuffle-word-1855-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1345 max words - at ../dataset/shuffle-word-1345-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1950 max words - at ../dataset/shuffle-word-1950-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1625 max words - at ../dataset/shuffle-word-1625-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1465 max words - at ../dataset/shuffle-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 292 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 147 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1510 max words - at ../dataset/shuffle-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1680 max words - at ../dataset/shuffle-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1255 max words - at ../dataset/shuffle-word-1255-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1505 max words - at ../dataset/shuffle-word-1505-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1295 max words - at ../dataset/shuffle-word-1295-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1790 max words - at ../dataset/shuffle-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1710 max words - at ../dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 54 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1775 max words - at ../dataset/shuffle-word-1775-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 220 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1750 max words - at ../dataset/shuffle-word-1750-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1715 max words - at ../dataset/shuffle-word-1715-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1235 max words - at ../dataset/shuffle-word-1235-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1915 max words - at ../dataset/shuffle-word-1915-count.jsonl\n",
      "Generated a single JSONL file with 44 samples (20 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1630 max words - at ../dataset/shuffle-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1195 max words - at ../dataset/shuffle-word-1195-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1585 max words - at ../dataset/shuffle-word-1585-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1175 max words - at ../dataset/shuffle-word-1175-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1435 max words - at ../dataset/shuffle-word-1435-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1530 max words - at ../dataset/shuffle-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1355 max words - at ../dataset/shuffle-word-1355-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1560 max words - at ../dataset/shuffle-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1375 max words - at ../dataset/shuffle-word-1375-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1405 max words - at ../dataset/shuffle-word-1405-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1335 max words - at ../dataset/shuffle-word-1335-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1985 max words - at ../dataset/shuffle-word-1985-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1555 max words - at ../dataset/shuffle-word-1555-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1990 max words - at ../dataset/shuffle-word-1990-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1115 max words - at ../dataset/shuffle-word-1115-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1285 max words - at ../dataset/shuffle-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1205 max words - at ../dataset/shuffle-word-1205-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1075 max words - at ../dataset/shuffle-word-1075-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1820 max words - at ../dataset/shuffle-word-1820-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1155 max words - at ../dataset/shuffle-word-1155-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1605 max words - at ../dataset/shuffle-word-1605-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1125 max words - at ../dataset/shuffle-word-1125-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1795 max words - at ../dataset/shuffle-word-1795-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1965 max words - at ../dataset/shuffle-word-1965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1425 max words - at ../dataset/shuffle-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1940 max words - at ../dataset/shuffle-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1885 max words - at ../dataset/shuffle-word-1885-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1920 max words - at ../dataset/shuffle-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1325 max words - at ../dataset/shuffle-word-1325-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1845 max words - at ../dataset/shuffle-word-1845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1880 max words - at ../dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1265 max words - at ../dataset/shuffle-word-1265-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1705 max words - at ../dataset/shuffle-word-1705-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1185 max words - at ../dataset/shuffle-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1045 max words - at ../dataset/shuffle-word-1045-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1145 max words - at ../dataset/shuffle-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (20 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1740 max words - at ../dataset/shuffle-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1675 max words - at ../dataset/shuffle-word-1675-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1660 max words - at ../dataset/shuffle-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1515 max words - at ../dataset/shuffle-word-1515-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1590 max words - at ../dataset/shuffle-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1650 max words - at ../dataset/shuffle-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1850 max words - at ../dataset/shuffle-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1615 max words - at ../dataset/shuffle-word-1615-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1610 max words - at ../dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1690 max words - at ../dataset/shuffle-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 2000 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1575 max words - at ../dataset/shuffle-word-1575-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1545 max words - at ../dataset/shuffle-word-1545-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1135 max words - at ../dataset/shuffle-word-1135-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1570 max words - at ../dataset/shuffle-word-1570-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1955 max words - at ../dataset/shuffle-word-1955-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1670 max words - at ../dataset/shuffle-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1930 max words - at ../dataset/shuffle-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1055 max words - at ../dataset/shuffle-word-1055-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1105 max words - at ../dataset/shuffle-word-1105-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1415 max words - at ../dataset/shuffle-word-1415-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1065 max words - at ../dataset/shuffle-word-1065-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1485 max words - at ../dataset/shuffle-word-1485-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1495 max words - at ../dataset/shuffle-word-1495-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1620 max words - at ../dataset/shuffle-word-1620-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1640 max words - at ../dataset/shuffle-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1535 max words - at ../dataset/shuffle-word-1535-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1960 max words - at ../dataset/shuffle-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1520 max words - at ../dataset/shuffle-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1695 max words - at ../dataset/shuffle-word-1695-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1665 max words - at ../dataset/shuffle-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1540 max words - at ../dataset/shuffle-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1860 max words - at ../dataset/shuffle-word-1860-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1760 max words - at ../dataset/shuffle-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1305 max words - at ../dataset/shuffle-word-1305-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1785 max words - at ../dataset/shuffle-word-1785-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1645 max words - at ../dataset/shuffle-word-1645-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1365 max words - at ../dataset/shuffle-word-1365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1875 max words - at ../dataset/shuffle-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1935 max words - at ../dataset/shuffle-word-1935-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1780 max words - at ../dataset/shuffle-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1830 max words - at ../dataset/shuffle-word-1830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1995 max words - at ../dataset/shuffle-word-1995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1970 max words - at ../dataset/shuffle-word-1970-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1895 max words - at ../dataset/shuffle-word-1895-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1805 max words - at ../dataset/shuffle-word-1805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1870 max words - at ../dataset/shuffle-word-1870-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1765 max words - at ../dataset/shuffle-word-1765-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1730 max words - at ../dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1975 max words - at ../dataset/shuffle-word-1975-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1655 max words - at ../dataset/shuffle-word-1655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1905 max words - at ../dataset/shuffle-word-1905-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1865 max words - at ../dataset/shuffle-word-1865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1840 max words - at ../dataset/shuffle-word-1840-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 2000 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 2000 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1890 max words - at ../dataset/shuffle-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1745 max words - at ../dataset/shuffle-word-1745-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1980 max words - at ../dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1315 max words - at ../dataset/shuffle-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1735 max words - at ../dataset/shuffle-word-1735-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1725 max words - at ../dataset/shuffle-word-1725-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1770 max words - at ../dataset/shuffle-word-1770-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1925 max words - at ../dataset/shuffle-word-1925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1910 max words - at ../dataset/shuffle-word-1910-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1720 max words - at ../dataset/shuffle-word-1720-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1755 max words - at ../dataset/shuffle-word-1755-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1825 max words - at ../dataset/shuffle-word-1825-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1835 max words - at ../dataset/shuffle-word-1835-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 2000 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 2000 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 2000 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 2000 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 2000 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 2000 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 2000 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 2000 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 2000 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 2000 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 2000 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 2000 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 2000 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 2000 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 2000 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 2000 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 2000 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 2000 samples - at ../dataset/gen-word-1560-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 2000 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 2000 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 2000 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 2000 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 2000 samples - at ../dataset/gen-word-1510-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 2000 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 2000 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 2000 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 2000 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 2000 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 2000 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 2000 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 2000 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 2000 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 2000 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 2000 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 2000 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 2000 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 2000 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 2000 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 2000 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 2000 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 2000 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 2000 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated JSONL file with - 1295 max words, 2000 samples - at ../dataset/gen-word-1295-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 2000 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 2000 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 2000 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 2000 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 2000 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 1095 max words, 2000 samples - at ../dataset/gen-word-1095-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 2000 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 2000 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 1765 max words, 2000 samples - at ../dataset/gen-word-1765-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 2000 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 2000 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 2000 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 2000 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 2000 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 2000 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 2000 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 2000 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 2000 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 2000 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 2000 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 2000 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 1665 max words, 2000 samples - at ../dataset/gen-word-1665-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 2000 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 2000 samples - at ../dataset/gen-word-1650-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 2000 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 2000 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated JSONL file with - 1055 max words, 2000 samples - at ../dataset/gen-word-1055-count.jsonl\n",
      "Generated JSONL file with - 1005 max words, 2000 samples - at ../dataset/gen-word-1005-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 2000 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 2000 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 2000 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 2000 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 2000 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 2000 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 2000 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 2000 samples - at ../dataset/gen-word-1530-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 2000 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 2000 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 2000 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 2000 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 1015 max words, 2000 samples - at ../dataset/gen-word-1015-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 2000 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 2000 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 2000 samples - at ../dataset/gen-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1305 max words, 2000 samples - at ../dataset/gen-word-1305-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 2000 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 2000 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated JSONL file with - 1965 max words, 2000 samples - at ../dataset/gen-word-1965-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 2000 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 2000 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 2000 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 2000 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated JSONL file with - 1725 max words, 2000 samples - at ../dataset/gen-word-1725-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 2000 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1155 max words, 2000 samples - at ../dataset/gen-word-1155-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 2000 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 2000 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated JSONL file with - 1315 max words, 2000 samples - at ../dataset/gen-word-1315-count.jsonl\n",
      "Generated JSONL file with - 1115 max words, 2000 samples - at ../dataset/gen-word-1115-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 2000 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 2000 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 2000 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 2000 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 1985 max words, 2000 samples - at ../dataset/gen-word-1985-count.jsonl\n",
      "Generated JSONL file with - 1415 max words, 2000 samples - at ../dataset/gen-word-1415-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 2000 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 2000 samples - at ../dataset/gen-word-1550-count.jsonl\n",
      "Generated JSONL file with - 1275 max words, 2000 samples - at ../dataset/gen-word-1275-count.jsonl\n",
      "Generated JSONL file with - 1555 max words, 2000 samples - at ../dataset/gen-word-1555-count.jsonl\n",
      "Generated JSONL file with - 1475 max words, 2000 samples - at ../dataset/gen-word-1475-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 2000 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated JSONL file with - 1585 max words, 2000 samples - at ../dataset/gen-word-1585-count.jsonl\n",
      "Generated JSONL file with - 1165 max words, 2000 samples - at ../dataset/gen-word-1165-count.jsonl\n",
      "Generated JSONL file with - 1425 max words, 2000 samples - at ../dataset/gen-word-1425-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 2000 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 2000 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 1385 max words, 2000 samples - at ../dataset/gen-word-1385-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 2000 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 2000 samples - at ../dataset/gen-word-1580-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 2000 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated JSONL file with - 1785 max words, 2000 samples - at ../dataset/gen-word-1785-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 2000 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1085 max words, 2000 samples - at ../dataset/gen-word-1085-count.jsonl\n",
      "Generated JSONL file with - 1655 max words, 2000 samples - at ../dataset/gen-word-1655-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 2000 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 2000 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 2000 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 2000 samples - at ../dataset/gen-word-1620-count.jsonl\n",
      "Generated JSONL file with - 1995 max words, 2000 samples - at ../dataset/gen-word-1995-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 2000 samples - at ../dataset/gen-word-1520-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 2000 samples - at ../dataset/gen-word-1820-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 2000 samples - at ../dataset/gen-word-1660-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 2000 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 2000 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated JSONL file with - 1025 max words, 2000 samples - at ../dataset/gen-word-1025-count.jsonl\n",
      "Generated JSONL file with - 1145 max words, 2000 samples - at ../dataset/gen-word-1145-count.jsonl\n",
      "Generated JSONL file with - 1575 max words, 2000 samples - at ../dataset/gen-word-1575-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 2000 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated JSONL file with - 1175 max words, 2000 samples - at ../dataset/gen-word-1175-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 2000 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated JSONL file with - 1445 max words, 2000 samples - at ../dataset/gen-word-1445-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 2000 samples - at ../dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1915 max words, 2000 samples - at ../dataset/gen-word-1915-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 2000 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 2000 samples - at ../dataset/gen-word-1810-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 2000 samples - at ../dataset/gen-word-1730-count.jsonl\n",
      "Generated JSONL file with - 1795 max words, 2000 samples - at ../dataset/gen-word-1795-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 2000 samples - at ../dataset/gen-word-1590-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 2000 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 2000 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated JSONL file with - 1135 max words, 2000 samples - at ../dataset/gen-word-1135-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 2000 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated JSONL file with - 1035 max words, 2000 samples - at ../dataset/gen-word-1035-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 2000 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated JSONL file with - 1395 max words, 2000 samples - at ../dataset/gen-word-1395-count.jsonl\n",
      "Generated JSONL file with - 1105 max words, 2000 samples - at ../dataset/gen-word-1105-count.jsonl\n",
      "Generated JSONL file with - 1755 max words, 2000 samples - at ../dataset/gen-word-1755-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 2000 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated JSONL file with - 1045 max words, 2000 samples - at ../dataset/gen-word-1045-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 2000 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 1685 max words, 2000 samples - at ../dataset/gen-word-1685-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 2000 samples - at ../dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 2000 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated JSONL file with - 1255 max words, 2000 samples - at ../dataset/gen-word-1255-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 2000 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 1895 max words, 2000 samples - at ../dataset/gen-word-1895-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 2000 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 2000 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 2000 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated JSONL file with - 1075 max words, 2000 samples - at ../dataset/gen-word-1075-count.jsonl\n",
      "Generated JSONL file with - 1235 max words, 2000 samples - at ../dataset/gen-word-1235-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 2000 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 2000 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 2000 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 1865 max words, 2000 samples - at ../dataset/gen-word-1865-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 2000 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 1945 max words, 2000 samples - at ../dataset/gen-word-1945-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 2000 samples - at ../dataset/gen-word-1540-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 2000 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 2000 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 1195 max words, 2000 samples - at ../dataset/gen-word-1195-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 2000 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 2000 samples - at ../dataset/gen-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 2000 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 2000 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 2000 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1065 max words, 2000 samples - at ../dataset/gen-word-1065-count.jsonl\n",
      "Generated JSONL file with - 1505 max words, 2000 samples - at ../dataset/gen-word-1505-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 2000 samples - at ../dataset/gen-word-1910-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 2000 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1905 max words, 2000 samples - at ../dataset/gen-word-1905-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 2000 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 2000 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated JSONL file with - 1185 max words, 2000 samples - at ../dataset/gen-word-1185-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 2000 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 2000 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated JSONL file with - 1265 max words, 2000 samples - at ../dataset/gen-word-1265-count.jsonl\n",
      "Generated JSONL file with - 1245 max words, 2000 samples - at ../dataset/gen-word-1245-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 2000 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated JSONL file with - 1815 max words, 2000 samples - at ../dataset/gen-word-1815-count.jsonl\n",
      "Generated JSONL file with - 1285 max words, 2000 samples - at ../dataset/gen-word-1285-count.jsonl\n",
      "Generated JSONL file with - 1565 max words, 2000 samples - at ../dataset/gen-word-1565-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 2000 samples - at ../dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 2000 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 2000 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 2000 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 2000 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 2000 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 2000 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated JSONL file with - 1435 max words, 2000 samples - at ../dataset/gen-word-1435-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 2000 samples - at ../dataset/gen-word-1940-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 2000 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 1335 max words, 2000 samples - at ../dataset/gen-word-1335-count.jsonl\n",
      "Generated JSONL file with - 1545 max words, 2000 samples - at ../dataset/gen-word-1545-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 2000 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated JSONL file with - 1215 max words, 2000 samples - at ../dataset/gen-word-1215-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 2000 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated JSONL file with - 1615 max words, 2000 samples - at ../dataset/gen-word-1615-count.jsonl\n",
      "Generated JSONL file with - 1125 max words, 2000 samples - at ../dataset/gen-word-1125-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 2000 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated JSONL file with - 1365 max words, 2000 samples - at ../dataset/gen-word-1365-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 2000 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 2000 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 2000 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 2000 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1465 max words, 2000 samples - at ../dataset/gen-word-1465-count.jsonl\n",
      "Generated JSONL file with - 1625 max words, 2000 samples - at ../dataset/gen-word-1625-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 2000 samples - at ../dataset/gen-word-1830-count.jsonl\n",
      "Generated JSONL file with - 1515 max words, 2000 samples - at ../dataset/gen-word-1515-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 2000 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 2000 samples - at ../dataset/gen-word-1760-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 2000 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 2000 samples - at ../dataset/gen-word-1920-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 2000 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 2000 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 2000 samples - at ../dataset/gen-word-1680-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 2000 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated JSONL file with - 1705 max words, 2000 samples - at ../dataset/gen-word-1705-count.jsonl\n",
      "Generated JSONL file with - 1355 max words, 2000 samples - at ../dataset/gen-word-1355-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 2000 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 2000 samples - at ../dataset/gen-word-1740-count.jsonl\n",
      "Generated JSONL file with - 1885 max words, 2000 samples - at ../dataset/gen-word-1885-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 2000 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated JSONL file with - 1535 max words, 2000 samples - at ../dataset/gen-word-1535-count.jsonl\n",
      "Generated JSONL file with - 1775 max words, 2000 samples - at ../dataset/gen-word-1775-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 2000 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1345 max words, 2000 samples - at ../dataset/gen-word-1345-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 2000 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1645 max words, 2000 samples - at ../dataset/gen-word-1645-count.jsonl\n",
      "Generated JSONL file with - 1205 max words, 2000 samples - at ../dataset/gen-word-1205-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 2000 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1525 max words, 2000 samples - at ../dataset/gen-word-1525-count.jsonl\n",
      "Generated JSONL file with - 1225 max words, 2000 samples - at ../dataset/gen-word-1225-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 2000 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated JSONL file with - 1635 max words, 2000 samples - at ../dataset/gen-word-1635-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 2000 samples - at ../dataset/gen-word-1690-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 2000 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 2000 samples - at ../dataset/gen-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1975 max words, 2000 samples - at ../dataset/gen-word-1975-count.jsonl\n",
      "Generated JSONL file with - 1495 max words, 2000 samples - at ../dataset/gen-word-1495-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 2000 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1455 max words, 2000 samples - at ../dataset/gen-word-1455-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 2000 samples - at ../dataset/gen-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 2000 samples - at ../dataset/gen-word-1960-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 2000 samples - at ../dataset/gen-word-1930-count.jsonl\n",
      "Generated JSONL file with - 1875 max words, 2000 samples - at ../dataset/gen-word-1875-count.jsonl\n",
      "Generated JSONL file with - 1675 max words, 2000 samples - at ../dataset/gen-word-1675-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 2000 samples - at ../dataset/gen-word-1850-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 2000 samples - at ../dataset/gen-word-1780-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 2000 samples - at ../dataset/gen-word-1750-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 2000 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 2000 samples - at ../dataset/gen-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1485 max words, 2000 samples - at ../dataset/gen-word-1485-count.jsonl\n",
      "Generated JSONL file with - 1735 max words, 2000 samples - at ../dataset/gen-word-1735-count.jsonl\n",
      "Generated JSONL file with - 1405 max words, 2000 samples - at ../dataset/gen-word-1405-count.jsonl\n",
      "Generated JSONL file with - 1605 max words, 2000 samples - at ../dataset/gen-word-1605-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 2000 samples - at ../dataset/gen-word-1670-count.jsonl\n",
      "Generated JSONL file with - 1835 max words, 2000 samples - at ../dataset/gen-word-1835-count.jsonl\n",
      "Generated JSONL file with - 1825 max words, 2000 samples - at ../dataset/gen-word-1825-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 2000 samples - at ../dataset/gen-word-1640-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 2000 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 2000 samples - at ../dataset/gen-word-1890-count.jsonl\n",
      "Generated JSONL file with - 1805 max words, 2000 samples - at ../dataset/gen-word-1805-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 2000 samples - at ../dataset/gen-word-1880-count.jsonl\n",
      "Generated JSONL file with - 1845 max words, 2000 samples - at ../dataset/gen-word-1845-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 2000 samples - at ../dataset/gen-word-1970-count.jsonl\n",
      "Generated JSONL file with - 1855 max words, 2000 samples - at ../dataset/gen-word-1855-count.jsonl\n",
      "Generated JSONL file with - 1325 max words, 2000 samples - at ../dataset/gen-word-1325-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 2000 samples - at ../dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 2000 samples - at ../dataset/gen-word-1840-count.jsonl\n",
      "Generated JSONL file with - 1745 max words, 2000 samples - at ../dataset/gen-word-1745-count.jsonl\n",
      "Generated JSONL file with - 1375 max words, 2000 samples - at ../dataset/gen-word-1375-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 2000 samples - at ../dataset/gen-word-1950-count.jsonl\n",
      "Generated JSONL file with - 1935 max words, 2000 samples - at ../dataset/gen-word-1935-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 2000 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated JSONL file with - 1955 max words, 2000 samples - at ../dataset/gen-word-1955-count.jsonl\n",
      "Generated JSONL file with - 1595 max words, 2000 samples - at ../dataset/gen-word-1595-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 2000 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 2000 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 1695 max words, 2000 samples - at ../dataset/gen-word-1695-count.jsonl\n",
      "Generated JSONL file with - 1715 max words, 2000 samples - at ../dataset/gen-word-1715-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 2000 samples - at ../dataset/gen-word-1610-count.jsonl\n",
      "Generated JSONL file with - 1925 max words, 2000 samples - at ../dataset/gen-word-1925-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 2000 samples - at ../dataset/gen-word-1710-count.jsonl\n",
      "## Done ##\n",
      "total 7.8G\n",
      "drwxr-xr-x  2 root root  36K Jul 25 10:12 .\n",
      "drwxr-xr-x 13 root root  248 Jul 24 13:07 ..\n",
      "-rw-r--r--  1 root root  20K Jul 25 10:12 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.1M Jul 25 10:12 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Jul 25 10:12 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 25 10:12 gen-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Jul 25 10:12 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 25 10:12 gen-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Jul 25 10:12 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root 2.5M Jul 25 10:12 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 25 10:12 gen-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root 2.6M Jul 25 10:12 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 25 10:12 gen-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root 2.7M Jul 25 10:12 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 25 10:12 gen-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Jul 25 10:12 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 25 10:12 gen-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Jul 25 10:12 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 25 10:12 gen-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root 3.0M Jul 25 10:12 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 25 10:12 gen-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root  25K Jul 25 10:12 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Jul 25 10:12 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 25 10:12 gen-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root 3.2M Jul 25 10:12 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 25 10:12 gen-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root 3.3M Jul 25 10:12 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 25 10:12 gen-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root 3.3M Jul 25 10:12 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 25 10:12 gen-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root 3.5M Jul 25 10:12 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 25 10:12 gen-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root 3.6M Jul 25 10:12 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 25 10:12 gen-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root 3.6M Jul 25 10:12 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 25 10:12 gen-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Jul 25 10:12 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 25 10:12 gen-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Jul 25 10:12 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 25 10:12 gen-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root 3.9M Jul 25 10:12 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 25 10:12 gen-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  39M Jul 25 10:12 gen-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root  30K Jul 25 10:12 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Jul 25 10:12 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  39M Jul 25 10:12 gen-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Jul 25 10:12 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 4.2M Jul 25 10:12 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Jul 25 10:12 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 4.4M Jul 25 10:12 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Jul 25 10:12 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 4.6M Jul 25 10:12 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 4.7M Jul 25 10:12 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Jul 25 10:12 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 4.9M Jul 25 10:12 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Jul 25 10:12 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Jul 25 10:12 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 5.1M Jul 25 10:12 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Jul 25 10:12 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Jul 25 10:12 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 5.4M Jul 25 10:12 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Jul 25 10:12 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 5.5M Jul 25 10:12 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 5.6M Jul 25 10:12 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 5.7M Jul 25 10:12 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 5.8M Jul 25 10:12 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  39K Jul 25 10:12 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 5.9M Jul 25 10:12 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 6.0M Jul 25 10:12 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 6.1M Jul 25 10:12 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 6.2M Jul 25 10:12 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 6.3M Jul 25 10:12 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 6.4M Jul 25 10:12 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 6.5M Jul 25 10:12 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 6.6M Jul 25 10:12 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 6.7M Jul 25 10:12 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 6.8M Jul 25 10:12 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  43K Jul 25 10:12 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 6.9M Jul 25 10:12 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 7.0M Jul 25 10:12 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 7.1M Jul 25 10:12 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Jul 25 10:12 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 7.2M Jul 25 10:12 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 7.4M Jul 25 10:12 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 7.4M Jul 25 10:12 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 7.5M Jul 25 10:12 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 7.6M Jul 25 10:12 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Jul 25 10:12 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  51K Jul 25 10:12 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 7.8M Jul 25 10:12 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 7.9M Jul 25 10:12 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 8.0M Jul 25 10:12 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 8.1M Jul 25 10:12 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 8.2M Jul 25 10:12 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 8.3M Jul 25 10:12 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 8.4M Jul 25 10:12 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 8.5M Jul 25 10:12 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 8.6M Jul 25 10:12 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 8.7M Jul 25 10:12 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  53K Jul 25 10:12 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 8.8M Jul 25 10:12 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 8.9M Jul 25 10:12 gen-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root 9.0M Jul 25 10:12 gen-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root 9.1M Jul 25 10:12 gen-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root 9.2M Jul 25 10:12 gen-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root 9.3M Jul 25 10:12 gen-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root 9.3M Jul 25 10:12 gen-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root 9.5M Jul 25 10:12 gen-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root 9.5M Jul 25 10:12 gen-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root 9.6M Jul 25 10:12 gen-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root  15K Jul 25 10:12 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Jul 25 10:12 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 9.7M Jul 25 10:12 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 9.8M Jul 25 10:12 gen-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 9.9M Jul 25 10:12 gen-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  10M Jul 25 10:12 gen-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root 1.3M Jul 25 10:12 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 25 10:12 gen-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Jul 25 10:12 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 25 10:12 gen-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Jul 25 10:12 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 25 10:12 gen-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Jul 25 10:12 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 25 10:12 gen-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Jul 25 10:12 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 25 10:12 gen-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Jul 25 10:12 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 25 10:12 gen-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Jul 25 10:12 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 25 10:12 gen-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Jul 25 10:12 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 25 10:12 gen-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Jul 25 10:12 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 25 10:12 gen-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 25 10:12 gen-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root  49K Jul 25 10:12 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 575K Jul 25 10:12 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root 552K Jul 25 10:12 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root 553K Jul 25 10:12 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root 552K Jul 25 10:12 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root 552K Jul 25 10:12 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Jul 25 10:12 shuffle-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root 553K Jul 25 10:12 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root 550K Jul 25 10:12 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root 551K Jul 25 10:12 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root 554K Jul 25 10:12 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root 548K Jul 25 10:12 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root  45K Jul 25 10:12 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Jul 25 10:12 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root 541K Jul 25 10:12 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root 547K Jul 25 10:12 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Jul 25 10:12 shuffle-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root 541K Jul 25 10:12 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Jul 25 10:12 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Jul 25 10:12 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root 515K Jul 25 10:12 shuffle-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 10:12 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root 543K Jul 25 10:12 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root 537K Jul 25 10:12 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root 517K Jul 25 10:12 shuffle-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root 544K Jul 25 10:12 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root 519K Jul 25 10:12 shuffle-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root 541K Jul 25 10:12 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root  41K Jul 25 10:12 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 542K Jul 25 10:12 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root 539K Jul 25 10:12 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 10:12 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Jul 25 10:12 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 10:12 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root 535K Jul 25 10:12 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 10:12 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  37K Jul 25 10:12 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 10:12 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 10:12 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root 536K Jul 25 10:12 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 10:12 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root 534K Jul 25 10:12 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  35K Jul 25 10:12 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 10:12 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 10:12 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 10:12 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  32K Jul 25 10:12 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 10:12 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root 533K Jul 25 10:12 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root 532K Jul 25 10:12 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 10:12 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 10:12 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 10:12 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  31K Jul 25 10:12 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Jul 25 10:12 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root 530K Jul 25 10:12 shuffle-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root  82K Jul 25 10:12 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 624K Jul 25 10:12 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 531K Jul 25 10:12 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root 604K Jul 25 10:12 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root 600K Jul 25 10:12 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root 588K Jul 25 10:12 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root 528K Jul 25 10:12 shuffle-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root 591K Jul 25 10:12 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root 526K Jul 25 10:12 shuffle-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root 575K Jul 25 10:12 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root 529K Jul 25 10:12 shuffle-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root 574K Jul 25 10:12 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root 569K Jul 25 10:12 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root 579K Jul 25 10:12 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root 527K Jul 25 10:12 shuffle-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root 518K Jul 25 10:12 shuffle-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root 567K Jul 25 10:12 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root 520K Jul 25 10:12 shuffle-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root 523K Jul 25 10:12 shuffle-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root 524K Jul 25 10:12 shuffle-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root 521K Jul 25 10:12 shuffle-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root 525K Jul 25 10:12 shuffle-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root 522K Jul 25 10:12 shuffle-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root 120K Jul 25 10:12 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 2000 words dataset\n",
    "# \n",
    "for i in {50..2000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 192143.98it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-862270205dc70075/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 141.73it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-862270205dc70075/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.60it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-4.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-D-mem-finetune-4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2929658149\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2929658149\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230725_101602-mlvshwzg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTokenShift-D - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/mlvshwzg\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 291888.58it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-862270205dc70075/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.81it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-862270205dc70075/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2ab844e36d8ee086_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-862270205dc70075/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0dd6983134f49272_*_of_00064.arrow\n",
      "Saving the dataset (0/16 shards):   4%| | 29000/824749 [00:00<00:12, 63948.74 exSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/16 shards):   6%| | 47000/824749 [00:00<00:14, 55199.35 ex[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/16 shards):   6%| | 51547/824749 [00:01<00:14, 55199.35 ex[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/16 shards):  11%| | 92547/824749 [00:01<00:10, 68871.68 ex[rank: 6] Global seed set to 2929658149\n",
      "[rank: 2] Global seed set to 2929658149\n",
      "Saving the dataset (1/16 shards):  12%| | 102547/824749 [00:01<00:09, 75193.56 e[rank: 5] Global seed set to 2929658149\n",
      "[rank: 1] Global seed set to 2929658149\n",
      "[rank: 7] Global seed set to 2929658149\n",
      "[rank: 4] Global seed set to 2929658149\n",
      "[rank: 3] Global seed set to 2929658149\n",
      "Saving the dataset (2/16 shards):  13%|â–| 103094/824749 [00:01<00:09, 75193.56 eUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Saving the dataset (2/16 shards):  14%|â–| 113094/824749 [00:02<00:13, 52258.17 eninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 0] Global seed set to 2929658149                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-25 10:16:28,077] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-25 10:16:33,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-25 10:16:50,344] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-25 10:16:50,579] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-25 10:16:50,661] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-25 10:16:50,680] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-25 10:16:50,689] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2929658149\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-25 10:16:50,692] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06677007675170898 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10132145881652832 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10138845443725586 seconds\n",
      "Time to load fused_adam op: 0.10138773918151855 seconds\n",
      "Time to load fused_adam op: 0.10117387771606445 seconds\n",
      "Time to load fused_adam op: 0.10120368003845215 seconds\n",
      "Time to load fused_adam op: 0.10149073600769043 seconds\n",
      "Time to load fused_adam op: 0.10155177116394043 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06635332107543945 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10148382186889648 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10190606117248535 seconds\n",
      "Time to load utils op: 0.10159087181091309 seconds\n",
      "Time to load utils op: 0.1021726131439209 seconds\n",
      "Time to load utils op: 0.10179615020751953 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1018376350402832 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179734230041504 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002789497375488281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003631114959716797 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027108192443847656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026798248291015625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002799034118652344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002713203430175781 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005011558532714844 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   1%| | 800/103094 [12:44<27:09:46,  1.05it/s, v_num=hwzg, train/loss=6/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 103094/103094 [27:34:52<00:00,  1.04it/s, v_num=hwzg, train/los\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|â–                 | 1/104 [00:00<00:37,  2.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–Ž                 | 2/104 [00:00<00:34,  2.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–Œ                 | 3/104 [00:00<00:31,  3.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                 | 4/104 [00:01<00:31,  3.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–Š                 | 5/104 [00:01<00:29,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                 | 6/104 [00:01<00:27,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|â–ˆâ–                | 7/104 [00:02<00:27,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                | 8/104 [00:02<00:26,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–Œ                | 9/104 [00:02<00:26,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‹               | 10/104 [00:02<00:25,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|â–ˆâ–Š               | 11/104 [00:03<00:25,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–‰               | 12/104 [00:03<00:24,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–              | 13/104 [00:03<00:24,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–Ž              | 14/104 [00:03<00:24,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|â–ˆâ–ˆâ–              | 15/104 [00:04<00:24,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–Œ              | 16/104 [00:04<00:23,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|â–ˆâ–ˆâ–Š              | 17/104 [00:04<00:23,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–‰              | 18/104 [00:04<00:22,  3.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆ              | 19/104 [00:05<00:22,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–Ž             | 20/104 [00:05<00:22,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–             | 21/104 [00:05<00:22,  3.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–Œ             | 22/104 [00:05<00:22,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|â–ˆâ–ˆâ–ˆâ–Š             | 23/104 [00:06<00:21,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–‰             | 24/104 [00:06<00:21,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆ             | 25/104 [00:06<00:21,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 26/104 [00:06<00:20,  3.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–            | 27/104 [00:07<00:20,  3.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 28/104 [00:07<00:20,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 29/104 [00:07<00:19,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 30/104 [00:07<00:19,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 31/104 [00:08<00:19,  3.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 32/104 [00:08<00:19,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 33/104 [00:08<00:18,  3.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 34/104 [00:09<00:18,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 35/104 [00:09<00:18,  3.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 36/104 [00:09<00:18,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 37/104 [00:09<00:17,  3.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 38/104 [00:10<00:17,  3.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 39/104 [00:10<00:17,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 40/104 [00:10<00:16,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 41/104 [00:10<00:16,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 42/104 [00:11<00:16,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 43/104 [00:11<00:16,  3.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 44/104 [00:11<00:15,  3.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 45/104 [00:11<00:15,  3.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 46/104 [00:11<00:15,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 47/104 [00:12<00:14,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 48/104 [00:12<00:14,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 49/104 [00:12<00:14,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 50/104 [00:13<00:14,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 51/104 [00:13<00:13,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 52/104 [00:13<00:13,  3.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 53/104 [00:13<00:13,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 54/104 [00:14<00:12,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 55/104 [00:14<00:12,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 56/104 [00:14<00:12,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 57/104 [00:14<00:12,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 58/104 [00:15<00:11,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 59/104 [00:15<00:11,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 60/104 [00:15<00:11,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 61/104 [00:15<00:11,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 62/104 [00:15<00:10,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 63/104 [00:16<00:10,  3.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 64/104 [00:16<00:10,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 65/104 [00:16<00:09,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 66/104 [00:16<00:09,  3.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 67/104 [00:17<00:09,  3.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 68/104 [00:17<00:09,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 69/104 [00:17<00:08,  3.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 70/104 [00:17<00:08,  3.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 71/104 [00:18<00:08,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/104 [00:18<00:08,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 73/104 [00:18<00:07,  3.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 74/104 [00:18<00:07,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 75/104 [00:19<00:07,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 76/104 [00:19<00:07,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 77/104 [00:19<00:06,  3.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 78/104 [00:19<00:06,  3.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 79/104 [00:20<00:06,  3.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 80/104 [00:20<00:06,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/104 [00:20<00:05,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82/104 [00:20<00:05,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 83/104 [00:21<00:05,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/104 [00:21<00:05,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 85/104 [00:21<00:04,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 86/104 [00:21<00:04,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 87/104 [00:22<00:04,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 88/104 [00:22<00:04,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 89/104 [00:22<00:03,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 90/104 [00:22<00:03,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 91/104 [00:23<00:03,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 92/104 [00:23<00:03,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 93/104 [00:23<00:02,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 94/104 [00:23<00:02,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 95/104 [00:24<00:02,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 96/104 [00:24<00:02,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 97/104 [00:24<00:01,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 98/104 [00:24<00:01,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/104 [00:24<00:01,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 100/104 [00:25<00:01,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 101/104 [00:25<00:00,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 102/104 [00:25<00:00,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 103/104 [00:26<00:00,  3.96it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 103094/103094 [27:35:24<00:00,  1.04it/s, v_num=hwzg, train/los\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 103094/103094 [27:35:24<00:00,  1.04it/s, v_num=hwzg, train/los\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 103094/103094 [27:35:36<00:00,  1.04it/s, v_num=hwzg, train/los\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–…â–‡â–„â–‚â–ƒâ–ƒâ–†â–ƒâ–…â–‚â–‡â–…â–†â–‡â–…â–†â–ƒâ–‡â–†â–ƒâ–…â–‚â–ƒâ–…â–ˆâ–â–â–„â–‡â–…â–‚â–‡â–„â–â–‚â–†â–†â–ƒâ–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–†â–…â–â–…â–ƒâ–…â–…â–â–‚â–ƒâ–„â–ƒâ–â–ƒâ–â–‚â–â–„â–ƒâ–„â–ƒâ–â–‚â–‚â–â–‚â–ƒâ–â–â–â–ƒâ–ƒâ–ƒâ–â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.06787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.43029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mTokenShift-D - Mem-Finetune-4 (bs=256, train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/mlvshwzg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230725_101602-mlvshwzg/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-4.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-4 (bs=256, train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-D-mem-finetune-4/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-D-Tune4.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 26 13:54 ../model/TokenShift-D-Tune4.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-D-mem-finetune-4/last.ckpt\" \\\n",
    "        \"../model/TokenShift-D-Tune4.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-D-Tune4.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 96.66666666666667% similarity, with 29 matched token, and 1 token mismatch\n",
      "## Model validation for 35 tokens : 97.14285714285714% similarity, with 34 matched token, and 1 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 99.1304347826087% similarity, with 114 matched token, and 1 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 100.0% similarity, with 150 matched token, and 0 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 98.23529411764706% similarity, with 167 matched token, and 3 token mismatch\n",
      "## Model validation for 180 tokens : 98.33333333333333% similarity, with 177 matched token, and 3 token mismatch\n",
      "## Model validation for 190 tokens : 98.42105263157895% similarity, with 187 matched token, and 3 token mismatch\n",
      "## Model validation for 200 tokens : 98.5% similarity, with 197 matched token, and 3 token mismatch\n",
      "## Model validation for 210 tokens : 98.57142857142858% similarity, with 207 matched token, and 3 token mismatch\n",
      "## Model validation for 220 tokens : 98.63636363636363% similarity, with 217 matched token, and 3 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 98.75% similarity, with 237 matched token, and 3 token mismatch\n",
      "## Model validation for 250 tokens : 98.8% similarity, with 247 matched token, and 3 token mismatch\n",
      "## Model validation for 260 tokens : 98.84615384615385% similarity, with 257 matched token, and 3 token mismatch\n",
      "## Model validation for 270 tokens : 99.25925925925925% similarity, with 268 matched token, and 2 token mismatch\n",
      "## Model validation for 280 tokens : 99.28571428571429% similarity, with 278 matched token, and 2 token mismatch\n",
      "## Model validation for 290 tokens : 99.3103448275862% similarity, with 288 matched token, and 2 token mismatch\n",
      "## Model validation for 300 tokens : 99.0% similarity, with 297 matched token, and 3 token mismatch\n",
      "## Model validation for 325 tokens : 98.76923076923076% similarity, with 321 matched token, and 4 token mismatch\n",
      "## Model validation for 350 tokens : 99.14285714285714% similarity, with 347 matched token, and 3 token mismatch\n",
      "## Model validation for 375 tokens : 98.66666666666667% similarity, with 370 matched token, and 5 token mismatch\n",
      "## Model validation for 400 tokens : 98.5% similarity, with 394 matched token, and 6 token mismatch\n",
      "## Model validation for 425 tokens : 98.3529411764706% similarity, with 418 matched token, and 7 token mismatch\n",
      "## Model validation for 450 tokens : 98.0% similarity, with 441 matched token, and 9 token mismatch\n",
      "## Model validation for 475 tokens : 97.47368421052632% similarity, with 463 matched token, and 12 token mismatch\n",
      "## Model validation for 500 tokens : 97.6% similarity, with 488 matched token, and 12 token mismatch\n",
      "## Model validation for 525 tokens : 97.52380952380952% similarity, with 512 matched token, and 13 token mismatch\n",
      "## Model validation for 550 tokens : 96.54545454545455% similarity, with 531 matched token, and 19 token mismatch\n",
      "## Model validation for 575 tokens : 96.34782608695652% similarity, with 554 matched token, and 21 token mismatch\n",
      "## Model validation for 600 tokens : 96.33333333333334% similarity, with 578 matched token, and 22 token mismatch\n",
      "## Model validation for 625 tokens : 95.84% similarity, with 599 matched token, and 26 token mismatch\n",
      "## Model validation for 650 tokens : 95.84615384615385% similarity, with 623 matched token, and 27 token mismatch\n",
      "## Model validation for 675 tokens : 95.4074074074074% similarity, with 644 matched token, and 31 token mismatch\n",
      "## Model validation for 700 tokens : 95.42857142857143% similarity, with 668 matched token, and 32 token mismatch\n",
      "## Model validation for 750 tokens : 95.06666666666666% similarity, with 713 matched token, and 37 token mismatch\n",
      "## Model validation for 800 tokens : 92.75% similarity, with 742 matched token, and 58 token mismatch\n",
      "## Model validation for 850 tokens : 93.17647058823529% similarity, with 792 matched token, and 58 token mismatch\n",
      "## Model validation for 900 tokens : 92.33333333333333% similarity, with 831 matched token, and 69 token mismatch\n",
      "## Model validation for 950 tokens : 91.26315789473685% similarity, with 867 matched token, and 83 token mismatch\n",
      "## Model validation for 1000 tokens : 90.4% similarity, with 904 matched token, and 96 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-D-Tune4.pth\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 5 : Ramping up the ctx size (8192), memory training\n",
    "\n",
    "- Tune 5: Mid ctx size (8192), same as tune 4, but extended in context size again!\n",
    "\n",
    "This intentionally a much larger dataset, with increased leaarning rate, specifically for L24+ tokenshift models, who have been shown to clear Tune 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 558 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 109 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3300 max words - at ../dataset/shuffle-word-3300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (10 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (10 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 70 samples (10 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 132 samples (10 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (10 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 2800 max words - at ../dataset/shuffle-word-2800-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 27 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (10 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 92 samples (10 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (10 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (10 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 179 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7300 max words - at ../dataset/shuffle-word-7300-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4300 max words - at ../dataset/shuffle-word-4300-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (10 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (10 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (10 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6000 max words - at ../dataset/shuffle-word-6000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 2200 max words - at ../dataset/shuffle-word-2200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3200 max words - at ../dataset/shuffle-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5100 max words - at ../dataset/shuffle-word-5100-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (10 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3900 max words - at ../dataset/shuffle-word-3900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 2300 max words - at ../dataset/shuffle-word-2300-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7700 max words - at ../dataset/shuffle-word-7700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5000 max words - at ../dataset/shuffle-word-5000-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4600 max words - at ../dataset/shuffle-word-4600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4500 max words - at ../dataset/shuffle-word-4500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5800 max words - at ../dataset/shuffle-word-5800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5700 max words - at ../dataset/shuffle-word-5700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7200 max words - at ../dataset/shuffle-word-7200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6700 max words - at ../dataset/shuffle-word-6700-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (10 token repeat) - 2600 max words - at ../dataset/shuffle-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4900 max words - at ../dataset/shuffle-word-4900-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3000 max words - at ../dataset/shuffle-word-3000-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4200 max words - at ../dataset/shuffle-word-4200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6800 max words - at ../dataset/shuffle-word-6800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4400 max words - at ../dataset/shuffle-word-4400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4100 max words - at ../dataset/shuffle-word-4100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 2100 max words - at ../dataset/shuffle-word-2100-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6200 max words - at ../dataset/shuffle-word-6200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6500 max words - at ../dataset/shuffle-word-6500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 2400 max words - at ../dataset/shuffle-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (10 token repeat) - 2500 max words - at ../dataset/shuffle-word-2500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6100 max words - at ../dataset/shuffle-word-6100-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 2900 max words - at ../dataset/shuffle-word-2900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (10 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5600 max words - at ../dataset/shuffle-word-5600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3800 max words - at ../dataset/shuffle-word-3800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3400 max words - at ../dataset/shuffle-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 8200 max words - at ../dataset/shuffle-word-8200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5900 max words - at ../dataset/shuffle-word-5900-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7100 max words - at ../dataset/shuffle-word-7100-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7000 max words - at ../dataset/shuffle-word-7000-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 1000 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4800 max words - at ../dataset/shuffle-word-4800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3600 max words - at ../dataset/shuffle-word-3600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5300 max words - at ../dataset/shuffle-word-5300-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5400 max words - at ../dataset/shuffle-word-5400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7600 max words - at ../dataset/shuffle-word-7600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3100 max words - at ../dataset/shuffle-word-3100-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (10 token repeat) - 2700 max words - at ../dataset/shuffle-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3700 max words - at ../dataset/shuffle-word-3700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 3500 max words - at ../dataset/shuffle-word-3500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6300 max words - at ../dataset/shuffle-word-6300-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4000 max words - at ../dataset/shuffle-word-4000-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5500 max words - at ../dataset/shuffle-word-5500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6900 max words - at ../dataset/shuffle-word-6900-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 4700 max words - at ../dataset/shuffle-word-4700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6400 max words - at ../dataset/shuffle-word-6400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7800 max words - at ../dataset/shuffle-word-7800-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 6600 max words - at ../dataset/shuffle-word-6600-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 5200 max words - at ../dataset/shuffle-word-5200-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 8100 max words - at ../dataset/shuffle-word-8100-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7900 max words - at ../dataset/shuffle-word-7900-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7400 max words - at ../dataset/shuffle-word-7400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 7500 max words - at ../dataset/shuffle-word-7500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (10 token repeat) - 8000 max words - at ../dataset/shuffle-word-8000-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 1000 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 1000 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 1000 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 1000 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 1000 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 1000 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 1000 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 1000 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 1000 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 1000 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 1000 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 1000 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 1000 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 1000 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 1000 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 1000 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 1000 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 1000 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 1000 samples - at ../dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 1000 samples - at ../dataset/gen-word-2300-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 1000 samples - at ../dataset/gen-word-2100-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 1000 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 2600 max words, 1000 samples - at ../dataset/gen-word-2600-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 1000 samples - at ../dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 2900 max words, 1000 samples - at ../dataset/gen-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2800 max words, 1000 samples - at ../dataset/gen-word-2800-count.jsonl\n",
      "Generated JSONL file with - 2700 max words, 1000 samples - at ../dataset/gen-word-2700-count.jsonl\n",
      "Generated JSONL file with - 3000 max words, 1000 samples - at ../dataset/gen-word-3000-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 1000 samples - at ../dataset/gen-word-2400-count.jsonl\n",
      "Generated JSONL file with - 3100 max words, 1000 samples - at ../dataset/gen-word-3100-count.jsonl\n",
      "Generated JSONL file with - 3400 max words, 1000 samples - at ../dataset/gen-word-3400-count.jsonl\n",
      "Generated JSONL file with - 3600 max words, 1000 samples - at ../dataset/gen-word-3600-count.jsonl\n",
      "Generated JSONL file with - 3200 max words, 1000 samples - at ../dataset/gen-word-3200-count.jsonl\n",
      "Generated JSONL file with - 3300 max words, 1000 samples - at ../dataset/gen-word-3300-count.jsonl\n",
      "Generated JSONL file with - 4400 max words, 1000 samples - at ../dataset/gen-word-4400-count.jsonl\n",
      "Generated JSONL file with - 3900 max words, 1000 samples - at ../dataset/gen-word-3900-count.jsonl\n",
      "Generated JSONL file with - 4200 max words, 1000 samples - at ../dataset/gen-word-4200-count.jsonl\n",
      "Generated JSONL file with - 4000 max words, 1000 samples - at ../dataset/gen-word-4000-count.jsonl\n",
      "Generated JSONL file with - 3800 max words, 1000 samples - at ../dataset/gen-word-3800-count.jsonl\n",
      "Generated JSONL file with - 4100 max words, 1000 samples - at ../dataset/gen-word-4100-count.jsonl\n",
      "Generated JSONL file with - 3500 max words, 1000 samples - at ../dataset/gen-word-3500-count.jsonl\n",
      "Generated JSONL file with - 4500 max words, 1000 samples - at ../dataset/gen-word-4500-count.jsonl\n",
      "Generated JSONL file with - 4300 max words, 1000 samples - at ../dataset/gen-word-4300-count.jsonl\n",
      "Generated JSONL file with - 3700 max words, 1000 samples - at ../dataset/gen-word-3700-count.jsonl\n",
      "Generated JSONL file with - 5000 max words, 1000 samples - at ../dataset/gen-word-5000-count.jsonl\n",
      "Generated JSONL file with - 6000 max words, 1000 samples - at ../dataset/gen-word-6000-count.jsonl\n",
      "Generated JSONL file with - 4600 max words, 1000 samples - at ../dataset/gen-word-4600-count.jsonl\n",
      "Generated JSONL file with - 5400 max words, 1000 samples - at ../dataset/gen-word-5400-count.jsonl\n",
      "Generated JSONL file with - 5300 max words, 1000 samples - at ../dataset/gen-word-5300-count.jsonl\n",
      "Generated JSONL file with - 5500 max words, 1000 samples - at ../dataset/gen-word-5500-count.jsonl\n",
      "Generated JSONL file with - 4800 max words, 1000 samples - at ../dataset/gen-word-4800-count.jsonl\n",
      "Generated JSONL file with - 4900 max words, 1000 samples - at ../dataset/gen-word-4900-count.jsonl\n",
      "Generated JSONL file with - 4700 max words, 1000 samples - at ../dataset/gen-word-4700-count.jsonl\n",
      "Generated JSONL file with - 5900 max words, 1000 samples - at ../dataset/gen-word-5900-count.jsonl\n",
      "Generated JSONL file with - 5100 max words, 1000 samples - at ../dataset/gen-word-5100-count.jsonl\n",
      "Generated JSONL file with - 5200 max words, 1000 samples - at ../dataset/gen-word-5200-count.jsonl\n",
      "Generated JSONL file with - 5700 max words, 1000 samples - at ../dataset/gen-word-5700-count.jsonl\n",
      "Generated JSONL file with - 6400 max words, 1000 samples - at ../dataset/gen-word-6400-count.jsonl\n",
      "Generated JSONL file with - 6200 max words, 1000 samples - at ../dataset/gen-word-6200-count.jsonl\n",
      "Generated JSONL file with - 7100 max words, 1000 samples - at ../dataset/gen-word-7100-count.jsonl\n",
      "Generated JSONL file with - 6100 max words, 1000 samples - at ../dataset/gen-word-6100-count.jsonl\n",
      "Generated JSONL file with - 5600 max words, 1000 samples - at ../dataset/gen-word-5600-count.jsonl\n",
      "Generated JSONL file with - 6600 max words, 1000 samples - at ../dataset/gen-word-6600-count.jsonl\n",
      "Generated JSONL file with - 6700 max words, 1000 samples - at ../dataset/gen-word-6700-count.jsonl\n",
      "Generated JSONL file with - 5800 max words, 1000 samples - at ../dataset/gen-word-5800-count.jsonl\n",
      "Generated JSONL file with - 7500 max words, 1000 samples - at ../dataset/gen-word-7500-count.jsonl\n",
      "Generated JSONL file with - 6500 max words, 1000 samples - at ../dataset/gen-word-6500-count.jsonl\n",
      "Generated JSONL file with - 8200 max words, 1000 samples - at ../dataset/gen-word-8200-count.jsonl\n",
      "Generated JSONL file with - 6300 max words, 1000 samples - at ../dataset/gen-word-6300-count.jsonl\n",
      "Generated JSONL file with - 6900 max words, 1000 samples - at ../dataset/gen-word-6900-count.jsonl\n",
      "Generated JSONL file with - 6800 max words, 1000 samples - at ../dataset/gen-word-6800-count.jsonl\n",
      "Generated JSONL file with - 7900 max words, 1000 samples - at ../dataset/gen-word-7900-count.jsonl\n",
      "Generated JSONL file with - 7400 max words, 1000 samples - at ../dataset/gen-word-7400-count.jsonl\n",
      "Generated JSONL file with - 7200 max words, 1000 samples - at ../dataset/gen-word-7200-count.jsonl\n",
      "Generated JSONL file with - 7000 max words, 1000 samples - at ../dataset/gen-word-7000-count.jsonl\n",
      "Generated JSONL file with - 7800 max words, 1000 samples - at ../dataset/gen-word-7800-count.jsonl\n",
      "Generated JSONL file with - 8000 max words, 1000 samples - at ../dataset/gen-word-8000-count.jsonl\n",
      "Generated JSONL file with - 7700 max words, 1000 samples - at ../dataset/gen-word-7700-count.jsonl\n",
      "Generated JSONL file with - 7300 max words, 1000 samples - at ../dataset/gen-word-7300-count.jsonl\n",
      "Generated JSONL file with - 7600 max words, 1000 samples - at ../dataset/gen-word-7600-count.jsonl\n",
      "Generated JSONL file with - 8100 max words, 1000 samples - at ../dataset/gen-word-8100-count.jsonl\n",
      "## Done ##\n",
      "total 3.2G\n",
      "drwxr-xr-x  2 root root 8.0K Jul 26 15:04 .\n",
      "drwxr-xr-x 13 root root  248 Jul 24 13:07 ..\n",
      "-rw-r--r--  1 root root 1.1M Jul 26 15:04 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 9.6M Jul 26 15:04 gen-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  11M Jul 26 15:04 gen-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  12M Jul 26 15:04 gen-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  13M Jul 26 15:04 gen-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  14M Jul 26 15:04 gen-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  26K Jul 26 15:04 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  15M Jul 26 15:04 gen-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  16M Jul 26 15:04 gen-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  17M Jul 26 15:04 gen-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  18M Jul 26 15:04 gen-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  19M Jul 26 15:04 gen-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root 2.0M Jul 26 15:04 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  20M Jul 26 15:04 gen-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 26 15:04 gen-word-2100-count.jsonl\n",
      "-rw-r--r--  1 root root  21M Jul 26 15:04 gen-word-2200-count.jsonl\n",
      "-rw-r--r--  1 root root  22M Jul 26 15:04 gen-word-2300-count.jsonl\n",
      "-rw-r--r--  1 root root  23M Jul 26 15:04 gen-word-2400-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Jul 26 15:04 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  24M Jul 26 15:04 gen-word-2500-count.jsonl\n",
      "-rw-r--r--  1 root root  25M Jul 26 15:04 gen-word-2600-count.jsonl\n",
      "-rw-r--r--  1 root root  26M Jul 26 15:04 gen-word-2700-count.jsonl\n",
      "-rw-r--r--  1 root root  27M Jul 26 15:04 gen-word-2800-count.jsonl\n",
      "-rw-r--r--  1 root root  28M Jul 26 15:04 gen-word-2900-count.jsonl\n",
      "-rw-r--r--  1 root root 3.0M Jul 26 15:04 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  29M Jul 26 15:04 gen-word-3000-count.jsonl\n",
      "-rw-r--r--  1 root root  30M Jul 26 15:04 gen-word-3100-count.jsonl\n",
      "-rw-r--r--  1 root root  31M Jul 26 15:04 gen-word-3200-count.jsonl\n",
      "-rw-r--r--  1 root root  32M Jul 26 15:04 gen-word-3300-count.jsonl\n",
      "-rw-r--r--  1 root root  33M Jul 26 15:04 gen-word-3400-count.jsonl\n",
      "-rw-r--r--  1 root root  44K Jul 26 15:04 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  34M Jul 26 15:04 gen-word-3500-count.jsonl\n",
      "-rw-r--r--  1 root root  35M Jul 26 15:04 gen-word-3600-count.jsonl\n",
      "-rw-r--r--  1 root root  36M Jul 26 15:04 gen-word-3700-count.jsonl\n",
      "-rw-r--r--  1 root root  37M Jul 26 15:04 gen-word-3800-count.jsonl\n",
      "-rw-r--r--  1 root root  38M Jul 26 15:04 gen-word-3900-count.jsonl\n",
      "-rw-r--r--  1 root root 4.0M Jul 26 15:04 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  39M Jul 26 15:04 gen-word-4000-count.jsonl\n",
      "-rw-r--r--  1 root root  40M Jul 26 15:04 gen-word-4100-count.jsonl\n",
      "-rw-r--r--  1 root root  40M Jul 26 15:04 gen-word-4200-count.jsonl\n",
      "-rw-r--r--  1 root root  41M Jul 26 15:04 gen-word-4300-count.jsonl\n",
      "-rw-r--r--  1 root root  42M Jul 26 15:04 gen-word-4400-count.jsonl\n",
      "-rw-r--r--  1 root root  53K Jul 26 15:04 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  43M Jul 26 15:04 gen-word-4500-count.jsonl\n",
      "-rw-r--r--  1 root root  44M Jul 26 15:04 gen-word-4600-count.jsonl\n",
      "-rw-r--r--  1 root root  45M Jul 26 15:04 gen-word-4700-count.jsonl\n",
      "-rw-r--r--  1 root root  46M Jul 26 15:04 gen-word-4800-count.jsonl\n",
      "-rw-r--r--  1 root root  47M Jul 26 15:04 gen-word-4900-count.jsonl\n",
      "-rw-r--r--  1 root root  15K Jul 26 15:04 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 4.9M Jul 26 15:04 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  48M Jul 26 15:04 gen-word-5000-count.jsonl\n",
      "-rw-r--r--  1 root root  49M Jul 26 15:04 gen-word-5100-count.jsonl\n",
      "-rw-r--r--  1 root root  50M Jul 26 15:04 gen-word-5200-count.jsonl\n",
      "-rw-r--r--  1 root root  51M Jul 26 15:04 gen-word-5300-count.jsonl\n",
      "-rw-r--r--  1 root root  52M Jul 26 15:04 gen-word-5400-count.jsonl\n",
      "-rw-r--r--  1 root root  62K Jul 26 15:04 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  53M Jul 26 15:04 gen-word-5500-count.jsonl\n",
      "-rw-r--r--  1 root root  54M Jul 26 15:04 gen-word-5600-count.jsonl\n",
      "-rw-r--r--  1 root root  55M Jul 26 15:04 gen-word-5700-count.jsonl\n",
      "-rw-r--r--  1 root root  56M Jul 26 15:04 gen-word-5800-count.jsonl\n",
      "-rw-r--r--  1 root root  57M Jul 26 15:04 gen-word-5900-count.jsonl\n",
      "-rw-r--r--  1 root root 5.8M Jul 26 15:04 gen-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  58M Jul 26 15:04 gen-word-6000-count.jsonl\n",
      "-rw-r--r--  1 root root  59M Jul 26 15:04 gen-word-6100-count.jsonl\n",
      "-rw-r--r--  1 root root  59M Jul 26 15:04 gen-word-6200-count.jsonl\n",
      "-rw-r--r--  1 root root  60M Jul 26 15:04 gen-word-6300-count.jsonl\n",
      "-rw-r--r--  1 root root  61M Jul 26 15:04 gen-word-6400-count.jsonl\n",
      "-rw-r--r--  1 root root  72K Jul 26 15:04 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  62M Jul 26 15:04 gen-word-6500-count.jsonl\n",
      "-rw-r--r--  1 root root  63M Jul 26 15:04 gen-word-6600-count.jsonl\n",
      "-rw-r--r--  1 root root  64M Jul 26 15:04 gen-word-6700-count.jsonl\n",
      "-rw-r--r--  1 root root  65M Jul 26 15:04 gen-word-6800-count.jsonl\n",
      "-rw-r--r--  1 root root  66M Jul 26 15:04 gen-word-6900-count.jsonl\n",
      "-rw-r--r--  1 root root 6.8M Jul 26 15:04 gen-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  67M Jul 26 15:04 gen-word-7000-count.jsonl\n",
      "-rw-r--r--  1 root root  68M Jul 26 15:04 gen-word-7100-count.jsonl\n",
      "-rw-r--r--  1 root root  69M Jul 26 15:04 gen-word-7200-count.jsonl\n",
      "-rw-r--r--  1 root root  70M Jul 26 15:04 gen-word-7300-count.jsonl\n",
      "-rw-r--r--  1 root root  71M Jul 26 15:04 gen-word-7400-count.jsonl\n",
      "-rw-r--r--  1 root root  83K Jul 26 15:04 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  72M Jul 26 15:04 gen-word-7500-count.jsonl\n",
      "-rw-r--r--  1 root root  73M Jul 26 15:04 gen-word-7600-count.jsonl\n",
      "-rw-r--r--  1 root root  74M Jul 26 15:04 gen-word-7700-count.jsonl\n",
      "-rw-r--r--  1 root root  75M Jul 26 15:04 gen-word-7800-count.jsonl\n",
      "-rw-r--r--  1 root root  76M Jul 26 15:04 gen-word-7900-count.jsonl\n",
      "-rw-r--r--  1 root root 7.7M Jul 26 15:04 gen-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  77M Jul 26 15:04 gen-word-8000-count.jsonl\n",
      "-rw-r--r--  1 root root  78M Jul 26 15:04 gen-word-8100-count.jsonl\n",
      "-rw-r--r--  1 root root  78M Jul 26 15:04 gen-word-8200-count.jsonl\n",
      "-rw-r--r--  1 root root  93K Jul 26 15:04 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 8.7M Jul 26 15:04 gen-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root 104K Jul 26 15:04 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 282K Jul 26 15:04 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root 262K Jul 26 15:04 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root 260K Jul 26 15:04 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root 258K Jul 26 15:04 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  42K Jul 26 15:04 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 262K Jul 26 15:04 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root 262K Jul 26 15:04 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root 262K Jul 26 15:04 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root 268K Jul 26 15:04 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root 259K Jul 26 15:04 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r--  1 root root 260K Jul 26 15:04 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r--  1 root root  38K Jul 26 15:04 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 259K Jul 26 15:04 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r--  1 root root 259K Jul 26 15:04 shuffle-word-2600-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-2700-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-2800-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-2900-count.jsonl\n",
      "-rw-r--r--  1 root root 270K Jul 26 15:04 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3000-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3100-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-3200-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3300-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3400-count.jsonl\n",
      "-rw-r--r--  1 root root  34K Jul 26 15:04 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3500-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3600-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-3700-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3800-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-3900-count.jsonl\n",
      "-rw-r--r--  1 root root 267K Jul 26 15:04 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4000-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4100-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-4200-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4300-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4400-count.jsonl\n",
      "-rw-r--r--  1 root root  33K Jul 26 15:04 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4500-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4600-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4700-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4800-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-4900-count.jsonl\n",
      "-rw-r--r--  1 root root  89K Jul 26 15:04 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 264K Jul 26 15:04 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-5000-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5100-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-5200-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-5300-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5400-count.jsonl\n",
      "-rw-r--r--  1 root root  28K Jul 26 15:04 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5500-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5600-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-5700-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5800-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-5900-count.jsonl\n",
      "-rw-r--r--  1 root root 262K Jul 26 15:04 shuffle-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-6000-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-6100-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6200-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6300-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6400-count.jsonl\n",
      "-rw-r--r--  1 root root  30K Jul 26 15:04 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6500-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6600-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6700-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6800-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-6900-count.jsonl\n",
      "-rw-r--r--  1 root root 264K Jul 26 15:04 shuffle-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-7000-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7100-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7200-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7300-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7400-count.jsonl\n",
      "-rw-r--r--  1 root root  30K Jul 26 15:04 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7500-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7600-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7700-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-7800-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-7900-count.jsonl\n",
      "-rw-r--r--  1 root root 260K Jul 26 15:04 shuffle-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root 254K Jul 26 15:04 shuffle-word-8000-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-8100-count.jsonl\n",
      "-rw-r--r--  1 root root 255K Jul 26 15:04 shuffle-word-8200-count.jsonl\n",
      "-rw-r--r--  1 root root  28K Jul 26 15:04 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 261K Jul 26 15:04 shuffle-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  28K Jul 26 15:04 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 119K Jul 26 15:04 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..95..10} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 100+ - 8200 words dataset\n",
    "# \n",
    "for i in {100..8200..100} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:00<00:00, 100968.93it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c7c94d4eb4ce27c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 698.12it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.82it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c7c94d4eb4ce27c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 62.66it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-5.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/TokenShift-D-mem-finetune-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1558535245\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1558535245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230726_150601-kvg8wm8c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTokenShift-D - Mem-Finetune-5 (bs=256, train-ctx=2*4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/kvg8wm8c\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:00<00:00, 119560.28it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c7c94d4eb4ce27c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 77.61it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c7c94d4eb4ce27c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-cbd784d465ac0d91_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c7c94d4eb4ce27c1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b0cb61e249d8383c_*_of_00064.arrow\n",
      "Saving the dataset (4/5 shards):  89%|â–‰| 50368/56710 [00:01<00:00, 34878.42 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (4/5 shards):  99%|â–‰| 56368/56710 [00:01<00:00, 37853.15 examSetting ds_accelerator to cuda (auto detect)\n",
      "[rank: 0] Global seed set to 1558535245                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-26 15:06:19,100] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 4] Global seed set to 1558535245\n",
      "[rank: 1] Global seed set to 1558535245\n",
      "[rank: 2] Global seed set to 1558535245\n",
      "[rank: 6] Global seed set to 1558535245\n",
      "[rank: 7] Global seed set to 1558535245\n",
      "[rank: 5] Global seed set to 1558535245\n",
      "[rank: 3] Global seed set to 1558535245\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 3] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-26 15:06:55,250] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-26 15:06:55,620] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-26 15:06:55,692] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-26 15:06:55,724] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-26 15:06:55,770] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1558535245\n",
      "[rank: 7] Global seed set to 1558535245\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-26 15:06:55,777] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-26 15:06:55,777] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06437945365905762 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10143065452575684 seconds\n",
      "Time to load fused_adam op: 0.1010141372680664 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10147404670715332 seconds\n",
      "Time to load fused_adam op: 0.10139608383178711 seconds\n",
      "Time to load fused_adam op: 0.10139942169189453 seconds\n",
      "Time to load fused_adam op: 0.1018209457397461 seconds\n",
      "Time to load fused_adam op: 0.10202312469482422 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06530094146728516 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10168600082397461 seconds\n",
      "Time to load utils op: 0.1017308235168457 seconds\n",
      "Time to load utils op: 0.10150527954101562 seconds\n",
      "Time to load utils op: 0.10212182998657227 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10192370414733887 seconds\n",
      "Time to load utils op: 0.10190415382385254 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10180974006652832 seconds\n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027823448181152344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002689361572265625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002741813659667969 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002701282501220703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002818107604980469 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027751922607421875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002777576446533203 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004830360412597656 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:  11%| | 800/7089 [39:41<5:12:04,  2.98s/it, v_num=wm8c, train/loss=7.22/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 7089/7089 [5:51:43<00:00,  2.98s/it, v_num=wm8c, train/loss=2.8\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–Œ                 | 1/8 [00:00<00:01,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/8 [00:00<00:01,  4.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 3/8 [00:01<00:01,  2.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 4/8 [00:01<00:01,  2.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 5/8 [00:02<00:01,  2.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/8 [00:02<00:00,  2.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7/8 [00:02<00:00,  2.75it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 7089/7089 [5:51:53<00:00,  2.98s/it, v_num=wm8c, train/loss=2.8\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 7089/7089 [5:51:53<00:00,  2.98s/it, v_num=wm8c, train/loss=2.8\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 7089/7089 [5:52:06<00:00,  2.98s/it, v_num=wm8c, train/loss=2.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–†â–ˆâ–‚â–ƒâ–‚â–ˆâ–„â–â–„â–â–‡â–†â–‚â–‡â–†â–‚â–†â–„â–â–…â–„â–‡â–„â–‡â–„â–â–â–ˆâ–ˆâ–ƒâ–ƒâ–â–†â–‡â–ƒâ–‡â–†â–‚â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–ƒâ–„â–ˆâ–ˆâ–‡â–‚â–‡â–‡â–‡â–‚â–â–…â–†â–â–‡â–‚â–ƒâ–â–â–‡â–â–‡â–„â–‡â–„â–‚â–‡â–„â–‡â–‡â–‡â–‡â–†â–ƒâ–‚â–‚â–†â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.12695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.0752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mTokenShift-D - Mem-Finetune-5 (bs=256, train-ctx=2*4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/kvg8wm8c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230726_150601-kvg8wm8c/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-D-mem-finetune-5.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-5 (bs=256, train-ctx=2*4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-D-mem-finetune-5/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-D-Tune5.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 27 00:53 ../model/TokenShift-D-Tune5.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/TokenShift-D-mem-finetune-5/last.ckpt\" \\\n",
    "        \"../model/TokenShift-D-Tune5.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/TokenShift-D-Tune5.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 98.09523809523809% similarity, with 103 matched token, and 2 token mismatch\n",
      "## Model validation for 110 tokens : 99.0909090909091% similarity, with 109 matched token, and 1 token mismatch\n",
      "## Model validation for 115 tokens : 99.1304347826087% similarity, with 114 matched token, and 1 token mismatch\n",
      "## Model validation for 120 tokens : 98.33333333333333% similarity, with 118 matched token, and 2 token mismatch\n",
      "## Model validation for 125 tokens : 98.4% similarity, with 123 matched token, and 2 token mismatch\n",
      "## Model validation for 130 tokens : 99.23076923076923% similarity, with 129 matched token, and 1 token mismatch\n",
      "## Model validation for 135 tokens : 97.77777777777777% similarity, with 132 matched token, and 3 token mismatch\n",
      "## Model validation for 140 tokens : 97.85714285714285% similarity, with 137 matched token, and 3 token mismatch\n",
      "## Model validation for 145 tokens : 97.93103448275862% similarity, with 142 matched token, and 3 token mismatch\n",
      "## Model validation for 150 tokens : 98.0% similarity, with 147 matched token, and 3 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 99.47368421052632% similarity, with 189 matched token, and 1 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.52380952380952% similarity, with 209 matched token, and 1 token mismatch\n",
      "## Model validation for 220 tokens : 99.0909090909091% similarity, with 218 matched token, and 2 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 98.75% similarity, with 237 matched token, and 3 token mismatch\n",
      "## Model validation for 250 tokens : 98.8% similarity, with 247 matched token, and 3 token mismatch\n",
      "## Model validation for 260 tokens : 98.84615384615385% similarity, with 257 matched token, and 3 token mismatch\n",
      "## Model validation for 270 tokens : 98.14814814814815% similarity, with 265 matched token, and 5 token mismatch\n",
      "## Model validation for 280 tokens : 97.5% similarity, with 273 matched token, and 7 token mismatch\n",
      "## Model validation for 290 tokens : 98.27586206896551% similarity, with 285 matched token, and 5 token mismatch\n",
      "## Model validation for 300 tokens : 97.66666666666667% similarity, with 293 matched token, and 7 token mismatch\n",
      "## Model validation for 325 tokens : 97.53846153846155% similarity, with 317 matched token, and 8 token mismatch\n",
      "## Model validation for 350 tokens : 96.85714285714285% similarity, with 339 matched token, and 11 token mismatch\n",
      "## Model validation for 375 tokens : 96.53333333333333% similarity, with 362 matched token, and 13 token mismatch\n",
      "## Model validation for 400 tokens : 96.0% similarity, with 384 matched token, and 16 token mismatch\n",
      "## Model validation for 425 tokens : 95.52941176470588% similarity, with 406 matched token, and 19 token mismatch\n",
      "## Model validation for 450 tokens : 94.88888888888889% similarity, with 427 matched token, and 23 token mismatch\n",
      "## Model validation for 475 tokens : 94.10526315789474% similarity, with 447 matched token, and 28 token mismatch\n",
      "## Model validation for 500 tokens : 93.4% similarity, with 467 matched token, and 33 token mismatch\n",
      "## Model validation for 525 tokens : 92.57142857142857% similarity, with 486 matched token, and 39 token mismatch\n",
      "## Model validation for 550 tokens : 91.81818181818183% similarity, with 505 matched token, and 45 token mismatch\n",
      "## Model validation for 575 tokens : 90.95652173913044% similarity, with 523 matched token, and 52 token mismatch\n",
      "## Model validation for 600 tokens : 88.33333333333333% similarity, with 530 matched token, and 70 token mismatch\n",
      "## Model validation for 625 tokens : 87.03999999999999% similarity, with 544 matched token, and 81 token mismatch\n",
      "## Model validation for 650 tokens : 85.84615384615385% similarity, with 558 matched token, and 92 token mismatch\n",
      "## Model validation for 675 tokens : 85.03703703703704% similarity, with 574 matched token, and 101 token mismatch\n",
      "## Model validation for 700 tokens : 84.14285714285714% similarity, with 589 matched token, and 111 token mismatch\n",
      "## Model validation for 750 tokens : 84.13333333333334% similarity, with 631 matched token, and 119 token mismatch\n",
      "## Model validation for 800 tokens : 82.5% similarity, with 660 matched token, and 140 token mismatch\n",
      "## Model validation for 850 tokens : 81.76470588235294% similarity, with 695 matched token, and 155 token mismatch\n",
      "## Model validation for 900 tokens : 80.11111111111111% similarity, with 721 matched token, and 179 token mismatch\n",
      "## Model validation for 950 tokens : 79.05263157894737% similarity, with 751 matched token, and 199 token mismatch\n",
      "## Model validation for 1000 tokens : 76.8% similarity, with 768 matched token, and 232 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-D-Tune5.pth\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

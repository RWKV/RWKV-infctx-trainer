{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 1B5 / Embedding range 1e-01 / 16k\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the modified memory training for v5 models, across various initial embedding model weights\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dependencies for eval stuff\n",
    "!pip install -q aiocsv aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "WANDB_PREFIX: EWR-1B5-0.1\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "EMBED_SCALE=0.1\n",
    "\n",
    "WANDB_PREFIX=f\"EWR-1B5-{EMBED_SCALE}\"\n",
    "\n",
    "EMBED_SCALE_LABEL=str(EMBED_SCALE).replace(\".\", \"_\")\n",
    "FILENAME_PREFIX=f\"EWR-1B5-E{EMBED_SCALE_LABEL}\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"WANDB_PREFIX:\", WANDB_PREFIX)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enwiki Stage 1 : Foundation 16k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-E0_1-neox-v5-init.pth\n",
      "Vocab size: 50277\n",
      "Emb scale: 0.1\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 24 --n_embd 2048 \\\n",
    "        --emb-scale \"{EMBED_SCALE}\" \\\n",
    "        --vocab_size neox --skip-if-exists \\\n",
    "        \"../model/L24-D2048-E{EMBED_SCALE_LABEL}-neox-v5-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.27it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8c145c390c889a8f_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6ea97834c464ff4e_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-50665dd4de80b503_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b463f1fcb37e6075.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-db0203585b19d957.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/EWR-1B5-enwiki-16k.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-enwiki-16k.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-enwiki-16k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5-init.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=4'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-enwiki-16k.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-enwiki-16k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5-init.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=4'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 799300389\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 799300389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230813_133309-wljncu16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/wljncu16\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 80.83it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8c145c390c889a8f_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6ea97834c464ff4e_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-50665dd4de80b503_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b463f1fcb37e6075.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-db0203585b19d957.arrow\n",
      "Saving the dataset (0/5 shards):  15%|â–| 3000/20347 [00:00<00:01, 10915.58 exampSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/5 shards):  35%|â–Ž| 7070/20347 [00:00<00:01, 9305.70 exampl[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (3/5 shards):  60%|â–Œ| 12209/20347 [00:01<00:00, 9482.38 examp[rank: 4] Global seed set to 799300389\n",
      "[rank: 2] Global seed set to 799300389\n",
      "Saving the dataset (3/5 shards):  65%|â–‹| 13209/20347 [00:01<00:00, 9144.68 examp[rank: 7] Global seed set to 799300389\n",
      "[rank: 3] Global seed set to 799300389\n",
      "[rank: 1] Global seed set to 799300389\n",
      "[rank: 6] Global seed set to 799300389\n",
      "[rank: 5] Global seed set to 799300389\n",
      "[rank: 0] Global seed set to 799300389                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-13 13:33:25,989] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-13 13:33:41,512] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-13 13:33:42,341] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-13 13:33:42,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-13 13:33:42,394] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 799300389\n",
      "[rank: 7] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-13 13:33:42,416] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-08-13 13:33:42,417] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 799300389\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-13 13:33:42,425] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 21.077146291732788 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 21.1233229637146 seconds\n",
      "Time to load fused_adam op: 21.123411655426025 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 21.124607801437378 seconds\n",
      "Time to load fused_adam op: 21.124627590179443 seconds\n",
      "Time to load fused_adam op: 21.124515771865845 seconds\n",
      "Time to load fused_adam op: 21.125020265579224 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 21.125834703445435 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.827286005020142 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.917538166046143 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.913166284561157 seconds\n",
      "Time to load utils op: 10.912944793701172 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.914202451705933 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 10.913362979888916 seconds\n",
      "Time to load utils op: 10.913849115371704 seconds\n",
      "Time to load utils op: 10.913243293762207 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027108192443847656 seconds\n",
      "Time to load utils op: 0.000274658203125 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002732276916503906 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002803802490234375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002372264862060547 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00036072731018066406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030922889709472656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005166530609130859 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  39%|â–| 1000/2544 [2:21:43<3:38:48,  8.50s/it, v_num=cu16, train/loss=5/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 2544/2544 [6:00:59<00:00,  8.51s/it, v_num=cu16, train/loss=4.4\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 1/13 [00:01<00:23,  1.99s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 2/13 [00:03<00:21,  1.97s/it]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 3/13 [00:05<00:19,  1.97s/it]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4/13 [00:07<00:17,  1.97s/it]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 5/13 [00:09<00:15,  1.97s/it]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 6/13 [00:11<00:13,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 7/13 [00:13<00:11,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 8/13 [00:15<00:09,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 9/13 [00:17<00:07,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 10/13 [00:19<00:05,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11/13 [00:21<00:03,  1.96s/it]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/13 [00:23<00:01,  1.96s/it]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 2544/2544 [6:01:31<00:00,  8.53s/it, v_num=cu16, train/loss=4.4\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 2544/2544 [6:01:31<00:00,  8.53s/it, v_num=cu16, train/loss=4.4\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 2544/2544 [6:01:50<00:00,  8.53s/it, v_num=cu16, train/loss=4.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–ƒâ–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 16383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 4.26878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/wljncu16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230813_133309-wljncu16/logs\u001b[0m\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 257, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 213, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\", line 797, in deliver_network_status\n",
      "    return self._deliver_network_status(status)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
      "    return self._deliver_record(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-enwiki-16k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-16k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-16k/\" \\\n",
    "        --model.load_model=\"../model/L24-D2048-E{EMBED_SCALE_LABEL}-neox-v5-init.pth\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-enwiki-16k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-enwiki-16k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 13 19:37 ../model/EWR-1B5-E0_1-enwiki-16k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}-enwiki-16k/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-16k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-16k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "The premise of the boy was, at the time, one of the planets' long-term choices, was the idea of the disease. This concept was successful in the idea of smallpox and the experiment that a male and theropods had the largest genetic changes of the star. This pattern of view was found in the construction of a gigantic human body called the Devil in the Cave of the Dark. The specific epithet, which is now in the original world of the Last Power, is considered to be a particular cause of the destruction of the Tornonite that was used to make a clear test for the giant. The perubron was drawn in the same vein as the giant cockatose in the Sky: The Hunger Black, which is an extinct skull of the North Island but still probably the last skull of the Moon. The recipe was coined in 1975, the Japanese Prime Minister's Ministry of Foreign Affairs and the Black Sea Sun. The Big Percival\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-16k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-16k.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enwiki Stage 2 : Basic Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 989.92it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/EWR-1B5-enwiki-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-enwiki-instruct.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-enwiki-instruct/', '--model.load_model=../model/EWR-1B5-E0_1-enwiki-16k.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-enwiki-instruct.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-enwiki-instruct/', '--model.load_model=../model/EWR-1B5-E0_1-enwiki-16k.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1371332172\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1371332172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230813_193806-okavo5qp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/okavo5qp\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 913.39it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "[rank: 0] Global seed set to 1371332172                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-13 19:38:21,311] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 1371332172\n",
      "[rank: 5] Global seed set to 1371332172\n",
      "[rank: 1] Global seed set to 1371332172\n",
      "[rank: 2] Global seed set to 1371332172\n",
      "[rank: 4] Global seed set to 1371332172\n",
      "[rank: 3] Global seed set to 1371332172\n",
      "[rank: 6] Global seed set to 1371332172\n",
      "[rank: 5] Global seed set to 1371332172\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-13 19:38:39,499] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1371332172\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-13 19:38:41,193] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1371332172\n",
      "[rank: 2] Global seed set to 1371332172\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-13 19:38:41,368] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-13 19:38:41,369] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1371332172\n",
      "[rank: 6] Global seed set to 1371332172\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-13 19:38:41,440] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-13 19:38:41,440] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1371332172\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-13 19:38:41,444] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07355284690856934 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10115385055541992 seconds\n",
      "Time to load fused_adam op: 0.10143494606018066 seconds\n",
      "Time to load fused_adam op: 0.10125279426574707 seconds\n",
      "Time to load fused_adam op: 0.1013193130493164 seconds\n",
      "Time to load fused_adam op: 0.1013944149017334 seconds\n",
      "Time to load fused_adam op: 0.10169625282287598 seconds\n",
      "Time to load fused_adam op: 0.10176467895507812 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0723123550415039 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1019434928894043 seconds\n",
      "Time to load utils op: 0.10113072395324707 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10169506072998047 seconds\n",
      "Time to load utils op: 0.10196471214294434 seconds\n",
      "Time to load utils op: 0.1019291877746582 seconds\n",
      "Time to load utils op: 0.1018064022064209 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10134530067443848 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002815723419189453 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002295970916748047 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002651214599609375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023245811462402344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00039005279541015625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002574920654296875 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00022077560424804688 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005362033843994141 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|â–Œ| 1000/1867 [18:26<15:59,  1.11s/it, v_num=o5qp, train/loss=5.620/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [34:02<00:00,  1.09s/it, v_num=o5qp, train/loss=4.530\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 1/10 [00:00<00:02,  3.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–Š               | 2/10 [00:00<00:01,  4.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3/10 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 4/10 [00:00<00:01,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 5/10 [00:01<00:01,  4.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 6/10 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7/10 [00:01<00:00,  4.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/10 [00:01<00:00,  4.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9/10 [00:01<00:00,  4.85it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [34:11<00:00,  1.10s/it, v_num=o5qp, train/loss=4.530\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [34:11<00:00,  1.10s/it, v_num=o5qp, train/loss=4.530\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [34:28<00:00,  1.11s/it, v_num=o5qp, train/loss=4.530\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–‚â–ƒâ–‚â–‚â–‚â–â–„â–ˆâ–‚â–…â–â–â–„â–â–‚â–ƒâ–â–ƒâ–‚â–â–„â–ƒâ–‡â–‚â–â–ƒâ–†â–‚â–„â–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–…â–‡â–…â–…â–‚â–†â–‡â–…â–„â–‚â–ƒâ–â–‚â–„â–†â–†â–…â–†â–â–‡â–„â–„â–‡â–„â–‚â–„â–ƒâ–‚â–„â–„â–…â–„â–â–…â–‚â–†â–ƒâ–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.59375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.72725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/okavo5qp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230813_193806-okavo5qp/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-enwiki-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-Instruct (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-instruct/\" \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-enwiki-16k.pth\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-enwiki-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-enwiki-instruct.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 13 20:14 ../model/EWR-1B5-E0_1-enwiki-instruct.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}truct/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. As the body of the creature and its owner, they say it might be traced to a man, which is actually different from the first human to the world. It may also have been described by the natural disasters that had been intended to find a galaxy of adult spiders. These \"Her races\" creatures are preserved in the way of living in the endothelial. \n",
      "\n",
      "There are no scientific evidence for some of the most deadly encounter.\n",
      "\n",
      "1) Clododendrons are a drug of zoos in their orbit.\n",
      "\n",
      "The classification is similar to its eyes and the next, which is different from the world's geography.\n",
      "\n",
      "The phrase \"Chis Canbero\" is derived from a character called \"African antics\" in \"Give me a bulleted list of the best ones in this list\" by Seinfeld. In \"Ansel Cove\" by \"The Storm\" refers to a creature in the universe of Jesus's calendar. The first reference of the novel is\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-instruct.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ../dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ../dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ../dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ../dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ../dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ../dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x 2 root root  330 Aug 14 03:13 .\n",
      "drwxr-xr-x 6 root root   86 Aug 13 13:22 ..\n",
      "-rw-r--r-- 1 root root 610K Aug 14 03:13 word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Aug 14 03:13 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 724K Aug 14 03:13 word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 840K Aug 14 03:13 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root 852K Aug 14 03:13 word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Aug 14 03:13 word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 974K Aug 14 03:13 word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Aug 14 03:13 word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 967K Aug 14 03:13 word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Aug 14 03:13 word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Aug 14 03:13 word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Aug 14 03:13 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-5-count.jsonl  5  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-10-count.jsonl 10 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-15-count.jsonl 15 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-20-count.jsonl 20 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-25-count.jsonl 25 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-40-count.jsonl 40 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-50-count.jsonl 50 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-60-count.jsonl 80 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-100-count.jsonl 100 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-instruct.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-instruct/', '--model.load_model=../model/EWR-1B5-E0_1-enwiki-instruct.pth', '--model.ctx_len=512', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-instruct.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-instruct/', '--model.load_model=../model/EWR-1B5-E0_1-enwiki-instruct.pth', '--model.ctx_len=512', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1413300084\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1413300084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230814_031311-t5yf7tzt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/t5yf7tzt\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5499efdd51b04398/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 4788.02it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 309.70it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5499efdd51b04398/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 254.86it/s]\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 1413300084\n",
      "[rank: 5] Global seed set to 1413300084\n",
      "[rank: 6] Global seed set to 1413300084\n",
      "[rank: 4] Global seed set to 1413300084\n",
      "[rank: 2] Global seed set to 1413300084\n",
      "[rank: 1] Global seed set to 1413300084\n",
      "[rank: 3] Global seed set to 1413300084\n",
      "Filter (num_proc=64):   6%|â–      | 2188/35000 [00:00<00:11, 2965.83 examples/s][rank: 7] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-14 03:13:46,377] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 1413300084                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-14 03:13:47,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-14 03:13:47,688] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-14 03:13:47,740] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-14 03:13:47,774] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-14 03:13:47,787] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1413300084\n",
      "[rank: 1] Global seed set to 1413300084\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-14 03:13:47,788] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-14 03:13:47,789] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07133293151855469 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10119962692260742 seconds\n",
      "Time to load fused_adam op: 0.10105299949645996 seconds\n",
      "Time to load fused_adam op: 0.10128378868103027 seconds\n",
      "Time to load fused_adam op: 0.10169267654418945 seconds\n",
      "Time to load fused_adam op: 0.10160374641418457 seconds\n",
      "Time to load fused_adam op: 0.10168218612670898 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10193347930908203 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07202911376953125 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10195708274841309 seconds\n",
      "Time to load utils op: 0.10182547569274902 seconds\n",
      "Time to load utils op: 0.10140299797058105 seconds\n",
      "Time to load utils op: 0.1018979549407959 seconds\n",
      "Time to load utils op: 0.10108804702758789 seconds\n",
      "Time to load utils op: 0.10210275650024414 seconds\n",
      "Time to load utils op: 0.10202407836914062 seconds\n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028061866760253906 seconds\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002486705780029297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002300739288330078 seconds\n",
      "Time to load utils op: 0.00022172927856445312 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002722740173339844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025963783264160156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005297660827636719 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|â–| 800/4371 [03:31<15:43,  3.78it/s, v_num=7tzt, train/loss=3.200]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:27<00:00,  3.56it/s, v_num=7tzt, train/loss=5.880\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆ                | 1/5 [00:00<00:01,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 2/5 [00:00<00:00,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 3/5 [00:00<00:00,  4.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4/5 [00:00<00:00,  4.60it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:35<00:00,  3.54it/s, v_num=7tzt, train/loss=5.880\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:35<00:00,  3.54it/s, v_num=7tzt, train/loss=5.880\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:55<00:00,  3.48it/s, v_num=7tzt, train/loss=5.880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–„â–â–†â–ƒâ–â–â–â–‚â–ˆâ–„â–ƒâ–‚â–†â–ƒâ–‚â–ƒâ–‡â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–â–‚â–â–â–â–ƒâ–â–ƒâ–‚â–‚â–‚â–ˆâ–„â–â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–ˆâ–ƒâ–‡â–…â–‚â–â–‚â–…â–‡â–†â–†â–„â–‡â–…â–†â–…â–â–„â–†â–‚â–†â–ƒâ–†â–‚â–„â–…â–†â–â–…â–†â–†â–‚â–…â–„â–…â–‡â–…â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.7165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/t5yf7tzt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230814_031311-t5yf7tzt/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Instruct (train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-instruct/\" \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\" \\\n",
    "        --model.ctx_len=512 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-instruct.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 14 03:35 ../model/EWR-1B5-E0_1-mem-instruct.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-instruct/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-instruct.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 5247 samples (20 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 3565 samples (20 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 3170 samples (30 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 679 samples (50 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 1333 samples (50 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 1772 samples (50 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 2620 samples (50 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 79M\n",
      "drwxr-xr-x 2 root root  4.0K Aug 14 03:35 .\n",
      "drwxr-xr-x 6 root root    86 Aug 13 13:22 ..\n",
      "-rw-r--r-- 1 root root  980K Aug 14 03:35 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 03:35 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 03:35 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 03:35 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 03:35 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 03:35 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 03:35 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 03:35 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  724K Aug 14 03:35 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 03:35 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Aug 14 03:35 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Aug 14 03:35 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Aug 14 03:35 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Aug 14 03:35 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Aug 14 03:35 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Aug 14 03:35 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Aug 14 03:35 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Aug 14 03:35 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Aug 14 03:35 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 1016K Aug 14 03:35 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 03:35 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  856K Aug 14 03:35 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 03:35 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 03:35 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 03:35 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 03:35 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Aug 14 03:35 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  595K Aug 14 03:35 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root   10M Aug 14 03:35 word-200-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-100-count.jsonl 100 5000 &\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-512/', '--model.lr_init=5e-4', '--model.lr_final=4e-4', '--data.max_token_size=512', '--model.ctx_len=512', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-instruct.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-512/', '--model.lr_init=5e-4', '--model.lr_final=4e-4', '--data.max_token_size=512', '--model.ctx_len=512', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-instruct.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1155882287\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1155882287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230814_033555-vyf7uv1u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/vyf7uv1u\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 108408.93it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-848ac4195cb118fe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2874.78it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 144.47it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 6] Global seed set to 1155882287\n",
      "[rank: 2] Global seed set to 1155882287\n",
      "[rank: 1] Global seed set to 1155882287\n",
      "[rank: 7] Global seed set to 1155882287\n",
      "[rank: 4] Global seed set to 1155882287\n",
      "[rank: 5] Global seed set to 1155882287\n",
      "[rank: 3] Global seed set to 1155882287\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-848ac4195cb118fe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 144.49it/s]\n",
      "[rank: 2] Global seed set to 1155882287                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-14 03:36:30,622] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-14 03:36:31,660] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-14 03:36:31,707] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-14 03:36:31,744] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-14 03:36:31,780] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-14 03:36:31,790] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1155882287\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-14 03:36:31,799] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 1155882287                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-14 03:36:34,536] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07205557823181152 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10102224349975586 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10112452507019043 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10141253471374512 seconds\n",
      "Time to load fused_adam op: 0.10157513618469238 seconds\n",
      "Time to load fused_adam op: 0.10153508186340332 seconds\n",
      "Time to load fused_adam op: 0.10164761543273926 seconds\n",
      "Time to load fused_adam op: 0.10195183753967285 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07009387016296387 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10188531875610352 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1017005443572998 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10111618041992188 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10171675682067871 seconds\n",
      "Time to load utils op: 0.10203337669372559 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10197758674621582 seconds\n",
      "Time to load utils op: 0.10166478157043457 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002865791320800781 seconds\n",
      "Time to load utils op: 0.0002567768096923828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002701282501220703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002899169921875 seconds\n",
      "Time to load utils op: 0.0002677440643310547 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002532005310058594 seconds\n",
      "Time to load utils op: 0.0002911090850830078 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005598068237304688 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/16033 [03:30<1:06:57,  3.79it/s, v_num=uv1u, train/loss=6.5/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 16033/16033 [1:14:49<00:00,  3.57it/s, v_num=uv1u, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 1/17 [00:00<00:04,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 2/17 [00:00<00:03,  4.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–Ž               | 3/17 [00:00<00:03,  4.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 4/17 [00:00<00:02,  4.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 5/17 [00:01<00:02,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 6/17 [00:01<00:02,  4.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 7/17 [00:01<00:02,  4.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 8/17 [00:01<00:01,  4.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 9/17 [00:01<00:01,  4.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 10/17 [00:02<00:01,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 11/17 [00:02<00:01,  4.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12/17 [00:02<00:01,  4.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13/17 [00:02<00:00,  4.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 14/17 [00:02<00:00,  4.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/17 [00:03<00:00,  4.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/17 [00:03<00:00,  5.01it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16033/16033 [1:14:59<00:00,  3.56it/s, v_num=uv1u, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16033/16033 [1:14:59<00:00,  3.56it/s, v_num=uv1u, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 16033/16033 [1:15:23<00:00,  3.54it/s, v_num=uv1u, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–ƒâ–ƒâ–â–„â–‚â–„â–â–„â–â–â–‚â–ƒâ–â–ƒâ–‚â–„â–…â–‚â–„â–â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‡â–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ƒâ–‡â–ˆâ–‡â–‚â–‡â–‚â–‡â–‡â–†â–‡â–†â–†â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.83984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.11873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/vyf7uv1u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230814_033555-vyf7uv1u/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-512 (train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-512/\" \\\n",
    "        --model.lr_init=5e-4 \\\n",
    "        --model.lr_final=4e-4 \\\n",
    "        --data.max_token_size=512 \\\n",
    "        --model.ctx_len=512 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-ctx-512/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-ctx-512.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 14 04:53 ../model/EWR-1B5-E0_1-mem-ctx-512.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-512/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 98.66666666666667% similarity, with 74 matched token, and 1 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 97.6470588235294% similarity, with 83 matched token, and 2 token mismatch\n",
      "## Model validation for 90 tokens : 92.22222222222223% similarity, with 83 matched token, and 7 token mismatch\n",
      "## Model validation for 95 tokens : 92.63157894736842% similarity, with 88 matched token, and 7 token mismatch\n",
      "## Model validation for 100 tokens : 94.0% similarity, with 94 matched token, and 6 token mismatch\n",
      "## Model validation for 105 tokens : 92.38095238095238% similarity, with 97 matched token, and 8 token mismatch\n",
      "## Model validation for 110 tokens : 90.0% similarity, with 99 matched token, and 11 token mismatch\n",
      "## Model validation for 115 tokens : 87.82608695652175% similarity, with 101 matched token, and 14 token mismatch\n",
      "## Model validation for 120 tokens : 84.16666666666667% similarity, with 101 matched token, and 19 token mismatch\n",
      "## Model validation for 125 tokens : 83.2% similarity, with 104 matched token, and 21 token mismatch\n",
      "## Model validation for 130 tokens : 85.38461538461539% similarity, with 111 matched token, and 19 token mismatch\n",
      "## Model validation for 135 tokens : 85.18518518518519% similarity, with 115 matched token, and 20 token mismatch\n",
      "## Model validation for 140 tokens : 85.0% similarity, with 119 matched token, and 21 token mismatch\n",
      "## Model validation for 145 tokens : 84.13793103448276% similarity, with 122 matched token, and 23 token mismatch\n",
      "## Model validation for 150 tokens : 81.33333333333333% similarity, with 122 matched token, and 28 token mismatch\n",
      "## Model validation for 160 tokens : 81.25% similarity, with 130 matched token, and 30 token mismatch\n",
      "## Model validation for 170 tokens : 72.35294117647058% similarity, with 123 matched token, and 47 token mismatch\n",
      "## Model validation for 180 tokens : 72.22222222222221% similarity, with 130 matched token, and 50 token mismatch\n",
      "## Model validation for 190 tokens : 66.84210526315789% similarity, with 127 matched token, and 63 token mismatch\n",
      "## Model validation for 200 tokens : 61.0% similarity, with 122 matched token, and 78 token mismatch\n",
      "## Model validation for 210 tokens : 56.666666666666664% similarity, with 119 matched token, and 91 token mismatch\n",
      "## Model validation for 220 tokens : 49.54545454545455% similarity, with 109 matched token, and 111 token mismatch\n",
      "## Model validation for 230 tokens : 43.913043478260875% similarity, with 101 matched token, and 129 token mismatch\n",
      "## Model validation for 240 tokens : 40.0% similarity, with 96 matched token, and 144 token mismatch\n",
      "## Model validation for 250 tokens : 38.800000000000004% similarity, with 97 matched token, and 153 token mismatch\n",
      "## Model validation for 260 tokens : 35.38461538461539% similarity, with 92 matched token, and 168 token mismatch\n",
      "## Model validation for 270 tokens : 32.96296296296296% similarity, with 89 matched token, and 181 token mismatch\n",
      "## Model validation for 280 tokens : 31.071428571428573% similarity, with 87 matched token, and 193 token mismatch\n",
      "## Model validation for 290 tokens : 28.27586206896552% similarity, with 82 matched token, and 208 token mismatch\n",
      "## Model validation for 300 tokens : 25.666666666666664% similarity, with 77 matched token, and 223 token mismatch\n",
      "## Model validation for 325 tokens : 20.307692307692307% similarity, with 66 matched token, and 259 token mismatch\n",
      "## Model validation for 350 tokens : 17.71428571428571% similarity, with 62 matched token, and 288 token mismatch\n",
      "## Model validation for 375 tokens : 15.2% similarity, with 57 matched token, and 318 token mismatch\n",
      "## Model validation for 400 tokens : 10.0% similarity, with 40 matched token, and 360 token mismatch\n",
      "## Model validation for 425 tokens : 7.294117647058823% similarity, with 31 matched token, and 394 token mismatch\n",
      "## Model validation for 450 tokens : 5.555555555555555% similarity, with 25 matched token, and 425 token mismatch\n",
      "## Model validation for 475 tokens : 3.7894736842105265% similarity, with 18 matched token, and 457 token mismatch\n",
      "## Model validation for 500 tokens : 3.4000000000000004% similarity, with 17 matched token, and 483 token mismatch\n",
      "## Model validation for 525 tokens : 2.666666666666667% similarity, with 14 matched token, and 511 token mismatch\n",
      "## Model validation for 550 tokens : 2.5454545454545454% similarity, with 14 matched token, and 536 token mismatch\n",
      "## Model validation for 575 tokens : 2.608695652173913% similarity, with 15 matched token, and 560 token mismatch\n",
      "## Model validation for 600 tokens : 2.166666666666667% similarity, with 13 matched token, and 587 token mismatch\n",
      "## Model validation for 625 tokens : 2.08% similarity, with 13 matched token, and 612 token mismatch\n",
      "## Model validation for 650 tokens : 2.0% similarity, with 13 matched token, and 637 token mismatch\n",
      "## Model validation for 675 tokens : 1.925925925925926% similarity, with 13 matched token, and 662 token mismatch\n",
      "## Model validation for 700 tokens : 1.7142857142857144% similarity, with 12 matched token, and 688 token mismatch\n",
      "## Model validation for 750 tokens : 2.0% similarity, with 15 matched token, and 735 token mismatch\n",
      "## Model validation for 800 tokens : 1.7500000000000002% similarity, with 14 matched token, and 786 token mismatch\n",
      "## Model validation for 850 tokens : 1.7647058823529411% similarity, with 15 matched token, and 835 token mismatch\n",
      "## Model validation for 900 tokens : 1.7777777777777777% similarity, with 16 matched token, and 884 token mismatch\n",
      "## Model validation for 950 tokens : 1.789473684210526% similarity, with 17 matched token, and 933 token mismatch\n",
      "## Model validation for 1000 tokens : 1.9% similarity, with 19 matched token, and 981 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Low ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Low ctx size (1024), Scaling up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 1000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 1000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 661 samples (10 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 1000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 1000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 1000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 1000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 1791 samples (10 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 1000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 756 samples (10 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 887 samples (10 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 660 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 583 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 580 samples (10 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 194 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 1055 samples (10 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 2623 samples (10 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 620 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 1000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 751 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 323 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 137 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 1500 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 1314 samples (10 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 534 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 146 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 1000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 709 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 1053 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 368 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 960 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 1500 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 892 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 1500 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 293 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 1500 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 346 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 1500 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 1500 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 114 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 566 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 805 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 411 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 1500 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 1500 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 383 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 1500 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 1500 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 193 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 340 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 5553 samples (10 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 1500 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 1500 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 283 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 396 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 264 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 356 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 306 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 1500 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 195 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 1500 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 218 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 1500 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated a single JSONL file with 280 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 1500 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 276 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 302 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 1500 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 1500 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 203 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 1500 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 1500 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 313 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 1500 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 1500 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated a single JSONL file with 289 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 1500 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 1500 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 1500 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 1500 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 1500 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 1500 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 1500 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated a single JSONL file with 210 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 1500 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 1500 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 1500 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 1500 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 1500 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 1500 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 1500 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 1500 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 1500 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 1500 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 1500 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 1500 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 1500 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 1500 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 1500 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 1500 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 1500 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 1500 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 1500 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 1500 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 1500 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 1500 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 1500 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 1500 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 1500 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 1500 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 1500 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 1500 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 1500 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 1500 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 1500 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 1500 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 1500 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 1500 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 1500 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 1500 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 1500 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 1500 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 1500 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 1500 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 1500 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 1500 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 1500 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 1500 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 1500 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 1500 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 1500 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 1500 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 1500 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 1500 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 1500 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 1500 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 1500 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 1500 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 1500 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 1500 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 1500 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 1500 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 1500 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 1500 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 1500 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 1500 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 1500 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 1500 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 1500 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 1500 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 1500 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 1500 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 1500 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 1500 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 1500 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 1500 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "## Done ##\n",
      "total 507M\n",
      "drwxr-xr-x 2 root root   12K Aug 14 05:44 .\n",
      "drwxr-xr-x 6 root root    86 Aug 13 13:22 ..\n",
      "-rw-r--r-- 1 root root  196K Aug 14 05:44 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 05:44 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 05:44 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 05:44 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 05:44 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 05:44 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 05:44 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 05:44 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 05:44 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 05:44 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 05:44 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  244K Aug 14 05:44 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 05:44 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 05:44 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 05:44 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 05:44 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 05:44 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 05:44 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 05:44 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 05:44 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 05:44 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Aug 14 05:44 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  294K Aug 14 05:44 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Aug 14 05:44 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Aug 14 05:44 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.2M Aug 14 05:44 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Aug 14 05:44 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Aug 14 05:44 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Aug 14 05:44 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Aug 14 05:44 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Aug 14 05:44 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Aug 14 05:44 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Aug 14 05:44 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  341K Aug 14 05:44 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Aug 14 05:44 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Aug 14 05:44 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Aug 14 05:44 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Aug 14 05:44 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Aug 14 05:44 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Aug 14 05:44 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.2M Aug 14 05:44 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Aug 14 05:44 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Aug 14 05:44 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Aug 14 05:44 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  392K Aug 14 05:44 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Aug 14 05:44 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.5M Aug 14 05:44 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Aug 14 05:44 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.7M Aug 14 05:44 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Aug 14 05:44 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Aug 14 05:44 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.9M Aug 14 05:44 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Aug 14 05:44 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Aug 14 05:44 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Aug 14 05:44 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  435K Aug 14 05:44 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Aug 14 05:44 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Aug 14 05:44 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Aug 14 05:44 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Aug 14 05:44 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Aug 14 05:44 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Aug 14 05:44 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Aug 14 05:44 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.7M Aug 14 05:44 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.8M Aug 14 05:44 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.8M Aug 14 05:44 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  486K Aug 14 05:44 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.9M Aug 14 05:44 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.0M Aug 14 05:44 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.1M Aug 14 05:44 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.1M Aug 14 05:44 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.2M Aug 14 05:44 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.3M Aug 14 05:44 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.3M Aug 14 05:44 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.4M Aug 14 05:44 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.4M Aug 14 05:44 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.5M Aug 14 05:44 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.6M Aug 14 05:44 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.7M Aug 14 05:44 gen-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.8M Aug 14 05:44 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.8M Aug 14 05:44 gen-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.9M Aug 14 05:44 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  6.9M Aug 14 05:44 gen-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.0M Aug 14 05:44 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.1M Aug 14 05:44 gen-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.2M Aug 14 05:44 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.2M Aug 14 05:44 gen-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  146K Aug 14 05:44 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  874K Aug 14 05:44 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.3M Aug 14 05:44 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.4M Aug 14 05:44 gen-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.5M Aug 14 05:44 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.5M Aug 14 05:44 gen-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.6M Aug 14 05:44 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.7M Aug 14 05:44 gen-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.7M Aug 14 05:44 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.8M Aug 14 05:44 gen-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.9M Aug 14 05:44 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.9M Aug 14 05:44 gen-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  951K Aug 14 05:44 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  8.0M Aug 14 05:44 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 1017K Aug 14 05:44 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 05:44 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 05:44 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 05:44 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 05:44 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 05:44 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 05:44 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 05:44 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  501K Aug 14 05:44 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  567K Aug 14 05:44 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  559K Aug 14 05:44 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  557K Aug 14 05:44 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  557K Aug 14 05:44 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  547K Aug 14 05:44 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  552K Aug 14 05:44 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  548K Aug 14 05:44 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  551K Aug 14 05:44 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  548K Aug 14 05:44 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  541K Aug 14 05:44 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  438K Aug 14 05:44 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  547K Aug 14 05:44 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  544K Aug 14 05:44 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  538K Aug 14 05:44 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  539K Aug 14 05:44 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  540K Aug 14 05:44 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  540K Aug 14 05:44 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  546K Aug 14 05:44 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Aug 14 05:44 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Aug 14 05:44 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Aug 14 05:44 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  385K Aug 14 05:44 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Aug 14 05:44 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 05:44 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 05:44 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  538K Aug 14 05:44 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  532K Aug 14 05:44 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  539K Aug 14 05:44 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  541K Aug 14 05:44 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 05:44 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 05:44 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 05:44 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  347K Aug 14 05:44 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 05:44 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  535K Aug 14 05:44 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 05:44 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 05:44 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 05:44 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 05:44 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 05:44 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 05:44 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  535K Aug 14 05:44 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 05:44 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  340K Aug 14 05:44 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 05:44 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  327K Aug 14 05:44 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 05:44 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 05:44 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  535K Aug 14 05:44 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  321K Aug 14 05:44 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  309K Aug 14 05:44 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 05:44 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 05:44 shuffle-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 05:44 shuffle-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  813K Aug 14 05:44 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  605K Aug 14 05:44 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 05:44 shuffle-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 05:44 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 05:44 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 05:44 shuffle-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  606K Aug 14 05:44 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 05:44 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root  598K Aug 14 05:44 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  593K Aug 14 05:44 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  587K Aug 14 05:44 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  581K Aug 14 05:44 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  579K Aug 14 05:44 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  584K Aug 14 05:44 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  573K Aug 14 05:44 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  574K Aug 14 05:44 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  120K Aug 14 05:44 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for lower word count - and shift the focus upwards\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 510 words dataset\n",
    "# \n",
    "for i in {50..550..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 1500 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-1k/', '--model.lr_init=4e-4', '--model.lr_final=2e-4', '--data.max_token_size=1024', '--model.ctx_len=1024', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-512.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-1k/', '--model.lr_init=4e-4', '--model.lr_final=2e-4', '--data.max_token_size=1024', '--model.ctx_len=1024', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-512.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 628503088\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 628503088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230814_054417-19rgipxb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/19rgipxb\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [00:00<00:00, 276946.87it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0b7cca3fa4295269/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 520.39it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.27it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 4] Global seed set to 628503088\n",
      "[rank: 6] Global seed set to 628503088\n",
      "[rank: 5] Global seed set to 628503088\n",
      "[rank: 2] Global seed set to 628503088\n",
      "[rank: 7] Global seed set to 628503088\n",
      "[rank: 3] Global seed set to 628503088\n",
      "[rank: 1] Global seed set to 628503088\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0b7cca3fa4295269/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 72.61it/s]\n",
      "Map (num_proc=64):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 189949/201198 [00:11<00:01, 11221.34 examples/s][rank: 7] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-14 05:44:56,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192199/201198 [00:11<00:00, 9918.57 examples/s][rank: 4] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-14 05:44:56,671] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193237/201198 [00:11<00:00, 9858.08 examples/s][rank: 6] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-14 05:44:56,753] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-14 05:44:56,829] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-14 05:44:56,836] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-14 05:44:56,841] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 628503088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-14 05:44:56,852] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 628503088                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-14 05:45:10,016] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 2.000e-04 (0.0002)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07364726066589355 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10164856910705566 seconds\n",
      "Time to load fused_adam op: 0.1017603874206543 seconds\n",
      "Time to load fused_adam op: 0.10198068618774414 seconds\n",
      "Time to load fused_adam op: 0.10197734832763672 seconds\n",
      "Time to load fused_adam op: 0.1020195484161377 seconds\n",
      "Time to load fused_adam op: 0.10199785232543945 seconds\n",
      "Time to load fused_adam op: 0.1021726131439209 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07562470436096191 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10120964050292969 seconds\n",
      "Time to load utils op: 0.10259032249450684 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10175299644470215 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10197329521179199 seconds\n",
      "Time to load utils op: 0.1016845703125 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179615020751953 seconds\n",
      "Time to load utils op: 0.10163450241088867 seconds\n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002720355987548828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00023889541625976562 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00026345252990722656 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002455711364746094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002503395080566406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003216266632080078 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005168914794921875 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 800/25097 [05:03<2:33:37,  2.64it/s, v_num=ipxb, train/loss=0.0/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 25097/25097 [2:48:45<00:00,  2.48it/s, v_num=ipxb, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                  | 1/26 [00:00<00:06,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 2/26 [00:00<00:07,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 3/26 [00:00<00:06,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 4/26 [00:01<00:06,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–‹               | 5/26 [00:01<00:05,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 6/26 [00:01<00:05,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 7/26 [00:01<00:04,  4.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 8/26 [00:01<00:04,  4.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 9/26 [00:02<00:04,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 10/26 [00:02<00:03,  4.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 11/26 [00:02<00:03,  4.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 12/26 [00:02<00:03,  4.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 13/26 [00:03<00:03,  4.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 14/26 [00:03<00:02,  4.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 15/26 [00:03<00:02,  4.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 16/26 [00:03<00:02,  4.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 17/26 [00:03<00:02,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/26 [00:04<00:01,  4.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/26 [00:04<00:01,  4.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20/26 [00:04<00:01,  4.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/26 [00:04<00:01,  4.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 22/26 [00:04<00:00,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/26 [00:05<00:00,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 24/26 [00:05<00:00,  4.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/26 [00:05<00:00,  4.47it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 25097/25097 [2:48:58<00:00,  2.48it/s, v_num=ipxb, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 25097/25097 [2:48:58<00:00,  2.48it/s, v_num=ipxb, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 25097/25097 [2:49:16<00:00,  2.47it/s, v_num=ipxb, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–†â–ƒâ–‡â–ˆâ–…â–ƒâ–‚â–‚â–…â–â–„â–ƒâ–„â–†â–‡â–†â–†â–„â–†â–‡â–†â–†â–‚â–ƒâ–â–„â–‡â–ƒâ–‚â–‚â–†â–â–‚â–…â–â–„â–â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–†â–ˆâ–‚â–…â–…â–„â–ƒâ–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.05226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/19rgipxb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230814_054417-19rgipxb/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-1k (train-ctx=1k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-1k/\" \\\n",
    "        --model.lr_init=4e-4 \\\n",
    "        --model.lr_final=2e-4 \\\n",
    "        --data.max_token_size=1024 \\\n",
    "        --model.ctx_len=1024 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-ctx-1k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-ctx-1k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 14 08:35 ../model/EWR-1B5-E0_1-mem-ctx-1k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-1k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 99.1304347826087% similarity, with 114 matched token, and 1 token mismatch\n",
      "## Model validation for 120 tokens : 99.16666666666667% similarity, with 119 matched token, and 1 token mismatch\n",
      "## Model validation for 125 tokens : 99.2% similarity, with 124 matched token, and 1 token mismatch\n",
      "## Model validation for 130 tokens : 99.23076923076923% similarity, with 129 matched token, and 1 token mismatch\n",
      "## Model validation for 135 tokens : 99.25925925925925% similarity, with 134 matched token, and 1 token mismatch\n",
      "## Model validation for 140 tokens : 99.28571428571429% similarity, with 139 matched token, and 1 token mismatch\n",
      "## Model validation for 145 tokens : 99.3103448275862% similarity, with 144 matched token, and 1 token mismatch\n",
      "## Model validation for 150 tokens : 99.33333333333333% similarity, with 149 matched token, and 1 token mismatch\n",
      "## Model validation for 160 tokens : 99.375% similarity, with 159 matched token, and 1 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 100.0% similarity, with 180 matched token, and 0 token mismatch\n",
      "## Model validation for 190 tokens : 100.0% similarity, with 190 matched token, and 0 token mismatch\n",
      "## Model validation for 200 tokens : 99.5% similarity, with 199 matched token, and 1 token mismatch\n",
      "## Model validation for 210 tokens : 99.52380952380952% similarity, with 209 matched token, and 1 token mismatch\n",
      "## Model validation for 220 tokens : 99.54545454545455% similarity, with 219 matched token, and 1 token mismatch\n",
      "## Model validation for 230 tokens : 99.56521739130434% similarity, with 229 matched token, and 1 token mismatch\n",
      "## Model validation for 240 tokens : 99.58333333333333% similarity, with 239 matched token, and 1 token mismatch\n",
      "## Model validation for 250 tokens : 99.6% similarity, with 249 matched token, and 1 token mismatch\n",
      "## Model validation for 260 tokens : 99.23076923076923% similarity, with 258 matched token, and 2 token mismatch\n",
      "## Model validation for 270 tokens : 99.62962962962963% similarity, with 269 matched token, and 1 token mismatch\n",
      "## Model validation for 280 tokens : 99.28571428571429% similarity, with 278 matched token, and 2 token mismatch\n",
      "## Model validation for 290 tokens : 98.62068965517241% similarity, with 286 matched token, and 4 token mismatch\n",
      "## Model validation for 300 tokens : 98.66666666666667% similarity, with 296 matched token, and 4 token mismatch\n",
      "## Model validation for 325 tokens : 99.38461538461539% similarity, with 323 matched token, and 2 token mismatch\n",
      "## Model validation for 350 tokens : 98.28571428571429% similarity, with 344 matched token, and 6 token mismatch\n",
      "## Model validation for 375 tokens : 97.33333333333334% similarity, with 365 matched token, and 10 token mismatch\n",
      "## Model validation for 400 tokens : 95.0% similarity, with 380 matched token, and 20 token mismatch\n",
      "## Model validation for 425 tokens : 94.35294117647058% similarity, with 401 matched token, and 24 token mismatch\n",
      "## Model validation for 450 tokens : 91.55555555555556% similarity, with 412 matched token, and 38 token mismatch\n",
      "## Model validation for 475 tokens : 84.42105263157896% similarity, with 401 matched token, and 74 token mismatch\n",
      "## Model validation for 500 tokens : 77.8% similarity, with 389 matched token, and 111 token mismatch\n",
      "## Model validation for 525 tokens : 73.14285714285714% similarity, with 384 matched token, and 141 token mismatch\n",
      "## Model validation for 550 tokens : 67.27272727272727% similarity, with 370 matched token, and 180 token mismatch\n",
      "## Model validation for 575 tokens : 60.69565217391304% similarity, with 349 matched token, and 226 token mismatch\n",
      "## Model validation for 600 tokens : 54.166666666666664% similarity, with 325 matched token, and 275 token mismatch\n",
      "## Model validation for 625 tokens : 50.4% similarity, with 315 matched token, and 310 token mismatch\n",
      "## Model validation for 650 tokens : 44.0% similarity, with 286 matched token, and 364 token mismatch\n",
      "## Model validation for 675 tokens : 40.59259259259259% similarity, with 274 matched token, and 401 token mismatch\n",
      "## Model validation for 700 tokens : 36.285714285714285% similarity, with 254 matched token, and 446 token mismatch\n",
      "## Model validation for 750 tokens : 30.0% similarity, with 225 matched token, and 525 token mismatch\n",
      "## Model validation for 800 tokens : 24.125% similarity, with 193 matched token, and 607 token mismatch\n",
      "## Model validation for 850 tokens : 18.0% similarity, with 153 matched token, and 697 token mismatch\n",
      "## Model validation for 900 tokens : 12.777777777777777% similarity, with 115 matched token, and 785 token mismatch\n",
      "## Model validation for 950 tokens : 10.0% similarity, with 95 matched token, and 855 token mismatch\n",
      "## Model validation for 1000 tokens : 7.9% similarity, with 79 matched token, and 921 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 4 : Low ctx size (2048), memory training\n",
    "\n",
    "- Tune 4: Low ctx size (2048), Scaling up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 87 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 553 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 38 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 100 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 108 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 132 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 100 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 100 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 28 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 100 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 100 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 200 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 100 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 200 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 200 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 200 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 200 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 200 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 200 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 304 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 200 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated a single JSONL file with 382 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 397 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 200 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 100 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 286 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 27 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 200 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 200 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 326 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 200 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated a single JSONL file with 281 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 200 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 200 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 344 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 200 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 200 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 200 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 200 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 200 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 200 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated a single JSONL file with 298 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 200 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 413 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 200 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 203 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 200 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 146 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 357 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 200 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 144 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 367 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 200 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated a single JSONL file with 317 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 194 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 200 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 200 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 200 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 200 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 200 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 200 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 200 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 200 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 200 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated a single JSONL file with 340 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 200 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 200 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 200 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 200 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 200 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 200 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 200 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 200 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated a single JSONL file with 287 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 200 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 200 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 219 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 200 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 200 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 200 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 200 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 200 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1025 max words - at ../dataset/shuffle-word-1025-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 274 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 200 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 200 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 200 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 200 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 200 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 200 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 200 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1185 max words - at ../dataset/shuffle-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 200 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 200 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated a single JSONL file with 292 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 200 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 98 samples (20 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 114 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1225 max words - at ../dataset/shuffle-word-1225-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 200 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 200 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 200 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 214 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 200 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1275 max words - at ../dataset/shuffle-word-1275-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 200 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 200 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (20 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 200 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 200 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 98 samples (20 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1425 max words - at ../dataset/shuffle-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 200 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 200 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 200 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 200 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 200 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 200 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 200 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated a single JSONL file with 271 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 200 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1135 max words - at ../dataset/shuffle-word-1135-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 200 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 200 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1105 max words - at ../dataset/shuffle-word-1105-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 200 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 200 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1155 max words - at ../dataset/shuffle-word-1155-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 200 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 200 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 200 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 200 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (20 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 200 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1295 max words - at ../dataset/shuffle-word-1295-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 200 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 200 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 200 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 200 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 200 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 200 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 200 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1035 max words - at ../dataset/shuffle-word-1035-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1235 max words - at ../dataset/shuffle-word-1235-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 200 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1385 max words - at ../dataset/shuffle-word-1385-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 200 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 200 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 200 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 200 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1315 max words - at ../dataset/shuffle-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1325 max words - at ../dataset/shuffle-word-1325-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 200 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 200 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 200 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 200 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1125 max words - at ../dataset/shuffle-word-1125-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 200 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 200 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 200 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 200 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 200 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 200 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 200 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1015 max words - at ../dataset/shuffle-word-1015-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1115 max words - at ../dataset/shuffle-word-1115-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 200 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 200 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 200 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 200 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 200 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 200 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1435 max words - at ../dataset/shuffle-word-1435-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1145 max words - at ../dataset/shuffle-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1205 max words - at ../dataset/shuffle-word-1205-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1045 max words - at ../dataset/shuffle-word-1045-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 200 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1485 max words - at ../dataset/shuffle-word-1485-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 200 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 200 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 200 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 200 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1005 max words - at ../dataset/shuffle-word-1005-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 200 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 200 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 200 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 200 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 200 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1305 max words - at ../dataset/shuffle-word-1305-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 200 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1285 max words - at ../dataset/shuffle-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1395 max words - at ../dataset/shuffle-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1405 max words - at ../dataset/shuffle-word-1405-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 200 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 200 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1085 max words - at ../dataset/shuffle-word-1085-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1055 max words - at ../dataset/shuffle-word-1055-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 200 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 200 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 200 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 200 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 200 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 200 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1245 max words - at ../dataset/shuffle-word-1245-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 200 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 200 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated a single JSONL file with 54 samples (20 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated JSONL file with - 1015 max words, 200 samples - at ../dataset/gen-word-1015-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 200 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 200 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1165 max words - at ../dataset/shuffle-word-1165-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 200 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 200 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 200 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 200 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated JSONL file with - 1105 max words, 200 samples - at ../dataset/gen-word-1105-count.jsonl\n",
      "Generated JSONL file with - 1055 max words, 200 samples - at ../dataset/gen-word-1055-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated JSONL file with - 1425 max words, 200 samples - at ../dataset/gen-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 200 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 200 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 200 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 200 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 200 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 200 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1095 max words - at ../dataset/shuffle-word-1095-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 200 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 200 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated a single JSONL file with 54 samples (20 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated JSONL file with - 1185 max words, 200 samples - at ../dataset/gen-word-1185-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 200 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 200 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 200 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 200 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1075 max words - at ../dataset/shuffle-word-1075-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 200 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 200 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 200 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 200 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 200 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 200 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 1325 max words, 200 samples - at ../dataset/gen-word-1325-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 200 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated JSONL file with - 1235 max words, 200 samples - at ../dataset/gen-word-1235-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 200 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 200 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1335 max words - at ../dataset/shuffle-word-1335-count.jsonl\n",
      "Generated JSONL file with - 1095 max words, 200 samples - at ../dataset/gen-word-1095-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 200 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated JSONL file with - 1225 max words, 200 samples - at ../dataset/gen-word-1225-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 200 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 200 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1075 max words, 200 samples - at ../dataset/gen-word-1075-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 200 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 200 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 200 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 200 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 200 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 1395 max words, 200 samples - at ../dataset/gen-word-1395-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 200 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 200 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 200 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 200 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 200 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 200 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 200 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 1295 max words, 200 samples - at ../dataset/gen-word-1295-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 200 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1195 max words - at ../dataset/shuffle-word-1195-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 200 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated JSONL file with - 1485 max words, 200 samples - at ../dataset/gen-word-1485-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 200 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated JSONL file with - 1245 max words, 200 samples - at ../dataset/gen-word-1245-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 200 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1355 max words - at ../dataset/shuffle-word-1355-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 200 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 200 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated JSONL file with - 1145 max words, 200 samples - at ../dataset/gen-word-1145-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 200 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1335 max words, 200 samples - at ../dataset/gen-word-1335-count.jsonl\n",
      "Generated JSONL file with - 1215 max words, 200 samples - at ../dataset/gen-word-1215-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 200 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 200 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 200 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1465 max words - at ../dataset/shuffle-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1065 max words - at ../dataset/shuffle-word-1065-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 200 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1255 max words - at ../dataset/shuffle-word-1255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 200 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1115 max words, 200 samples - at ../dataset/gen-word-1115-count.jsonl\n",
      "Generated JSONL file with - 1205 max words, 200 samples - at ../dataset/gen-word-1205-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1495 max words - at ../dataset/shuffle-word-1495-count.jsonl\n",
      "Generated JSONL file with - 1155 max words, 200 samples - at ../dataset/gen-word-1155-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 200 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 200 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 1045 max words, 200 samples - at ../dataset/gen-word-1045-count.jsonl\n",
      "Generated JSONL file with - 1405 max words, 200 samples - at ../dataset/gen-word-1405-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 200 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 200 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 1025 max words, 200 samples - at ../dataset/gen-word-1025-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1175 max words - at ../dataset/shuffle-word-1175-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 200 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1315 max words, 200 samples - at ../dataset/gen-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1415 max words - at ../dataset/shuffle-word-1415-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 200 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1375 max words - at ../dataset/shuffle-word-1375-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1445 max words - at ../dataset/shuffle-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1455 max words - at ../dataset/shuffle-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 200 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated JSONL file with - 1365 max words, 200 samples - at ../dataset/gen-word-1365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1475 max words - at ../dataset/shuffle-word-1475-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 200 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated JSONL file with - 1475 max words, 200 samples - at ../dataset/gen-word-1475-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 200 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1265 max words - at ../dataset/shuffle-word-1265-count.jsonl\n",
      "Generated JSONL file with - 1065 max words, 200 samples - at ../dataset/gen-word-1065-count.jsonl\n",
      "Generated JSONL file with - 1125 max words, 200 samples - at ../dataset/gen-word-1125-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 200 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 200 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 200 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 200 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated JSONL file with - 1255 max words, 200 samples - at ../dataset/gen-word-1255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 200 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 200 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 200 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 200 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 200 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 200 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 200 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1365 max words - at ../dataset/shuffle-word-1365-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 200 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 200 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 200 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1005 max words, 200 samples - at ../dataset/gen-word-1005-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 200 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 200 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 200 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1345 max words - at ../dataset/shuffle-word-1345-count.jsonl\n",
      "Generated JSONL file with - 1085 max words, 200 samples - at ../dataset/gen-word-1085-count.jsonl\n",
      "Generated JSONL file with - 1355 max words, 200 samples - at ../dataset/gen-word-1355-count.jsonl\n",
      "Generated JSONL file with - 1445 max words, 200 samples - at ../dataset/gen-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated JSONL file with - 1135 max words, 200 samples - at ../dataset/gen-word-1135-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1265 max words, 200 samples - at ../dataset/gen-word-1265-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 200 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 200 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 45 samples (20 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1465 max words, 200 samples - at ../dataset/gen-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1215 max words - at ../dataset/shuffle-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 58 samples (20 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1305 max words, 200 samples - at ../dataset/gen-word-1305-count.jsonl\n",
      "Generated JSONL file with - 1285 max words, 200 samples - at ../dataset/gen-word-1285-count.jsonl\n",
      "Generated JSONL file with - 1375 max words, 200 samples - at ../dataset/gen-word-1375-count.jsonl\n",
      "Generated JSONL file with - 1345 max words, 200 samples - at ../dataset/gen-word-1345-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 200 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1275 max words, 200 samples - at ../dataset/gen-word-1275-count.jsonl\n",
      "Generated JSONL file with - 1195 max words, 200 samples - at ../dataset/gen-word-1195-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 200 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 200 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 200 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1165 max words, 200 samples - at ../dataset/gen-word-1165-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 200 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1455 max words, 200 samples - at ../dataset/gen-word-1455-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 200 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 200 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 1495 max words, 200 samples - at ../dataset/gen-word-1495-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 200 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 200 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1415 max words, 200 samples - at ../dataset/gen-word-1415-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 200 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1175 max words, 200 samples - at ../dataset/gen-word-1175-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 200 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated JSONL file with - 1435 max words, 200 samples - at ../dataset/gen-word-1435-count.jsonl\n",
      "Generated JSONL file with - 1385 max words, 200 samples - at ../dataset/gen-word-1385-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 200 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 200 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 200 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1035 max words, 200 samples - at ../dataset/gen-word-1035-count.jsonl\n",
      "## Done ##\n",
      "total 579M\n",
      "drwxr-xr-x 2 root root   28K Aug 14 09:41 .\n",
      "drwxr-xr-x 6 root root    86 Aug 13 13:22 ..\n",
      "-rw-r--r-- 1 root root   20K Aug 14 09:41 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  108K Aug 14 09:41 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1005-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1015-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1025-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1035-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1045-count.jsonl\n",
      "-rw-r--r-- 1 root root  225K Aug 14 09:41 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1055-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1065-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1075-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1085-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Aug 14 09:41 gen-word-1095-count.jsonl\n",
      "-rw-r--r-- 1 root root  235K Aug 14 09:41 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1105-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1115-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1135-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1145-count.jsonl\n",
      "-rw-r--r-- 1 root root  249K Aug 14 09:41 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Aug 14 09:41 gen-word-1155-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1165-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1175-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1185-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1195-count.jsonl\n",
      "-rw-r--r-- 1 root root  254K Aug 14 09:41 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.3M Aug 14 09:41 gen-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1205-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1215-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1225-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1235-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1245-count.jsonl\n",
      "-rw-r--r-- 1 root root  261K Aug 14 09:41 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.4M Aug 14 09:41 gen-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1255-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1265-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1275-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1285-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1295-count.jsonl\n",
      "-rw-r--r-- 1 root root  270K Aug 14 09:41 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Aug 14 09:41 gen-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1305-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1315-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1325-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1335-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1345-count.jsonl\n",
      "-rw-r--r-- 1 root root  280K Aug 14 09:41 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.6M Aug 14 09:41 gen-word-1355-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1365-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1375-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1385-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1395-count.jsonl\n",
      "-rw-r--r-- 1 root root  294K Aug 14 09:41 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1405-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.7M Aug 14 09:41 gen-word-1415-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1425-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1435-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1445-count.jsonl\n",
      "-rw-r--r-- 1 root root  299K Aug 14 09:41 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1455-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Aug 14 09:41 gen-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1465-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1475-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1485-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1495-count.jsonl\n",
      "-rw-r--r-- 1 root root   25K Aug 14 09:41 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  313K Aug 14 09:41 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Aug 14 09:41 gen-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  316K Aug 14 09:41 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  326K Aug 14 09:41 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  342K Aug 14 09:41 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  352K Aug 14 09:41 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  356K Aug 14 09:41 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  373K Aug 14 09:41 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  384K Aug 14 09:41 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  386K Aug 14 09:41 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  396K Aug 14 09:41 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root   29K Aug 14 09:41 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  411K Aug 14 09:41 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  416K Aug 14 09:41 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  432K Aug 14 09:41 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  445K Aug 14 09:41 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  453K Aug 14 09:41 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  458K Aug 14 09:41 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  467K Aug 14 09:41 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  474K Aug 14 09:41 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  487K Aug 14 09:41 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  500K Aug 14 09:41 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root   32K Aug 14 09:41 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  504K Aug 14 09:41 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  539K Aug 14 09:41 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  551K Aug 14 09:41 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  553K Aug 14 09:41 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  567K Aug 14 09:41 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  575K Aug 14 09:41 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  590K Aug 14 09:41 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  594K Aug 14 09:41 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root   41K Aug 14 09:41 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  616K Aug 14 09:41 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  608K Aug 14 09:41 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  617K Aug 14 09:41 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  631K Aug 14 09:41 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  644K Aug 14 09:41 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  648K Aug 14 09:41 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  664K Aug 14 09:41 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  666K Aug 14 09:41 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  679K Aug 14 09:41 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  683K Aug 14 09:41 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root   44K Aug 14 09:41 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  683K Aug 14 09:41 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  711K Aug 14 09:41 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  719K Aug 14 09:41 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  735K Aug 14 09:41 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  741K Aug 14 09:41 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  759K Aug 14 09:41 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  752K Aug 14 09:41 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  771K Aug 14 09:41 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  776K Aug 14 09:41 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  787K Aug 14 09:41 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root   49K Aug 14 09:41 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  797K Aug 14 09:41 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  806K Aug 14 09:41 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  815K Aug 14 09:41 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  824K Aug 14 09:41 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  839K Aug 14 09:41 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  850K Aug 14 09:41 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  859K Aug 14 09:41 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  863K Aug 14 09:41 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  877K Aug 14 09:41 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  893K Aug 14 09:41 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root   54K Aug 14 09:41 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  893K Aug 14 09:41 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  906K Aug 14 09:41 gen-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  908K Aug 14 09:41 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  932K Aug 14 09:41 gen-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  928K Aug 14 09:41 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  944K Aug 14 09:41 gen-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  953K Aug 14 09:41 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  956K Aug 14 09:41 gen-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  975K Aug 14 09:41 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  976K Aug 14 09:41 gen-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root   15K Aug 14 09:41 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root   59K Aug 14 09:41 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  993K Aug 14 09:41 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root 1012K Aug 14 09:41 gen-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root 1005K Aug 14 09:41 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root 1019K Aug 14 09:41 gen-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root   63K Aug 14 09:41 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Aug 14 09:41 gen-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root   69K Aug 14 09:41 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Aug 14 09:41 gen-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root   76K Aug 14 09:41 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Aug 14 09:41 gen-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root   77K Aug 14 09:41 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Aug 14 09:41 gen-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root   82K Aug 14 09:41 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Aug 14 09:41 gen-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root   86K Aug 14 09:41 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Aug 14 09:41 gen-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root   92K Aug 14 09:41 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Aug 14 09:41 gen-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root   99K Aug 14 09:41 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Aug 14 09:41 gen-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root   99K Aug 14 09:41 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Aug 14 09:41 gen-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Aug 14 09:41 gen-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root   53K Aug 14 09:41 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root   29K Aug 14 09:41 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1005-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1015-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1025-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1035-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1045-count.jsonl\n",
      "-rw-r--r-- 1 root root  560K Aug 14 09:41 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1055-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-1065-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1075-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1085-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1095-count.jsonl\n",
      "-rw-r--r-- 1 root root  546K Aug 14 09:41 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1105-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1115-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1125-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1135-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1145-count.jsonl\n",
      "-rw-r--r-- 1 root root  544K Aug 14 09:41 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1155-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-1165-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1175-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1185-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-1195-count.jsonl\n",
      "-rw-r--r-- 1 root root  548K Aug 14 09:41 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1205-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1215-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1225-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1235-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1245-count.jsonl\n",
      "-rw-r--r-- 1 root root  553K Aug 14 09:41 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1255-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1265-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1275-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1285-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1295-count.jsonl\n",
      "-rw-r--r-- 1 root root  541K Aug 14 09:41 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1305-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1315-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1325-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1335-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1345-count.jsonl\n",
      "-rw-r--r-- 1 root root  555K Aug 14 09:41 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1355-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1365-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1375-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1385-count.jsonl\n",
      "-rw-r--r-- 1 root root  517K Aug 14 09:41 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1395-count.jsonl\n",
      "-rw-r--r-- 1 root root  542K Aug 14 09:41 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-1405-count.jsonl\n",
      "-rw-r--r-- 1 root root  518K Aug 14 09:41 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1415-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1425-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1435-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1445-count.jsonl\n",
      "-rw-r--r-- 1 root root  546K Aug 14 09:41 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1455-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-1465-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1475-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1485-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-1495-count.jsonl\n",
      "-rw-r--r-- 1 root root   44K Aug 14 09:41 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Aug 14 09:41 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  552K Aug 14 09:41 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  540K Aug 14 09:41 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  542K Aug 14 09:41 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  545K Aug 14 09:41 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  546K Aug 14 09:41 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  550K Aug 14 09:41 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  540K Aug 14 09:41 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  545K Aug 14 09:41 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  540K Aug 14 09:41 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root   39K Aug 14 09:41 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  541K Aug 14 09:41 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  541K Aug 14 09:41 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 09:41 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 09:41 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 09:41 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  535K Aug 14 09:41 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 09:41 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  538K Aug 14 09:41 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 09:41 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  532K Aug 14 09:41 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root   36K Aug 14 09:41 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 09:41 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  534K Aug 14 09:41 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  537K Aug 14 09:41 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 09:41 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  535K Aug 14 09:41 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root   33K Aug 14 09:41 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  536K Aug 14 09:41 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root   34K Aug 14 09:41 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  532K Aug 14 09:41 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Aug 14 09:41 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  532K Aug 14 09:41 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  532K Aug 14 09:41 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root   31K Aug 14 09:41 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  531K Aug 14 09:41 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root   35K Aug 14 09:41 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 09:41 shuffle-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 09:41 shuffle-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 09:41 shuffle-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 09:41 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root   83K Aug 14 09:41 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root   34K Aug 14 09:41 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  530K Aug 14 09:41 shuffle-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root   31K Aug 14 09:41 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Aug 14 09:41 shuffle-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root   32K Aug 14 09:41 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root   30K Aug 14 09:41 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root   30K Aug 14 09:41 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root  528K Aug 14 09:41 shuffle-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root   29K Aug 14 09:41 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root  527K Aug 14 09:41 shuffle-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root   29K Aug 14 09:41 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root  526K Aug 14 09:41 shuffle-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root   29K Aug 14 09:41 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root  525K Aug 14 09:41 shuffle-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root   28K Aug 14 09:41 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  519K Aug 14 09:41 shuffle-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root   30K Aug 14 09:41 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root  520K Aug 14 09:41 shuffle-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  523K Aug 14 09:41 shuffle-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root  522K Aug 14 09:41 shuffle-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  524K Aug 14 09:41 shuffle-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Aug 14 09:41 shuffle-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root   12K Aug 14 09:41 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for lower word count - and shift the focus upwards\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 100 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 105+ - 1050 words dataset\n",
    "# \n",
    "for i in {105..1500..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 200 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-2k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=2048', '--model.ctx_len=2048', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-1k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-2k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=2048', '--model.ctx_len=2048', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-1k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2122500543\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2122500543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230814_094228-tkseym9v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/tkseym9v\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 601/601 [00:00<00:00, 300916.40it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-35c0b6444d2f58b1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 182.65it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.72it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 4] Global seed set to 2122500543\n",
      "[rank: 7] Global seed set to 2122500543\n",
      "[rank: 6] Global seed set to 2122500543\n",
      "[rank: 2] Global seed set to 2122500543\n",
      "[rank: 5] Global seed set to 2122500543\n",
      "[rank: 1] Global seed set to 2122500543\n",
      "[rank: 3] Global seed set to 2122500543\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-35c0b6444d2f58b1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 74.61it/s]\n",
      "Map (num_proc=64):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 79384/88114 [00:11<00:01, 6569.85 examples/s][rank: 6] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-14 09:43:09,170] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-14 09:43:09,209] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 80689/88114 [00:12<00:01, 6014.14 examples/s][rank: 4] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-14 09:43:09,506] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-14 09:43:09,516] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 81304/88114 [00:12<00:01, 5704.16 examples/s][rank: 5] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-14 09:43:09,537] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-14 09:43:09,567] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2122500543\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-14 09:43:09,622] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 2122500543                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-14 09:43:23,668] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /root/rwkv-x-playground/checkpoint/EWR-1B5-E0_1-mem-ctx-2k exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07239675521850586 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10104918479919434 seconds\n",
      "Time to load fused_adam op: 0.10123538970947266 seconds\n",
      "Time to load fused_adam op: 0.10130882263183594 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10164046287536621 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1016993522644043 seconds\n",
      "Time to load fused_adam op: 0.10155963897705078 seconds\n",
      "Time to load fused_adam op: 0.1015787124633789 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0640861988067627 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10117936134338379 seconds\n",
      "Time to load utils op: 0.10112857818603516 seconds\n",
      "Time to load utils op: 0.10144543647766113 seconds\n",
      "Time to load utils op: 0.10188174247741699 seconds\n",
      "Time to load utils op: 0.10179328918457031 seconds\n",
      "Time to load utils op: 0.10196232795715332 seconds\n",
      "Time to load utils op: 0.1022939682006836 seconds\n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002722740173339844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028324127197265625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000232696533203125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00022864341735839844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002803802490234375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023746490478515625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005440711975097656 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   8%| | 800/9830 [08:05<1:31:16,  1.65it/s, v_num=ym9v, train/loss=0.73/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 9830/9830 [1:44:01<00:00,  1.57it/s, v_num=ym9v, train/loss=0.0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 1/10 [00:00<00:04,  2.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–Š               | 2/10 [00:00<00:03,  2.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3/10 [00:01<00:03,  2.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 4/10 [00:01<00:02,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 5/10 [00:02<00:02,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 6/10 [00:02<00:01,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7/10 [00:02<00:01,  2.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/10 [00:03<00:00,  2.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9/10 [00:03<00:00,  2.67it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 9830/9830 [1:44:12<00:00,  1.57it/s, v_num=ym9v, train/loss=0.0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 9830/9830 [1:44:12<00:00,  1.57it/s, v_num=ym9v, train/loss=0.0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 9830/9830 [1:44:37<00:00,  1.57it/s, v_num=ym9v, train/loss=0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–†â–…â–ƒâ–‡â–‚â–ˆâ–‚â–‡â–‚â–ƒâ–„â–„â–‚â–‡â–‚â–‡â–ˆâ–…â–†â–‡â–†â–…â–‡â–‡â–†â–‡â–…â–„â–ƒâ–†â–â–ƒâ–ƒâ–…â–„â–…â–†â–„â–‚â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–„â–‚â–„â–â–ƒâ–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.05033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/tkseym9v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230814_094228-tkseym9v/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-2k (train-ctx=2k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-2k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=2048 \\\n",
    "        --model.ctx_len=2048 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-ctx-2k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-ctx-2k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 14 11:29 ../model/EWR-1B5-E0_1-mem-ctx-2k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-2k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-2k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-2k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 100.0% similarity, with 150 matched token, and 0 token mismatch\n",
      "## Model validation for 160 tokens : 100.0% similarity, with 160 matched token, and 0 token mismatch\n",
      "## Model validation for 170 tokens : 100.0% similarity, with 170 matched token, and 0 token mismatch\n",
      "## Model validation for 180 tokens : 100.0% similarity, with 180 matched token, and 0 token mismatch\n",
      "## Model validation for 190 tokens : 100.0% similarity, with 190 matched token, and 0 token mismatch\n",
      "## Model validation for 200 tokens : 100.0% similarity, with 200 matched token, and 0 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 99.54545454545455% similarity, with 219 matched token, and 1 token mismatch\n",
      "## Model validation for 230 tokens : 99.56521739130434% similarity, with 229 matched token, and 1 token mismatch\n",
      "## Model validation for 240 tokens : 99.58333333333333% similarity, with 239 matched token, and 1 token mismatch\n",
      "## Model validation for 250 tokens : 99.6% similarity, with 249 matched token, and 1 token mismatch\n",
      "## Model validation for 260 tokens : 99.61538461538461% similarity, with 259 matched token, and 1 token mismatch\n",
      "## Model validation for 270 tokens : 99.62962962962963% similarity, with 269 matched token, and 1 token mismatch\n",
      "## Model validation for 280 tokens : 99.28571428571429% similarity, with 278 matched token, and 2 token mismatch\n",
      "## Model validation for 290 tokens : 99.3103448275862% similarity, with 288 matched token, and 2 token mismatch\n",
      "## Model validation for 300 tokens : 99.33333333333333% similarity, with 298 matched token, and 2 token mismatch\n",
      "## Model validation for 325 tokens : 99.07692307692308% similarity, with 322 matched token, and 3 token mismatch\n",
      "## Model validation for 350 tokens : 98.85714285714286% similarity, with 346 matched token, and 4 token mismatch\n",
      "## Model validation for 375 tokens : 98.4% similarity, with 369 matched token, and 6 token mismatch\n",
      "## Model validation for 400 tokens : 98.5% similarity, with 394 matched token, and 6 token mismatch\n",
      "## Model validation for 425 tokens : 98.58823529411764% similarity, with 419 matched token, and 6 token mismatch\n",
      "## Model validation for 450 tokens : 98.44444444444444% similarity, with 443 matched token, and 7 token mismatch\n",
      "## Model validation for 475 tokens : 98.52631578947368% similarity, with 468 matched token, and 7 token mismatch\n",
      "## Model validation for 500 tokens : 98.0% similarity, with 490 matched token, and 10 token mismatch\n",
      "## Model validation for 525 tokens : 97.71428571428571% similarity, with 513 matched token, and 12 token mismatch\n",
      "## Model validation for 550 tokens : 98.0% similarity, with 539 matched token, and 11 token mismatch\n",
      "## Model validation for 575 tokens : 98.26086956521739% similarity, with 565 matched token, and 10 token mismatch\n",
      "## Model validation for 600 tokens : 98.0% similarity, with 588 matched token, and 12 token mismatch\n",
      "## Model validation for 625 tokens : 97.92% similarity, with 612 matched token, and 13 token mismatch\n",
      "## Model validation for 650 tokens : 97.84615384615385% similarity, with 636 matched token, and 14 token mismatch\n",
      "## Model validation for 675 tokens : 97.62962962962963% similarity, with 659 matched token, and 16 token mismatch\n",
      "## Model validation for 700 tokens : 97.14285714285714% similarity, with 680 matched token, and 20 token mismatch\n",
      "## Model validation for 750 tokens : 95.33333333333334% similarity, with 715 matched token, and 35 token mismatch\n",
      "## Model validation for 800 tokens : 92.125% similarity, with 737 matched token, and 63 token mismatch\n",
      "## Model validation for 850 tokens : 87.6470588235294% similarity, with 745 matched token, and 105 token mismatch\n",
      "## Model validation for 900 tokens : 82.0% similarity, with 738 matched token, and 162 token mismatch\n",
      "## Model validation for 950 tokens : 75.36842105263158% similarity, with 716 matched token, and 234 token mismatch\n",
      "## Model validation for 1000 tokens : 68.10000000000001% similarity, with 681 matched token, and 319 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-2k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 5 : Ramping up the ctx size (4096), memory training\n",
    "\n",
    "- Tune 5: Mid ctx size (4096), Scaling up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 100 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 573 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 131 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 178 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 28 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 54 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 86 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 261 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 100 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 100 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 36 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 32 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 32 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 100 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 100 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 100 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 100 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 100 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 100 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 100 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 100 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 100 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 100 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 100 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 100 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 100 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 100 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 100 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 100 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 106 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 100 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 100 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 100 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 100 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 100 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 100 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 100 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 100 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 100 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 100 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 100 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 100 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 100 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 100 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 100 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 100 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 100 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 100 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 100 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 100 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 100 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 124 samples (40 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 100 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 100 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 100 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 100 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 100 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 100 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 162 samples (40 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 100 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 100 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 100 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1870 max words - at ../dataset/shuffle-word-1870-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 100 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated a single JSONL file with 130 samples (40 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 100 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 100 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 100 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 100 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 100 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1860 max words - at ../dataset/shuffle-word-1860-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 100 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 100 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 100 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 100 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 100 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 100 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 100 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 100 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1520 max words - at ../dataset/shuffle-word-1520-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 100 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 100 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1820 max words - at ../dataset/shuffle-word-1820-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 100 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 100 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 100 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 100 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1780 max words - at ../dataset/shuffle-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2350 max words - at ../dataset/shuffle-word-2350-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 100 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated a single JSONL file with 161 samples (40 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 100 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1510 max words - at ../dataset/shuffle-word-1510-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 400 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 72 samples (40 token repeat) - 2490 max words - at ../dataset/shuffle-word-2490-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1620 max words - at ../dataset/shuffle-word-1620-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 100 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1930 max words - at ../dataset/shuffle-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 198 samples (40 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 161 samples (40 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated a single JSONL file with 159 samples (40 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 100 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2230 max words - at ../dataset/shuffle-word-2230-count.jsonl\n",
      "Generated a single JSONL file with 161 samples (40 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2340 max words - at ../dataset/shuffle-word-2340-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 100 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 400 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 100 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (40 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1590 max words - at ../dataset/shuffle-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 126 samples (40 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (40 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2360 max words - at ../dataset/shuffle-word-2360-count.jsonl\n",
      "Generated a single JSONL file with 161 samples (40 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1610 max words - at ../dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 400 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1880 max words - at ../dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2200 max words - at ../dataset/shuffle-word-2200-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 100 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 100 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated a single JSONL file with 96 samples (40 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1670 max words - at ../dataset/shuffle-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 400 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (40 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2260 max words - at ../dataset/shuffle-word-2260-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 400 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 400 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated a single JSONL file with 159 samples (40 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 100 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 100 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2270 max words - at ../dataset/shuffle-word-2270-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1920 max words - at ../dataset/shuffle-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1950 max words - at ../dataset/shuffle-word-1950-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 100 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 100 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 400 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 83 samples (40 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 159 samples (40 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (40 token repeat) - 2370 max words - at ../dataset/shuffle-word-2370-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1580 max words - at ../dataset/shuffle-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 128 samples (40 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1690 max words - at ../dataset/shuffle-word-1690-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 400 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2130 max words - at ../dataset/shuffle-word-2130-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1760 max words - at ../dataset/shuffle-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 400 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 400 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1570 max words - at ../dataset/shuffle-word-1570-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 400 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (40 token repeat) - 2460 max words - at ../dataset/shuffle-word-2460-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1970 max words - at ../dataset/shuffle-word-1970-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1660 max words - at ../dataset/shuffle-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 104 samples (40 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1790 max words - at ../dataset/shuffle-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1810 max words - at ../dataset/shuffle-word-1810-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (40 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2050 max words - at ../dataset/shuffle-word-2050-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1980 max words - at ../dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 127 samples (40 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 100 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2470 max words - at ../dataset/shuffle-word-2470-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 130 samples (40 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2210 max words - at ../dataset/shuffle-word-2210-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1540 max words - at ../dataset/shuffle-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2190 max words - at ../dataset/shuffle-word-2190-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 100 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (40 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 400 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 100 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated a single JSONL file with 102 samples (40 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 100 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1960 max words - at ../dataset/shuffle-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2060 max words - at ../dataset/shuffle-word-2060-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 400 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2020 max words - at ../dataset/shuffle-word-2020-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1770 max words - at ../dataset/shuffle-word-1770-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (40 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 400 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 400 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (40 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 400 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2150 max words - at ../dataset/shuffle-word-2150-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2160 max words - at ../dataset/shuffle-word-2160-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (40 token repeat) - 2410 max words - at ../dataset/shuffle-word-2410-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 400 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (40 token repeat) - 2450 max words - at ../dataset/shuffle-word-2450-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2400 max words - at ../dataset/shuffle-word-2400-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 400 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2120 max words - at ../dataset/shuffle-word-2120-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (40 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2110 max words - at ../dataset/shuffle-word-2110-count.jsonl\n",
      "Generated a single JSONL file with 71 samples (40 token repeat) - 2440 max words - at ../dataset/shuffle-word-2440-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1640 max words - at ../dataset/shuffle-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (40 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated a single JSONL file with 159 samples (40 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 400 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 400 samples - at ../dataset/gen-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1650 max words - at ../dataset/shuffle-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (40 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2250 max words - at ../dataset/shuffle-word-2250-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (40 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 400 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 126 samples (40 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 400 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (40 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 400 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 400 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1730 max words - at ../dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1840 max words - at ../dataset/shuffle-word-1840-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 100 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 400 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated a single JSONL file with 198 samples (40 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 400 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 126 samples (40 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (40 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2030 max words - at ../dataset/shuffle-word-2030-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 400 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 73 samples (40 token repeat) - 2500 max words - at ../dataset/shuffle-word-2500-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2310 max words - at ../dataset/shuffle-word-2310-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1530 max words - at ../dataset/shuffle-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 159 samples (40 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2040 max words - at ../dataset/shuffle-word-2040-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1990 max words - at ../dataset/shuffle-word-1990-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2010 max words - at ../dataset/shuffle-word-2010-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1910 max words - at ../dataset/shuffle-word-1910-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1550 max words - at ../dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2170 max words - at ../dataset/shuffle-word-2170-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1710 max words - at ../dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 70 samples (40 token repeat) - 2430 max words - at ../dataset/shuffle-word-2430-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2390 max words - at ../dataset/shuffle-word-2390-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1830 max words - at ../dataset/shuffle-word-1830-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1750 max words - at ../dataset/shuffle-word-1750-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (40 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 400 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 400 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (40 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 400 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 400 samples - at ../dataset/gen-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2080 max words - at ../dataset/shuffle-word-2080-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 400 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1630 max words - at ../dataset/shuffle-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2100 max words - at ../dataset/shuffle-word-2100-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 400 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1560 max words - at ../dataset/shuffle-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 102 samples (40 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 400 samples - at ../dataset/gen-word-1560-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 400 samples - at ../dataset/gen-word-1530-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 400 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2090 max words - at ../dataset/shuffle-word-2090-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 400 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1740 max words - at ../dataset/shuffle-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 400 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 400 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 400 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated a single JSONL file with 128 samples (40 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 400 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2300 max words - at ../dataset/shuffle-word-2300-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (40 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2280 max words - at ../dataset/shuffle-word-2280-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 400 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2380 max words - at ../dataset/shuffle-word-2380-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2140 max words - at ../dataset/shuffle-word-2140-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2070 max words - at ../dataset/shuffle-word-2070-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1850 max words - at ../dataset/shuffle-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1680 max words - at ../dataset/shuffle-word-1680-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 100 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2320 max words - at ../dataset/shuffle-word-2320-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2180 max words - at ../dataset/shuffle-word-2180-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 400 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2220 max words - at ../dataset/shuffle-word-2220-count.jsonl\n",
      "Generated a single JSONL file with 127 samples (40 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 400 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2290 max words - at ../dataset/shuffle-word-2290-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1890 max words - at ../dataset/shuffle-word-1890-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 400 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 400 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 400 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 2240 max words - at ../dataset/shuffle-word-2240-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 400 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 102 samples (40 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 400 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated a single JSONL file with 98 samples (40 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (40 token repeat) - 2480 max words - at ../dataset/shuffle-word-2480-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 400 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 400 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 400 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 400 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1720 max words - at ../dataset/shuffle-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 400 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (40 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (40 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 160 samples (40 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated JSONL file with - 2150 max words, 400 samples - at ../dataset/gen-word-2150-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 400 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 400 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 400 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 400 samples - at ../dataset/gen-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (40 token repeat) - 1940 max words - at ../dataset/shuffle-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 73 samples (40 token repeat) - 2420 max words - at ../dataset/shuffle-word-2420-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 400 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 400 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 400 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 400 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 400 samples - at ../dataset/gen-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (40 token repeat) - 2330 max words - at ../dataset/shuffle-word-2330-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 400 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 400 samples - at ../dataset/gen-word-1660-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 400 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 400 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 400 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 400 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 400 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 400 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 400 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 400 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 400 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated JSONL file with - 2260 max words, 400 samples - at ../dataset/gen-word-2260-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 400 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 400 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 400 samples - at ../dataset/gen-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 400 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 400 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 400 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 400 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 400 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 400 samples - at ../dataset/gen-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 400 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 400 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 400 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 400 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 400 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 400 samples - at ../dataset/gen-word-1810-count.jsonl\n",
      "Generated JSONL file with - 2360 max words, 400 samples - at ../dataset/gen-word-2360-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 400 samples - at ../dataset/gen-word-1590-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 400 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 400 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 400 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 400 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 400 samples - at ../dataset/gen-word-1960-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 400 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 400 samples - at ../dataset/gen-word-2300-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 400 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 2230 max words, 400 samples - at ../dataset/gen-word-2230-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 400 samples - at ../dataset/gen-word-1610-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 400 samples - at ../dataset/gen-word-1510-count.jsonl\n",
      "Generated JSONL file with - 2350 max words, 400 samples - at ../dataset/gen-word-2350-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 400 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 400 samples - at ../dataset/gen-word-2400-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 400 samples - at ../dataset/gen-word-1970-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 400 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 400 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 2110 max words, 400 samples - at ../dataset/gen-word-2110-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 400 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 400 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 400 samples - at ../dataset/gen-word-1820-count.jsonl\n",
      "Generated JSONL file with - 2470 max words, 400 samples - at ../dataset/gen-word-2470-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 400 samples - at ../dataset/gen-word-1890-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 400 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 400 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 400 samples - at ../dataset/gen-word-1730-count.jsonl\n",
      "Generated JSONL file with - 2030 max words, 400 samples - at ../dataset/gen-word-2030-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 400 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 400 samples - at ../dataset/gen-word-1520-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 400 samples - at ../dataset/gen-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 400 samples - at ../dataset/gen-word-1830-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 400 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 400 samples - at ../dataset/gen-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 400 samples - at ../dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 400 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 400 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 400 samples - at ../dataset/gen-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 400 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated JSONL file with - 2190 max words, 400 samples - at ../dataset/gen-word-2190-count.jsonl\n",
      "Generated JSONL file with - 2220 max words, 400 samples - at ../dataset/gen-word-2220-count.jsonl\n",
      "Generated JSONL file with - 2080 max words, 400 samples - at ../dataset/gen-word-2080-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 400 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 400 samples - at ../dataset/gen-word-1920-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 400 samples - at ../dataset/gen-word-1670-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 400 samples - at ../dataset/gen-word-1750-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 400 samples - at ../dataset/gen-word-1550-count.jsonl\n",
      "Generated JSONL file with - 2380 max words, 400 samples - at ../dataset/gen-word-2380-count.jsonl\n",
      "Generated JSONL file with - 2270 max words, 400 samples - at ../dataset/gen-word-2270-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 400 samples - at ../dataset/gen-word-1650-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 400 samples - at ../dataset/gen-word-1930-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 400 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 400 samples - at ../dataset/gen-word-1780-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 400 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 400 samples - at ../dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 2430 max words, 400 samples - at ../dataset/gen-word-2430-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 400 samples - at ../dataset/gen-word-1940-count.jsonl\n",
      "Generated JSONL file with - 2290 max words, 400 samples - at ../dataset/gen-word-2290-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 400 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 400 samples - at ../dataset/gen-word-1640-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 400 samples - at ../dataset/gen-word-1710-count.jsonl\n",
      "Generated JSONL file with - 2390 max words, 400 samples - at ../dataset/gen-word-2390-count.jsonl\n",
      "Generated JSONL file with - 2320 max words, 400 samples - at ../dataset/gen-word-2320-count.jsonl\n",
      "Generated JSONL file with - 2130 max words, 400 samples - at ../dataset/gen-word-2130-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 400 samples - at ../dataset/gen-word-1850-count.jsonl\n",
      "Generated JSONL file with - 2120 max words, 400 samples - at ../dataset/gen-word-2120-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 400 samples - at ../dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 400 samples - at ../dataset/gen-word-1760-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 400 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 400 samples - at ../dataset/gen-word-1740-count.jsonl\n",
      "Generated JSONL file with - 2310 max words, 400 samples - at ../dataset/gen-word-2310-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 400 samples - at ../dataset/gen-word-1840-count.jsonl\n",
      "Generated JSONL file with - 2140 max words, 400 samples - at ../dataset/gen-word-2140-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 400 samples - at ../dataset/gen-word-1950-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 400 samples - at ../dataset/gen-word-1910-count.jsonl\n",
      "Generated JSONL file with - 2420 max words, 400 samples - at ../dataset/gen-word-2420-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 400 samples - at ../dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 2240 max words, 400 samples - at ../dataset/gen-word-2240-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 400 samples - at ../dataset/gen-word-1690-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 400 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 400 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 2050 max words, 400 samples - at ../dataset/gen-word-2050-count.jsonl\n",
      "Generated JSONL file with - 2040 max words, 400 samples - at ../dataset/gen-word-2040-count.jsonl\n",
      "Generated JSONL file with - 2070 max words, 400 samples - at ../dataset/gen-word-2070-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 400 samples - at ../dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 2090 max words, 400 samples - at ../dataset/gen-word-2090-count.jsonl\n",
      "Generated JSONL file with - 2060 max words, 400 samples - at ../dataset/gen-word-2060-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 400 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 2480 max words, 400 samples - at ../dataset/gen-word-2480-count.jsonl\n",
      "Generated JSONL file with - 2170 max words, 400 samples - at ../dataset/gen-word-2170-count.jsonl\n",
      "Generated JSONL file with - 2160 max words, 400 samples - at ../dataset/gen-word-2160-count.jsonl\n",
      "Generated JSONL file with - 2180 max words, 400 samples - at ../dataset/gen-word-2180-count.jsonl\n",
      "Generated JSONL file with - 2010 max words, 400 samples - at ../dataset/gen-word-2010-count.jsonl\n",
      "Generated JSONL file with - 2330 max words, 400 samples - at ../dataset/gen-word-2330-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 400 samples - at ../dataset/gen-word-2100-count.jsonl\n",
      "Generated JSONL file with - 2410 max words, 400 samples - at ../dataset/gen-word-2410-count.jsonl\n",
      "Generated JSONL file with - 2440 max words, 400 samples - at ../dataset/gen-word-2440-count.jsonl\n",
      "Generated JSONL file with - 2340 max words, 400 samples - at ../dataset/gen-word-2340-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 400 samples - at ../dataset/gen-word-1880-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 400 samples - at ../dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 2250 max words, 400 samples - at ../dataset/gen-word-2250-count.jsonl\n",
      "Generated JSONL file with - 2460 max words, 400 samples - at ../dataset/gen-word-2460-count.jsonl\n",
      "Generated JSONL file with - 2280 max words, 400 samples - at ../dataset/gen-word-2280-count.jsonl\n",
      "Generated JSONL file with - 2020 max words, 400 samples - at ../dataset/gen-word-2020-count.jsonl\n",
      "Generated JSONL file with - 2490 max words, 400 samples - at ../dataset/gen-word-2490-count.jsonl\n",
      "Generated JSONL file with - 2370 max words, 400 samples - at ../dataset/gen-word-2370-count.jsonl\n",
      "Generated JSONL file with - 2210 max words, 400 samples - at ../dataset/gen-word-2210-count.jsonl\n",
      "Generated JSONL file with - 2450 max words, 400 samples - at ../dataset/gen-word-2450-count.jsonl\n",
      "## Done ##\n",
      "total 1.4G\n",
      "-rw-r--r-- 1 root root  20K Aug 14 23:55 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 107K Aug 14 23:55 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.9M Aug 14 23:55 gen-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.9M Aug 14 23:55 gen-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.9M Aug 14 23:55 gen-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.0M Aug 14 23:55 gen-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.0M Aug 14 23:55 gen-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root 113K Aug 14 23:55 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Aug 14 23:55 gen-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Aug 14 23:55 gen-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Aug 14 23:55 gen-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.2M Aug 14 23:55 gen-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.2M Aug 14 23:55 gen-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root 117K Aug 14 23:55 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.3M Aug 14 23:55 gen-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.3M Aug 14 23:55 gen-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.3M Aug 14 23:55 gen-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.4M Aug 14 23:55 gen-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.4M Aug 14 23:55 gen-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root 127K Aug 14 23:55 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.5M Aug 14 23:55 gen-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.5M Aug 14 23:55 gen-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.5M Aug 14 23:55 gen-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.6M Aug 14 23:55 gen-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.6M Aug 14 23:55 gen-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root 128K Aug 14 23:55 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.6M Aug 14 23:55 gen-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.7M Aug 14 23:55 gen-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.7M Aug 14 23:55 gen-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.8M Aug 14 23:55 gen-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.8M Aug 14 23:55 gen-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root 131K Aug 14 23:55 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.8M Aug 14 23:55 gen-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.9M Aug 14 23:55 gen-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.9M Aug 14 23:55 gen-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.0M Aug 14 23:55 gen-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.0M Aug 14 23:55 gen-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root 133K Aug 14 23:55 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.0M Aug 14 23:55 gen-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Aug 14 23:55 gen-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Aug 14 23:55 gen-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Aug 14 23:55 gen-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Aug 14 23:55 gen-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root 138K Aug 14 23:55 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Aug 14 23:55 gen-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Aug 14 23:55 gen-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.3M Aug 14 23:55 gen-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.3M Aug 14 23:55 gen-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.4M Aug 14 23:55 gen-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root 148K Aug 14 23:55 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.4M Aug 14 23:55 gen-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Aug 14 23:55 gen-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Aug 14 23:55 gen-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Aug 14 23:55 gen-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Aug 14 23:55 gen-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root 150K Aug 14 23:55 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.6M Aug 14 23:55 gen-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.6M Aug 14 23:55 gen-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.7M Aug 14 23:55 gen-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.7M Aug 14 23:55 gen-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Aug 14 23:55 gen-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root  24K Aug 14 23:55 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 155K Aug 14 23:55 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Aug 14 23:55 gen-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Aug 14 23:55 gen-word-1510-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9M Aug 14 23:55 gen-word-1520-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9M Aug 14 23:55 gen-word-1530-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9M Aug 14 23:55 gen-word-1540-count.jsonl\n",
      "-rw-r--r-- 1 root root 158K Aug 14 23:55 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.0M Aug 14 23:55 gen-word-1550-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.0M Aug 14 23:55 gen-word-1560-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Aug 14 23:55 gen-word-1570-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Aug 14 23:55 gen-word-1580-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Aug 14 23:55 gen-word-1590-count.jsonl\n",
      "-rw-r--r-- 1 root root 168K Aug 14 23:55 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.2M Aug 14 23:55 gen-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.2M Aug 14 23:55 gen-word-1610-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.3M Aug 14 23:55 gen-word-1620-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.3M Aug 14 23:55 gen-word-1630-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.3M Aug 14 23:55 gen-word-1640-count.jsonl\n",
      "-rw-r--r-- 1 root root 169K Aug 14 23:55 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.4M Aug 14 23:55 gen-word-1650-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.4M Aug 14 23:55 gen-word-1660-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.4M Aug 14 23:55 gen-word-1670-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.5M Aug 14 23:55 gen-word-1680-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.5M Aug 14 23:55 gen-word-1690-count.jsonl\n",
      "-rw-r--r-- 1 root root 175K Aug 14 23:55 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.5M Aug 14 23:55 gen-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.6M Aug 14 23:55 gen-word-1710-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.6M Aug 14 23:55 gen-word-1720-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.7M Aug 14 23:55 gen-word-1730-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.7M Aug 14 23:55 gen-word-1740-count.jsonl\n",
      "-rw-r--r-- 1 root root 180K Aug 14 23:55 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.7M Aug 14 23:55 gen-word-1750-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.8M Aug 14 23:55 gen-word-1760-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.8M Aug 14 23:55 gen-word-1770-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.8M Aug 14 23:55 gen-word-1780-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.9M Aug 14 23:55 gen-word-1790-count.jsonl\n",
      "-rw-r--r-- 1 root root 185K Aug 14 23:55 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.9M Aug 14 23:55 gen-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Aug 14 23:55 gen-word-1810-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Aug 14 23:55 gen-word-1820-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Aug 14 23:55 gen-word-1830-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.1M Aug 14 23:55 gen-word-1840-count.jsonl\n",
      "-rw-r--r-- 1 root root 189K Aug 14 23:55 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.1M Aug 14 23:55 gen-word-1850-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.2M Aug 14 23:55 gen-word-1860-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.2M Aug 14 23:55 gen-word-1870-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.2M Aug 14 23:55 gen-word-1880-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.2M Aug 14 23:55 gen-word-1890-count.jsonl\n",
      "-rw-r--r-- 1 root root 196K Aug 14 23:55 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.3M Aug 14 23:55 gen-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Aug 14 23:55 gen-word-1910-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Aug 14 23:55 gen-word-1920-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Aug 14 23:55 gen-word-1930-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.4M Aug 14 23:55 gen-word-1940-count.jsonl\n",
      "-rw-r--r-- 1 root root 199K Aug 14 23:55 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.5M Aug 14 23:55 gen-word-1950-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.5M Aug 14 23:55 gen-word-1960-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.6M Aug 14 23:55 gen-word-1970-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.6M Aug 14 23:55 gen-word-1980-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Aug 14 23:55 gen-word-1990-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 203K Aug 14 23:55 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Aug 14 23:55 gen-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Aug 14 23:55 gen-word-2010-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.8M Aug 14 23:55 gen-word-2020-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.8M Aug 14 23:55 gen-word-2030-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.8M Aug 14 23:55 gen-word-2040-count.jsonl\n",
      "-rw-r--r-- 1 root root 209K Aug 14 23:55 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.9M Aug 14 23:55 gen-word-2050-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.9M Aug 14 23:55 gen-word-2060-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Aug 14 23:55 gen-word-2070-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Aug 14 23:55 gen-word-2080-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Aug 14 23:55 gen-word-2090-count.jsonl\n",
      "-rw-r--r-- 1 root root 214K Aug 14 23:55 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Aug 14 23:55 gen-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.1M Aug 14 23:55 gen-word-2110-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.1M Aug 14 23:55 gen-word-2120-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.2M Aug 14 23:55 gen-word-2130-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.2M Aug 14 23:55 gen-word-2140-count.jsonl\n",
      "-rw-r--r-- 1 root root 220K Aug 14 23:55 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.3M Aug 14 23:55 gen-word-2150-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.3M Aug 14 23:55 gen-word-2160-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.3M Aug 14 23:55 gen-word-2170-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.4M Aug 14 23:55 gen-word-2180-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.4M Aug 14 23:55 gen-word-2190-count.jsonl\n",
      "-rw-r--r-- 1 root root 225K Aug 14 23:55 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Aug 14 23:55 gen-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Aug 14 23:55 gen-word-2210-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Aug 14 23:55 gen-word-2220-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Aug 14 23:55 gen-word-2230-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.6M Aug 14 23:55 gen-word-2240-count.jsonl\n",
      "-rw-r--r-- 1 root root 221K Aug 14 23:55 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.6M Aug 14 23:55 gen-word-2250-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.7M Aug 14 23:55 gen-word-2260-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.7M Aug 14 23:55 gen-word-2270-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.8M Aug 14 23:55 gen-word-2280-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.8M Aug 14 23:55 gen-word-2290-count.jsonl\n",
      "-rw-r--r-- 1 root root 232K Aug 14 23:55 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.8M Aug 14 23:55 gen-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.9M Aug 14 23:55 gen-word-2310-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.9M Aug 14 23:55 gen-word-2320-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.9M Aug 14 23:55 gen-word-2330-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.0M Aug 14 23:55 gen-word-2340-count.jsonl\n",
      "-rw-r--r-- 1 root root 238K Aug 14 23:55 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.0M Aug 14 23:55 gen-word-2350-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.1M Aug 14 23:55 gen-word-2360-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.1M Aug 14 23:55 gen-word-2370-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.1M Aug 14 23:55 gen-word-2380-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.2M Aug 14 23:55 gen-word-2390-count.jsonl\n",
      "-rw-r--r-- 1 root root 250K Aug 14 23:55 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.2M Aug 14 23:55 gen-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.3M Aug 14 23:55 gen-word-2410-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.3M Aug 14 23:55 gen-word-2420-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.3M Aug 14 23:55 gen-word-2430-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.3M Aug 14 23:55 gen-word-2440-count.jsonl\n",
      "-rw-r--r-- 1 root root 249K Aug 14 23:55 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.4M Aug 14 23:55 gen-word-2450-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.4M Aug 14 23:55 gen-word-2460-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.5M Aug 14 23:55 gen-word-2470-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.5M Aug 14 23:55 gen-word-2480-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.5M Aug 14 23:55 gen-word-2490-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 14 23:55 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 251K Aug 14 23:55 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 9.6M Aug 14 23:55 gen-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root 255K Aug 14 23:55 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 263K Aug 14 23:55 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 272K Aug 14 23:55 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 278K Aug 14 23:55 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 283K Aug 14 23:55 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 280K Aug 14 23:55 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 287K Aug 14 23:55 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 295K Aug 14 23:55 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 297K Aug 14 23:55 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  40K Aug 14 23:55 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 300K Aug 14 23:55 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 305K Aug 14 23:55 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 315K Aug 14 23:55 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 318K Aug 14 23:55 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 324K Aug 14 23:55 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 334K Aug 14 23:55 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 329K Aug 14 23:55 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 336K Aug 14 23:55 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 342K Aug 14 23:55 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 343K Aug 14 23:55 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  43K Aug 14 23:55 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 353K Aug 14 23:55 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 353K Aug 14 23:55 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 359K Aug 14 23:55 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 363K Aug 14 23:55 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 373K Aug 14 23:55 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 374K Aug 14 23:55 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 382K Aug 14 23:55 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 385K Aug 14 23:55 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 392K Aug 14 23:55 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 397K Aug 14 23:55 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  49K Aug 14 23:55 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 397K Aug 14 23:55 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 402K Aug 14 23:55 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 409K Aug 14 23:55 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 412K Aug 14 23:55 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 421K Aug 14 23:55 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 426K Aug 14 23:55 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 428K Aug 14 23:55 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 433K Aug 14 23:55 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 440K Aug 14 23:55 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 439K Aug 14 23:55 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  54K Aug 14 23:55 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 450K Aug 14 23:55 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 454K Aug 14 23:55 gen-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root 459K Aug 14 23:55 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root 456K Aug 14 23:55 gen-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root 471K Aug 14 23:55 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root 470K Aug 14 23:55 gen-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root 483K Aug 14 23:55 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root 479K Aug 14 23:55 gen-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root 485K Aug 14 23:55 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root 494K Aug 14 23:55 gen-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  15K Aug 14 23:55 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  59K Aug 14 23:55 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 499K Aug 14 23:55 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.0M Aug 14 23:55 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.1M Aug 14 23:55 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.1M Aug 14 23:55 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.1M Aug 14 23:55 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  62K Aug 14 23:55 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.2M Aug 14 23:55 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.2M Aug 14 23:55 gen-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Aug 14 23:55 gen-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Aug 14 23:55 gen-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Aug 14 23:55 gen-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  69K Aug 14 23:55 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.4M Aug 14 23:55 gen-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.4M Aug 14 23:55 gen-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.5M Aug 14 23:55 gen-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.5M Aug 14 23:55 gen-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.5M Aug 14 23:55 gen-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root  72K Aug 14 23:55 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.6M Aug 14 23:55 gen-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.6M Aug 14 23:55 gen-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.6M Aug 14 23:55 gen-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.7M Aug 14 23:55 gen-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.7M Aug 14 23:55 gen-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root  78K Aug 14 23:55 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Aug 14 23:55 gen-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Aug 14 23:55 gen-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Aug 14 23:55 gen-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.9M Aug 14 23:55 gen-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.9M Aug 14 23:55 gen-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root  83K Aug 14 23:55 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.9M Aug 14 23:55 gen-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.0M Aug 14 23:55 gen-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.0M Aug 14 23:55 gen-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.0M Aug 14 23:55 gen-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.1M Aug 14 23:55 gen-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root  88K Aug 14 23:55 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.1M Aug 14 23:55 gen-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.1M Aug 14 23:55 gen-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.2M Aug 14 23:55 gen-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.2M Aug 14 23:55 gen-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.3M Aug 14 23:55 gen-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  93K Aug 14 23:55 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.3M Aug 14 23:55 gen-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.3M Aug 14 23:55 gen-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.4M Aug 14 23:55 gen-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.4M Aug 14 23:55 gen-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.5M Aug 14 23:55 gen-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  95K Aug 14 23:55 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.5M Aug 14 23:55 gen-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.6M Aug 14 23:55 gen-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.6M Aug 14 23:55 gen-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.6M Aug 14 23:55 gen-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.7M Aug 14 23:55 gen-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root 103K Aug 14 23:55 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.7M Aug 14 23:55 gen-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.7M Aug 14 23:55 gen-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Aug 14 23:55 gen-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Aug 14 23:55 gen-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Aug 14 23:55 gen-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  53K Aug 14 23:55 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:55 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root  43K Aug 14 23:55 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r-- 1 root root  39K Aug 14 23:55 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2010-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2020-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2030-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2040-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2050-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2060-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2070-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2080-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2090-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2110-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2120-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2130-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2140-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2150-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2160-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2170-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2180-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2190-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2210-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2220-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2230-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2240-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2250-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2260-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2270-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2280-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2290-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2310-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2320-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2330-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2340-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2350-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2360-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2370-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2380-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2390-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2410-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2420-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2430-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2440-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2450-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2460-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2470-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2480-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2490-count.jsonl\n",
      "-rw-r--r-- 1 root root  34K Aug 14 23:55 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 14 23:55 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 14 23:55 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  33K Aug 14 23:55 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:55 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:55 shuffle-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  85K Aug 14 23:55 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  31K Aug 14 23:55 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:55 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 14 23:55 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root  31K Aug 14 23:55 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:55 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:55 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:55 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.1M Aug 14 23:55 shuffle-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  12K Aug 14 23:55 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 100 &\n",
    "for i in {5..500..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 2100 words dataset\n",
    "# \n",
    "for i in {510..2500..10} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 400 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 40 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -lh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-4k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=4096', '--model.ctx_len=4096', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-1k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-4k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=4096', '--model.ctx_len=4096', '--model.bptt_learning_range=1', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-1k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4130405425\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4130405425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230814_112950-gskamgxq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/gskamgxq\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1001/1001 [00:00<00:00, 38782.70it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b06d927baeb094f0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 107.57it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.58it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 4130405425\n",
      "[rank: 6] Global seed set to 4130405425\n",
      "[rank: 7] Global seed set to 4130405425\n",
      "[rank: 5] Global seed set to 4130405425\n",
      "[rank: 3] Global seed set to 4130405425\n",
      "[rank: 4] Global seed set to 4130405425\n",
      "[rank: 1] Global seed set to 4130405425\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b06d927baeb094f0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Map (num_proc=64):   6%|â–       | 20356/363689 [00:06<01:07, 5118.53 examples/s][rank: 2] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-14 11:30:26,323] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   9%|â–‹       | 32431/363689 [00:09<00:56, 5863.66 examples/s][rank: 1] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-14 11:30:28,892] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  10%|â–Š       | 35288/363689 [00:09<00:47, 6883.35 examples/s][rank: 6] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-14 11:30:29,281] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  10%|â–Š       | 36903/363689 [00:10<00:43, 7471.93 examples/s][rank: 4] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-14 11:30:29,422] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  11%|â–Š       | 39348/363689 [00:10<00:41, 7880.53 examples/s][rank: 5] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-14 11:30:29,791] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  11%|â–‰       | 40184/363689 [00:10<00:40, 8021.81 examples/s][rank: 7] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-14 11:30:29,826] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 4130405425\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-14 11:30:29,836] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 4130405425                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-14 11:32:09,805] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07107162475585938 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10141611099243164 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10148286819458008 seconds\n",
      "Time to load fused_adam op: 0.10147452354431152 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10180330276489258 seconds\n",
      "Time to load fused_adam op: 0.10170149803161621 seconds\n",
      "Time to load fused_adam op: 0.10192751884460449 seconds\n",
      "Time to load fused_adam op: 0.10192728042602539 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06857132911682129 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10177135467529297 seconds\n",
      "Time to load utils op: 0.10189485549926758 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10111331939697266 seconds\n",
      "Time to load utils op: 0.10171818733215332 seconds\n",
      "Time to load utils op: 0.10167479515075684 seconds\n",
      "Time to load utils op: 0.10128259658813477 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1011054515838623 seconds\n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002834796905517578 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028014183044433594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002582073211669922 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025725364685058594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002551078796386719 seconds\n",
      "Time to load utils op: 0.0002593994140625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003306865692138672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000530242919921875 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   2%| | 800/45295 [15:12<14:05:44,  1.14s/it, v_num=mgxq, train/loss=6./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:  27%|â–Ž| 12016/45295 [3:52:38<10:44:17,  1.16s/it, v_num=mgxq, train/los^C\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-4k (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-4k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=2e-4 \\\n",
    "        --data.max_token_size=4096 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-ctx-4k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-ctx-4k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 14 15:25 ../model/EWR-1B5-E0_1-mem-ctx-4k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-4k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 98.88888888888889% similarity, with 89 matched token, and 1 token mismatch\n",
      "## Model validation for 95 tokens : 98.94736842105263% similarity, with 94 matched token, and 1 token mismatch\n",
      "## Model validation for 100 tokens : 98.0% similarity, with 98 matched token, and 2 token mismatch\n",
      "## Model validation for 105 tokens : 97.14285714285714% similarity, with 102 matched token, and 3 token mismatch\n",
      "## Model validation for 110 tokens : 96.36363636363636% similarity, with 106 matched token, and 4 token mismatch\n",
      "## Model validation for 115 tokens : 97.3913043478261% similarity, with 112 matched token, and 3 token mismatch\n",
      "## Model validation for 120 tokens : 97.5% similarity, with 117 matched token, and 3 token mismatch\n",
      "## Model validation for 125 tokens : 97.6% similarity, with 122 matched token, and 3 token mismatch\n",
      "## Model validation for 130 tokens : 96.92307692307692% similarity, with 126 matched token, and 4 token mismatch\n",
      "## Model validation for 135 tokens : 97.77777777777777% similarity, with 132 matched token, and 3 token mismatch\n",
      "## Model validation for 140 tokens : 97.14285714285714% similarity, with 136 matched token, and 4 token mismatch\n",
      "## Model validation for 145 tokens : 97.24137931034483% similarity, with 141 matched token, and 4 token mismatch\n",
      "## Model validation for 150 tokens : 98.0% similarity, with 147 matched token, and 3 token mismatch\n",
      "## Model validation for 160 tokens : 96.875% similarity, with 155 matched token, and 5 token mismatch\n",
      "## Model validation for 170 tokens : 98.23529411764706% similarity, with 167 matched token, and 3 token mismatch\n",
      "## Model validation for 180 tokens : 96.66666666666667% similarity, with 174 matched token, and 6 token mismatch\n",
      "## Model validation for 190 tokens : 96.84210526315789% similarity, with 184 matched token, and 6 token mismatch\n",
      "## Model validation for 200 tokens : 95.5% similarity, with 191 matched token, and 9 token mismatch\n",
      "## Model validation for 210 tokens : 95.23809523809523% similarity, with 200 matched token, and 10 token mismatch\n",
      "## Model validation for 220 tokens : 95.0% similarity, with 209 matched token, and 11 token mismatch\n",
      "## Model validation for 230 tokens : 94.78260869565217% similarity, with 218 matched token, and 12 token mismatch\n",
      "## Model validation for 240 tokens : 95.83333333333334% similarity, with 230 matched token, and 10 token mismatch\n",
      "## Model validation for 250 tokens : 96.39999999999999% similarity, with 241 matched token, and 9 token mismatch\n",
      "## Model validation for 260 tokens : 96.92307692307692% similarity, with 252 matched token, and 8 token mismatch\n",
      "## Model validation for 270 tokens : 96.29629629629629% similarity, with 260 matched token, and 10 token mismatch\n",
      "## Model validation for 280 tokens : 96.78571428571429% similarity, with 271 matched token, and 9 token mismatch\n",
      "## Model validation for 290 tokens : 96.55172413793103% similarity, with 280 matched token, and 10 token mismatch\n",
      "## Model validation for 300 tokens : 96.33333333333334% similarity, with 289 matched token, and 11 token mismatch\n",
      "## Model validation for 325 tokens : 97.23076923076923% similarity, with 316 matched token, and 9 token mismatch\n",
      "## Model validation for 350 tokens : 95.42857142857143% similarity, with 334 matched token, and 16 token mismatch\n",
      "## Model validation for 375 tokens : 94.66666666666667% similarity, with 355 matched token, and 20 token mismatch\n",
      "## Model validation for 400 tokens : 95.25% similarity, with 381 matched token, and 19 token mismatch\n",
      "## Model validation for 425 tokens : 94.58823529411765% similarity, with 402 matched token, and 23 token mismatch\n",
      "## Model validation for 450 tokens : 93.77777777777779% similarity, with 422 matched token, and 28 token mismatch\n",
      "## Model validation for 475 tokens : 94.3157894736842% similarity, with 448 matched token, and 27 token mismatch\n",
      "## Model validation for 500 tokens : 93.4% similarity, with 467 matched token, and 33 token mismatch\n",
      "## Model validation for 525 tokens : 93.33333333333333% similarity, with 490 matched token, and 35 token mismatch\n",
      "## Model validation for 550 tokens : 93.81818181818183% similarity, with 516 matched token, and 34 token mismatch\n",
      "## Model validation for 575 tokens : 93.91304347826087% similarity, with 540 matched token, and 35 token mismatch\n",
      "## Model validation for 600 tokens : 94.0% similarity, with 564 matched token, and 36 token mismatch\n",
      "## Model validation for 625 tokens : 93.60000000000001% similarity, with 585 matched token, and 40 token mismatch\n",
      "## Model validation for 650 tokens : 94.15384615384616% similarity, with 612 matched token, and 38 token mismatch\n",
      "## Model validation for 675 tokens : 92.88888888888889% similarity, with 627 matched token, and 48 token mismatch\n",
      "## Model validation for 700 tokens : 92.71428571428572% similarity, with 649 matched token, and 51 token mismatch\n",
      "## Model validation for 750 tokens : 91.60000000000001% similarity, with 687 matched token, and 63 token mismatch\n",
      "## Model validation for 800 tokens : 91.25% similarity, with 730 matched token, and 70 token mismatch\n",
      "## Model validation for 850 tokens : 90.58823529411765% similarity, with 770 matched token, and 80 token mismatch\n",
      "## Model validation for 900 tokens : 89.66666666666666% similarity, with 807 matched token, and 93 token mismatch\n",
      "## Model validation for 950 tokens : 88.63157894736841% similarity, with 842 matched token, and 108 token mismatch\n",
      "## Model validation for 1000 tokens : 88.4% similarity, with 884 matched token, and 116 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 6 : Ramping up the ctx size (8192), memory training\n",
    "\n",
    "- Tune 6: Large ctx size (8192), Scaling up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 15 max words, 50 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 50 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 50 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 50 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 50 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 50 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 50 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 50 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 262 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 50 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 85 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 50 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 129 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 50 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 50 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 109 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 50 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 50 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 179 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 548 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 50 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 50 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 50 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 50 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 50 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 50 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 50 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 50 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (1 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 28 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 50 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 50 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 50 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 50 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 50 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 50 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 50 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 50 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 50 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 50 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 50 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 50 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 50 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 50 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 50 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 50 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 50 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 50 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 50 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 50 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 50 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 50 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 50 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 50 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 50 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 50 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 50 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 50 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 50 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 50 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 50 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 50 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 50 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 50 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 50 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 50 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 50 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 50 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 50 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 50 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 50 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 50 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 50 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 50 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 50 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 50 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 50 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 50 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 50 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 50 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 50 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 50 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 50 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 50 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 50 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 50 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 50 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 50 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 50 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 50 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 50 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 50 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 50 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 50 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 50 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 50 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 50 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 50 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 50 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 50 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 50 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 50 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 50 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 50 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 50 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 50 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 50 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 50 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 50 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 50 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 50 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 50 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 50 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 50 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 50 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 50 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 50 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 50 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 50 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 50 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 50 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 50 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 50 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 50 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 50 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 50 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 50 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 50 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 50 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 50 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 50 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 50 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 50 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 50 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 50 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 50 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 50 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 50 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 50 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 50 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 50 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 50 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 50 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 50 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 50 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 50 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 50 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 50 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 50 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 50 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 50 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 50 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 50 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 50 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 50 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 50 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 50 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 50 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 50 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 50 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 50 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4700 max words - at ../dataset/shuffle-word-4700-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 50 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 50 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4100 max words - at ../dataset/shuffle-word-4100-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 50 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 50 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2900 max words - at ../dataset/shuffle-word-2900-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 50 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 50 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 50 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 50 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 50 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 50 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 50 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2800 max words - at ../dataset/shuffle-word-2800-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 50 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 50 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 50 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 50 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 50 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 50 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 50 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 50 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 50 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 50 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4900 max words - at ../dataset/shuffle-word-4900-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 50 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 50 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 50 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 50 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 50 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4300 max words - at ../dataset/shuffle-word-4300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5400 max words - at ../dataset/shuffle-word-5400-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 50 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 50 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 50 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2200 max words - at ../dataset/shuffle-word-2200-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 50 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 50 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3300 max words - at ../dataset/shuffle-word-3300-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5200 max words - at ../dataset/shuffle-word-5200-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 50 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7900 max words - at ../dataset/shuffle-word-7900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7600 max words - at ../dataset/shuffle-word-7600-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 50 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 50 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6700 max words - at ../dataset/shuffle-word-6700-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7400 max words - at ../dataset/shuffle-word-7400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3700 max words - at ../dataset/shuffle-word-3700-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6400 max words - at ../dataset/shuffle-word-6400-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7100 max words - at ../dataset/shuffle-word-7100-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 50 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3100 max words - at ../dataset/shuffle-word-3100-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 50 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 50 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5900 max words - at ../dataset/shuffle-word-5900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 8000 max words - at ../dataset/shuffle-word-8000-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 50 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 50 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5300 max words - at ../dataset/shuffle-word-5300-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3200 max words - at ../dataset/shuffle-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (20 token repeat) - 2600 max words - at ../dataset/shuffle-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6000 max words - at ../dataset/shuffle-word-6000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4500 max words - at ../dataset/shuffle-word-4500-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2700 max words - at ../dataset/shuffle-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3500 max words - at ../dataset/shuffle-word-3500-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7500 max words - at ../dataset/shuffle-word-7500-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 50 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7800 max words - at ../dataset/shuffle-word-7800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3600 max words - at ../dataset/shuffle-word-3600-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 50 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 50 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4400 max words - at ../dataset/shuffle-word-4400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3000 max words - at ../dataset/shuffle-word-3000-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 50 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2100 max words - at ../dataset/shuffle-word-2100-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4800 max words - at ../dataset/shuffle-word-4800-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2300 max words - at ../dataset/shuffle-word-2300-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 50 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5800 max words - at ../dataset/shuffle-word-5800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4000 max words - at ../dataset/shuffle-word-4000-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2500 max words - at ../dataset/shuffle-word-2500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6100 max words - at ../dataset/shuffle-word-6100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7200 max words - at ../dataset/shuffle-word-7200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4200 max words - at ../dataset/shuffle-word-4200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5700 max words - at ../dataset/shuffle-word-5700-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5100 max words - at ../dataset/shuffle-word-5100-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 50 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5000 max words - at ../dataset/shuffle-word-5000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6200 max words - at ../dataset/shuffle-word-6200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6900 max words - at ../dataset/shuffle-word-6900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7300 max words - at ../dataset/shuffle-word-7300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7000 max words - at ../dataset/shuffle-word-7000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6500 max words - at ../dataset/shuffle-word-6500-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 50 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7700 max words - at ../dataset/shuffle-word-7700-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4600 max words - at ../dataset/shuffle-word-4600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3900 max words - at ../dataset/shuffle-word-3900-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 50 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3400 max words - at ../dataset/shuffle-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6300 max words - at ../dataset/shuffle-word-6300-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2400 max words - at ../dataset/shuffle-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3800 max words - at ../dataset/shuffle-word-3800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6800 max words - at ../dataset/shuffle-word-6800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5500 max words - at ../dataset/shuffle-word-5500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5600 max words - at ../dataset/shuffle-word-5600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6600 max words - at ../dataset/shuffle-word-6600-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 50 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 2000 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 2000 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 2000 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 2000 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 2000 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 2000 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 2000 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 2000 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 2000 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 2000 samples - at ../dataset/gen-word-2300-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 2000 samples - at ../dataset/gen-word-2400-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 2000 samples - at ../dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 2000 samples - at ../dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 2600 max words, 2000 samples - at ../dataset/gen-word-2600-count.jsonl\n",
      "Generated JSONL file with - 2700 max words, 2000 samples - at ../dataset/gen-word-2700-count.jsonl\n",
      "Generated JSONL file with - 3000 max words, 2000 samples - at ../dataset/gen-word-3000-count.jsonl\n",
      "Generated JSONL file with - 2800 max words, 2000 samples - at ../dataset/gen-word-2800-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 2000 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 2900 max words, 2000 samples - at ../dataset/gen-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 2000 samples - at ../dataset/gen-word-2100-count.jsonl\n",
      "Generated JSONL file with - 3100 max words, 2000 samples - at ../dataset/gen-word-3100-count.jsonl\n",
      "Generated JSONL file with - 3500 max words, 2000 samples - at ../dataset/gen-word-3500-count.jsonl\n",
      "Generated JSONL file with - 3300 max words, 2000 samples - at ../dataset/gen-word-3300-count.jsonl\n",
      "Generated JSONL file with - 3400 max words, 2000 samples - at ../dataset/gen-word-3400-count.jsonl\n",
      "Generated JSONL file with - 3200 max words, 2000 samples - at ../dataset/gen-word-3200-count.jsonl\n",
      "Generated JSONL file with - 3700 max words, 2000 samples - at ../dataset/gen-word-3700-count.jsonl\n",
      "Generated JSONL file with - 3600 max words, 2000 samples - at ../dataset/gen-word-3600-count.jsonl\n",
      "Generated JSONL file with - 3900 max words, 2000 samples - at ../dataset/gen-word-3900-count.jsonl\n",
      "Generated JSONL file with - 4100 max words, 2000 samples - at ../dataset/gen-word-4100-count.jsonl\n",
      "Generated JSONL file with - 3800 max words, 2000 samples - at ../dataset/gen-word-3800-count.jsonl\n",
      "Generated JSONL file with - 4200 max words, 2000 samples - at ../dataset/gen-word-4200-count.jsonl\n",
      "Generated JSONL file with - 4400 max words, 2000 samples - at ../dataset/gen-word-4400-count.jsonl\n",
      "Generated JSONL file with - 4600 max words, 2000 samples - at ../dataset/gen-word-4600-count.jsonl\n",
      "Generated JSONL file with - 4300 max words, 2000 samples - at ../dataset/gen-word-4300-count.jsonl\n",
      "Generated JSONL file with - 4000 max words, 2000 samples - at ../dataset/gen-word-4000-count.jsonl\n",
      "Generated JSONL file with - 4500 max words, 2000 samples - at ../dataset/gen-word-4500-count.jsonl\n",
      "Generated JSONL file with - 5100 max words, 2000 samples - at ../dataset/gen-word-5100-count.jsonl\n",
      "Generated JSONL file with - 4900 max words, 2000 samples - at ../dataset/gen-word-4900-count.jsonl\n",
      "Generated JSONL file with - 5000 max words, 2000 samples - at ../dataset/gen-word-5000-count.jsonl\n",
      "Generated JSONL file with - 4700 max words, 2000 samples - at ../dataset/gen-word-4700-count.jsonl\n",
      "Generated JSONL file with - 4800 max words, 2000 samples - at ../dataset/gen-word-4800-count.jsonl\n",
      "Generated JSONL file with - 5200 max words, 2000 samples - at ../dataset/gen-word-5200-count.jsonl\n",
      "Generated JSONL file with - 5600 max words, 2000 samples - at ../dataset/gen-word-5600-count.jsonl\n",
      "Generated JSONL file with - 5300 max words, 2000 samples - at ../dataset/gen-word-5300-count.jsonl\n",
      "Generated JSONL file with - 5700 max words, 2000 samples - at ../dataset/gen-word-5700-count.jsonl\n",
      "Generated JSONL file with - 5900 max words, 2000 samples - at ../dataset/gen-word-5900-count.jsonl\n",
      "Generated JSONL file with - 6100 max words, 2000 samples - at ../dataset/gen-word-6100-count.jsonl\n",
      "Generated JSONL file with - 5800 max words, 2000 samples - at ../dataset/gen-word-5800-count.jsonl\n",
      "Generated JSONL file with - 6300 max words, 2000 samples - at ../dataset/gen-word-6300-count.jsonl\n",
      "Generated JSONL file with - 5400 max words, 2000 samples - at ../dataset/gen-word-5400-count.jsonl\n",
      "Generated JSONL file with - 6000 max words, 2000 samples - at ../dataset/gen-word-6000-count.jsonl\n",
      "Generated JSONL file with - 6400 max words, 2000 samples - at ../dataset/gen-word-6400-count.jsonl\n",
      "Generated JSONL file with - 5500 max words, 2000 samples - at ../dataset/gen-word-5500-count.jsonl\n",
      "Generated JSONL file with - 6600 max words, 2000 samples - at ../dataset/gen-word-6600-count.jsonl\n",
      "Generated JSONL file with - 6500 max words, 2000 samples - at ../dataset/gen-word-6500-count.jsonl\n",
      "Generated JSONL file with - 6800 max words, 2000 samples - at ../dataset/gen-word-6800-count.jsonl\n",
      "Generated JSONL file with - 6700 max words, 2000 samples - at ../dataset/gen-word-6700-count.jsonl\n",
      "Generated JSONL file with - 6200 max words, 2000 samples - at ../dataset/gen-word-6200-count.jsonl\n",
      "Generated JSONL file with - 6900 max words, 2000 samples - at ../dataset/gen-word-6900-count.jsonl\n",
      "Generated JSONL file with - 7300 max words, 2000 samples - at ../dataset/gen-word-7300-count.jsonl\n",
      "Generated JSONL file with - 7500 max words, 2000 samples - at ../dataset/gen-word-7500-count.jsonl\n",
      "Generated JSONL file with - 7100 max words, 2000 samples - at ../dataset/gen-word-7100-count.jsonl\n",
      "Generated JSONL file with - 7600 max words, 2000 samples - at ../dataset/gen-word-7600-count.jsonl\n",
      "Generated JSONL file with - 7000 max words, 2000 samples - at ../dataset/gen-word-7000-count.jsonl\n",
      "Generated JSONL file with - 7200 max words, 2000 samples - at ../dataset/gen-word-7200-count.jsonl\n",
      "Generated JSONL file with - 7900 max words, 2000 samples - at ../dataset/gen-word-7900-count.jsonl\n",
      "Generated JSONL file with - 7800 max words, 2000 samples - at ../dataset/gen-word-7800-count.jsonl\n",
      "Generated JSONL file with - 7400 max words, 2000 samples - at ../dataset/gen-word-7400-count.jsonl\n",
      "Generated JSONL file with - 8000 max words, 2000 samples - at ../dataset/gen-word-8000-count.jsonl\n",
      "Generated JSONL file with - 7700 max words, 2000 samples - at ../dataset/gen-word-7700-count.jsonl\n",
      "## Done ##\n",
      "total 6.1G\n",
      "-rw-r--r-- 1 root root 9.9K Aug 14 23:56 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  55K Aug 14 23:56 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 491K Aug 14 23:56 gen-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  57K Aug 14 23:56 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  61K Aug 14 23:56 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  22M Aug 14 23:56 gen-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  62K Aug 14 23:56 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  63K Aug 14 23:56 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  23M Aug 14 23:56 gen-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  68K Aug 14 23:56 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  70K Aug 14 23:56 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  25M Aug 14 23:56 gen-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  73K Aug 14 23:56 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  73K Aug 14 23:56 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  27M Aug 14 23:56 gen-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  79K Aug 14 23:56 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  12K Aug 14 23:56 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  77K Aug 14 23:56 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  29M Aug 14 23:56 gen-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  80K Aug 14 23:56 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  81K Aug 14 23:56 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  31M Aug 14 23:56 gen-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  85K Aug 14 23:56 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  87K Aug 14 23:56 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  33M Aug 14 23:56 gen-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  89K Aug 14 23:56 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  94K Aug 14 23:56 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  35M Aug 14 23:56 gen-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  95K Aug 14 23:56 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  96K Aug 14 23:56 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  37M Aug 14 23:56 gen-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  97K Aug 14 23:56 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  16K Aug 14 23:56 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 106K Aug 14 23:56 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  39M Aug 14 23:56 gen-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root 104K Aug 14 23:56 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 107K Aug 14 23:56 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  41M Aug 14 23:56 gen-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root 111K Aug 14 23:56 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 113K Aug 14 23:56 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  42M Aug 14 23:56 gen-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root 116K Aug 14 23:56 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 118K Aug 14 23:56 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  44M Aug 14 23:56 gen-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root 120K Aug 14 23:56 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 122K Aug 14 23:56 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  46M Aug 14 23:56 gen-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root 123K Aug 14 23:56 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  17K Aug 14 23:56 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 125K Aug 14 23:56 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  48M Aug 14 23:56 gen-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root 128K Aug 14 23:56 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 131K Aug 14 23:56 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  50M Aug 14 23:56 gen-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root 138K Aug 14 23:56 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 137K Aug 14 23:56 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  52M Aug 14 23:56 gen-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root 137K Aug 14 23:56 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 141K Aug 14 23:56 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  54M Aug 14 23:56 gen-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root 139K Aug 14 23:56 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 149K Aug 14 23:56 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  56M Aug 14 23:56 gen-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root 146K Aug 14 23:56 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  20K Aug 14 23:56 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 152K Aug 14 23:56 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  58M Aug 14 23:56 gen-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root 155K Aug 14 23:56 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 156K Aug 14 23:56 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  59M Aug 14 23:56 gen-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root 161K Aug 14 23:56 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 165K Aug 14 23:56 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  61M Aug 14 23:56 gen-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root 163K Aug 14 23:56 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 168K Aug 14 23:56 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  63M Aug 14 23:56 gen-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root 166K Aug 14 23:56 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 172K Aug 14 23:56 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  65M Aug 14 23:56 gen-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root 175K Aug 14 23:56 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  22K Aug 14 23:56 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 178K Aug 14 23:56 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  67M Aug 14 23:56 gen-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root 177K Aug 14 23:56 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 181K Aug 14 23:56 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  69M Aug 14 23:56 gen-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root 185K Aug 14 23:56 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 183K Aug 14 23:56 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  71M Aug 14 23:56 gen-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root 189K Aug 14 23:56 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 190K Aug 14 23:56 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  73M Aug 14 23:56 gen-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root 194K Aug 14 23:56 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 195K Aug 14 23:56 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  75M Aug 14 23:56 gen-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root 194K Aug 14 23:56 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  25K Aug 14 23:56 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 202K Aug 14 23:56 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  77M Aug 14 23:56 gen-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root 203K Aug 14 23:56 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 203K Aug 14 23:56 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  79M Aug 14 23:56 gen-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root 204K Aug 14 23:56 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 214K Aug 14 23:56 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  81M Aug 14 23:56 gen-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root 213K Aug 14 23:56 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 218K Aug 14 23:56 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  82M Aug 14 23:56 gen-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root 219K Aug 14 23:56 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 223K Aug 14 23:56 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  84M Aug 14 23:56 gen-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root 222K Aug 14 23:56 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 229K Aug 14 23:56 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  86M Aug 14 23:56 gen-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root 226K Aug 14 23:56 gen-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root 231K Aug 14 23:56 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  88M Aug 14 23:56 gen-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root 230K Aug 14 23:56 gen-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root 230K Aug 14 23:56 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  90M Aug 14 23:56 gen-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root 237K Aug 14 23:56 gen-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root 238K Aug 14 23:56 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  92M Aug 14 23:56 gen-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root 241K Aug 14 23:56 gen-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root 246K Aug 14 23:56 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  94M Aug 14 23:56 gen-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root 245K Aug 14 23:56 gen-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7K Aug 14 23:56 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 246K Aug 14 23:56 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  96M Aug 14 23:56 gen-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root 255K Aug 14 23:56 gen-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root 253K Aug 14 23:56 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  98M Aug 14 23:56 gen-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root 258K Aug 14 23:56 gen-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root 257K Aug 14 23:56 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  99M Aug 14 23:56 gen-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root 262K Aug 14 23:56 gen-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root 263K Aug 14 23:56 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 101M Aug 14 23:56 gen-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root 264K Aug 14 23:56 gen-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root 270K Aug 14 23:56 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root 103M Aug 14 23:56 gen-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root 271K Aug 14 23:56 gen-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  31K Aug 14 23:56 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 280K Aug 14 23:56 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 105M Aug 14 23:56 gen-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root 273K Aug 14 23:56 gen-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root 290K Aug 14 23:56 gen-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 107M Aug 14 23:56 gen-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root 280K Aug 14 23:56 gen-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root 284K Aug 14 23:56 gen-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 109M Aug 14 23:56 gen-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root 279K Aug 14 23:56 gen-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root 284K Aug 14 23:56 gen-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 111M Aug 14 23:56 gen-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root 286K Aug 14 23:56 gen-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root 289K Aug 14 23:56 gen-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root 113M Aug 14 23:56 gen-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root 294K Aug 14 23:56 gen-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root  36K Aug 14 23:56 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 297K Aug 14 23:56 gen-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 115M Aug 14 23:56 gen-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root 296K Aug 14 23:56 gen-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root 302K Aug 14 23:56 gen-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 117M Aug 14 23:56 gen-word-6100-count.jsonl\n",
      "-rw-r--r-- 1 root root 305K Aug 14 23:56 gen-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root 305K Aug 14 23:56 gen-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 118M Aug 14 23:56 gen-word-6200-count.jsonl\n",
      "-rw-r--r-- 1 root root 304K Aug 14 23:56 gen-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root 311K Aug 14 23:56 gen-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 120M Aug 14 23:56 gen-word-6300-count.jsonl\n",
      "-rw-r--r-- 1 root root 316K Aug 14 23:56 gen-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root 321K Aug 14 23:56 gen-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root 122M Aug 14 23:56 gen-word-6400-count.jsonl\n",
      "-rw-r--r-- 1 root root 317K Aug 14 23:56 gen-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root  38K Aug 14 23:56 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 319K Aug 14 23:56 gen-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 124M Aug 14 23:56 gen-word-6500-count.jsonl\n",
      "-rw-r--r-- 1 root root 326K Aug 14 23:56 gen-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root 329K Aug 14 23:56 gen-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 126M Aug 14 23:56 gen-word-6600-count.jsonl\n",
      "-rw-r--r-- 1 root root 327K Aug 14 23:56 gen-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root 324K Aug 14 23:56 gen-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 128M Aug 14 23:56 gen-word-6700-count.jsonl\n",
      "-rw-r--r-- 1 root root 335K Aug 14 23:56 gen-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root 340K Aug 14 23:56 gen-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 130M Aug 14 23:56 gen-word-6800-count.jsonl\n",
      "-rw-r--r-- 1 root root 339K Aug 14 23:56 gen-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root 343K Aug 14 23:56 gen-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root 132M Aug 14 23:56 gen-word-6900-count.jsonl\n",
      "-rw-r--r-- 1 root root 339K Aug 14 23:56 gen-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root  40K Aug 14 23:56 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 338K Aug 14 23:56 gen-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 134M Aug 14 23:56 gen-word-7000-count.jsonl\n",
      "-rw-r--r-- 1 root root 350K Aug 14 23:56 gen-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root 346K Aug 14 23:56 gen-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 136M Aug 14 23:56 gen-word-7100-count.jsonl\n",
      "-rw-r--r-- 1 root root 355K Aug 14 23:56 gen-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root 358K Aug 14 23:56 gen-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 137M Aug 14 23:56 gen-word-7200-count.jsonl\n",
      "-rw-r--r-- 1 root root 359K Aug 14 23:56 gen-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root 366K Aug 14 23:56 gen-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 139M Aug 14 23:56 gen-word-7300-count.jsonl\n",
      "-rw-r--r-- 1 root root 365K Aug 14 23:56 gen-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root 363K Aug 14 23:56 gen-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root 141M Aug 14 23:56 gen-word-7400-count.jsonl\n",
      "-rw-r--r-- 1 root root 367K Aug 14 23:56 gen-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root  42K Aug 14 23:56 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 368K Aug 14 23:56 gen-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 143M Aug 14 23:56 gen-word-7500-count.jsonl\n",
      "-rw-r--r-- 1 root root 371K Aug 14 23:56 gen-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root 372K Aug 14 23:56 gen-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 145M Aug 14 23:56 gen-word-7600-count.jsonl\n",
      "-rw-r--r-- 1 root root 375K Aug 14 23:56 gen-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root 377K Aug 14 23:56 gen-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 147M Aug 14 23:56 gen-word-7700-count.jsonl\n",
      "-rw-r--r-- 1 root root 374K Aug 14 23:56 gen-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root 378K Aug 14 23:56 gen-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 149M Aug 14 23:56 gen-word-7800-count.jsonl\n",
      "-rw-r--r-- 1 root root 388K Aug 14 23:56 gen-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root 387K Aug 14 23:56 gen-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root 151M Aug 14 23:56 gen-word-7900-count.jsonl\n",
      "-rw-r--r-- 1 root root 383K Aug 14 23:56 gen-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root  43K Aug 14 23:56 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 398K Aug 14 23:56 gen-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 153M Aug 14 23:56 gen-word-8000-count.jsonl\n",
      "-rw-r--r-- 1 root root 391K Aug 14 23:56 gen-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root 395K Aug 14 23:56 gen-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root 392K Aug 14 23:56 gen-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root 407K Aug 14 23:56 gen-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root 403K Aug 14 23:56 gen-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root 404K Aug 14 23:56 gen-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root 417K Aug 14 23:56 gen-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root 416K Aug 14 23:56 gen-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root 418K Aug 14 23:56 gen-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root  44K Aug 14 23:56 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 420K Aug 14 23:56 gen-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root 427K Aug 14 23:56 gen-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root 426K Aug 14 23:56 gen-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root 426K Aug 14 23:56 gen-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root 430K Aug 14 23:56 gen-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root 430K Aug 14 23:56 gen-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root 436K Aug 14 23:56 gen-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root 432K Aug 14 23:56 gen-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root 440K Aug 14 23:56 gen-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root 441K Aug 14 23:56 gen-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root  50K Aug 14 23:56 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 447K Aug 14 23:56 gen-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root 442K Aug 14 23:56 gen-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root 445K Aug 14 23:56 gen-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root 452K Aug 14 23:56 gen-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root 456K Aug 14 23:56 gen-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root 459K Aug 14 23:56 gen-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root 461K Aug 14 23:56 gen-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root 461K Aug 14 23:56 gen-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root 462K Aug 14 23:56 gen-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root 467K Aug 14 23:56 gen-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root  49K Aug 14 23:56 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 473K Aug 14 23:56 gen-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root 469K Aug 14 23:56 gen-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root 473K Aug 14 23:56 gen-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root 467K Aug 14 23:56 gen-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root 483K Aug 14 23:56 gen-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root 480K Aug 14 23:56 gen-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root 484K Aug 14 23:56 gen-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root 487K Aug 14 23:56 gen-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root 483K Aug 14 23:56 gen-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root 492K Aug 14 23:56 gen-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root  55K Aug 14 23:56 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 523K Aug 14 23:56 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 524K Aug 14 23:56 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 518K Aug 14 23:56 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 519K Aug 14 23:56 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  44K Aug 14 23:56 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 522K Aug 14 23:56 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 520K Aug 14 23:56 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 520K Aug 14 23:56 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 14 23:56 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 523K Aug 14 23:56 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  40K Aug 14 23:56 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 14 23:56 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 14 23:56 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 520K Aug 14 23:56 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 523K Aug 14 23:56 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 518K Aug 14 23:56 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  36K Aug 14 23:56 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 524K Aug 14 23:56 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 514K Aug 14 23:56 shuffle-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 510K Aug 14 23:56 shuffle-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  36K Aug 14 23:56 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 14 23:56 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 14 23:56 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 14 23:56 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  84K Aug 14 23:56 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 14 23:56 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6200-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6400-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6600-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-6700-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-6800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-6900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 14 23:56 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7100-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7400-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7500-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7700-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 14 23:56 shuffle-word-7800-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-7900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 14 23:56 shuffle-word-8000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 14 23:56 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 14 23:56 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 14 23:56 shuffle-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 14 23:56 shuffle-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9K Aug 14 23:56 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 50 &\n",
    "for i in {5..1000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 50 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the word dataset\n",
    "# \n",
    "for i in {1100..8000..100} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -lh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-8k/', '--model.lr_init=1e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-8k-a.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/emb-weight-range/EWR-1B5-mem-template.yaml', '--trainer.logger.init_args.name=EWR-1B5-0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/EWR-1B5-E0_1-mem-ctx-8k/', '--model.lr_init=1e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/EWR-1B5-E0_1-mem-ctx-8k-a.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 828262989\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 828262989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230815_000059-5d01bfx0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/5d01bfx0\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [00:00<00:00, 245655.35it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-807220a526d71ea1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 205.23it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 4] Global seed set to 828262989\n",
      "[rank: 6] Global seed set to 828262989\n",
      "[rank: 5] Global seed set to 828262989\n",
      "[rank: 2] Global seed set to 828262989\n",
      "[rank: 3] Global seed set to 828262989\n",
      "[rank: 1] Global seed set to 828262989\n",
      "[rank: 7] Global seed set to 828262989\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-807220a526d71ea1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.51it/s]\n",
      "Map (num_proc=64):  17%|â–ˆâ–      | 26708/154894 [00:13<01:06, 1939.97 examples/s][rank: 1] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-15 00:01:40,995] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  18%|â–ˆâ–      | 27282/154894 [00:14<01:11, 1785.70 examples/s][rank: 6] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-15 00:01:41,341] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  18%|â–ˆâ–      | 27807/154894 [00:14<01:16, 1655.79 examples/s][rank: 2] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-15 00:01:41,738] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  19%|â–ˆâ–      | 28832/154894 [00:15<01:01, 2052.61 examples/s][rank: 5] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-15 00:01:42,217] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  19%|â–ˆâ–Œ      | 29962/154894 [00:15<00:47, 2615.45 examples/s][rank: 7] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-15 00:01:42,620] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  20%|â–ˆâ–Œ      | 30241/154894 [00:15<00:46, 2664.76 examples/s][rank: 4] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-15 00:01:42,724] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):  20%|â–ˆâ–Œ      | 30769/154894 [00:15<00:48, 2558.95 examples/s][rank: 3] Global seed set to 828262989\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-15 00:01:42,910] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 828262989                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-15 00:03:55,677] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  1.000e-04 (0.0001)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0672616958618164 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10126447677612305 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10120034217834473 seconds\n",
      "Time to load fused_adam op: 0.10125112533569336 seconds\n",
      "Time to load fused_adam op: 0.10119485855102539 seconds\n",
      "Time to load fused_adam op: 0.10111570358276367 seconds\n",
      "Time to load fused_adam op: 0.10129094123840332 seconds\n",
      "Time to load fused_adam op: 0.10147953033447266 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06873416900634766 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1018216609954834 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10098791122436523 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10169315338134766 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10159039497375488 seconds\n",
      "Time to load utils op: 0.10153865814208984 seconds\n",
      "Time to load utils op: 0.10153961181640625 seconds\n",
      "Time to load utils op: 0.10180783271789551 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026154518127441406 seconds\n",
      "Time to load utils op: 0.00022411346435546875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002689361572265625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030159950256347656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00022339820861816406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026535987854003906 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002257823944091797 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005013942718505859 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   6%| | 800/12317 [29:25<7:03:33,  2.21s/it, v_num=bfx0, train/loss=5.7/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 12317/12317 [7:41:29<00:00,  2.25s/it, v_num=bfx0, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 1/13 [00:00<00:05,  2.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 2/13 [00:00<00:04,  2.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 3/13 [00:01<00:03,  2.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4/13 [00:01<00:02,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 5/13 [00:02<00:03,  2.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 6/13 [00:02<00:03,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 7/13 [00:03<00:03,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 8/13 [00:04<00:02,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 9/13 [00:04<00:02,  1.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 10/13 [00:05<00:01,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11/13 [00:06<00:01,  1.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/13 [00:06<00:00,  1.82it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12317/12317 [7:41:46<00:00,  2.25s/it, v_num=bfx0, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12317/12317 [7:41:46<00:00,  2.25s/it, v_num=bfx0, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 12317/12317 [7:42:23<00:00,  2.25s/it, v_num=bfx0, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–„â–„â–…â–â–„â–…â–†â–ƒâ–„â–†â–…â–„â–†â–ƒâ–ƒâ–‡â–‡â–‡â–ƒâ–‚â–„â–†â–ˆâ–„â–‡â–…â–ƒâ–ˆâ–…â–ˆâ–ƒâ–‡â–ˆâ–…â–â–ƒâ–†â–â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–„â–„â–ƒâ–…â–â–â–ˆâ–â–†â–‚â–ƒâ–‚â–‚â–‚â–â–„â–…â–‡â–â–ˆâ–†â–â–â–â–ˆâ–â–â–â–â–‡â–…â–â–„â–‡â–â–ˆâ–…â–…â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 5586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.17188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.15399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mEWR-1B5-0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/5d01bfx0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230815_000059-5d01bfx0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/EWR-1B5-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-8k (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=8192 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=2 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/EWR-1B5-E0_1-mem-ctx-8k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/EWR-1B5-E0_1-mem-ctx-8k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 15 07:47 ../model/EWR-1B5-E0_1-mem-ctx-8k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-8k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-8k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 99.04761904761905% similarity, with 104 matched token, and 1 token mismatch\n",
      "## Model validation for 110 tokens : 99.0909090909091% similarity, with 109 matched token, and 1 token mismatch\n",
      "## Model validation for 115 tokens : 99.1304347826087% similarity, with 114 matched token, and 1 token mismatch\n",
      "## Model validation for 120 tokens : 99.16666666666667% similarity, with 119 matched token, and 1 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 98.62068965517241% similarity, with 143 matched token, and 2 token mismatch\n",
      "## Model validation for 150 tokens : 98.66666666666667% similarity, with 148 matched token, and 2 token mismatch\n",
      "## Model validation for 160 tokens : 99.375% similarity, with 159 matched token, and 1 token mismatch\n",
      "## Model validation for 170 tokens : 98.23529411764706% similarity, with 167 matched token, and 3 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.42105263157895% similarity, with 187 matched token, and 3 token mismatch\n",
      "## Model validation for 200 tokens : 98.5% similarity, with 197 matched token, and 3 token mismatch\n",
      "## Model validation for 210 tokens : 98.57142857142858% similarity, with 207 matched token, and 3 token mismatch\n",
      "## Model validation for 220 tokens : 97.72727272727273% similarity, with 215 matched token, and 5 token mismatch\n",
      "## Model validation for 230 tokens : 97.82608695652173% similarity, with 225 matched token, and 5 token mismatch\n",
      "## Model validation for 240 tokens : 98.33333333333333% similarity, with 236 matched token, and 4 token mismatch\n",
      "## Model validation for 250 tokens : 98.4% similarity, with 246 matched token, and 4 token mismatch\n",
      "## Model validation for 260 tokens : 98.84615384615385% similarity, with 257 matched token, and 3 token mismatch\n",
      "## Model validation for 270 tokens : 97.77777777777777% similarity, with 264 matched token, and 6 token mismatch\n",
      "## Model validation for 280 tokens : 98.21428571428571% similarity, with 275 matched token, and 5 token mismatch\n",
      "## Model validation for 290 tokens : 97.93103448275862% similarity, with 284 matched token, and 6 token mismatch\n",
      "## Model validation for 300 tokens : 98.0% similarity, with 294 matched token, and 6 token mismatch\n",
      "## Model validation for 325 tokens : 99.07692307692308% similarity, with 322 matched token, and 3 token mismatch\n",
      "## Model validation for 350 tokens : 98.57142857142858% similarity, with 345 matched token, and 5 token mismatch\n",
      "## Model validation for 375 tokens : 97.86666666666667% similarity, with 367 matched token, and 8 token mismatch\n",
      "## Model validation for 400 tokens : 97.5% similarity, with 390 matched token, and 10 token mismatch\n",
      "## Model validation for 425 tokens : 96.94117647058823% similarity, with 412 matched token, and 13 token mismatch\n",
      "## Model validation for 450 tokens : 97.11111111111111% similarity, with 437 matched token, and 13 token mismatch\n",
      "## Model validation for 475 tokens : 97.05263157894737% similarity, with 461 matched token, and 14 token mismatch\n",
      "## Model validation for 500 tokens : 97.0% similarity, with 485 matched token, and 15 token mismatch\n",
      "## Model validation for 525 tokens : 96.57142857142857% similarity, with 507 matched token, and 18 token mismatch\n",
      "## Model validation for 550 tokens : 97.0909090909091% similarity, with 534 matched token, and 16 token mismatch\n",
      "## Model validation for 575 tokens : 96.34782608695652% similarity, with 554 matched token, and 21 token mismatch\n",
      "## Model validation for 600 tokens : 96.16666666666667% similarity, with 577 matched token, and 23 token mismatch\n",
      "## Model validation for 625 tokens : 96.16% similarity, with 601 matched token, and 24 token mismatch\n",
      "## Model validation for 650 tokens : 95.38461538461539% similarity, with 620 matched token, and 30 token mismatch\n",
      "## Model validation for 675 tokens : 95.55555555555556% similarity, with 645 matched token, and 30 token mismatch\n",
      "## Model validation for 700 tokens : 95.57142857142857% similarity, with 669 matched token, and 31 token mismatch\n",
      "## Model validation for 750 tokens : 96.39999999999999% similarity, with 723 matched token, and 27 token mismatch\n",
      "## Model validation for 800 tokens : 96.25% similarity, with 770 matched token, and 30 token mismatch\n",
      "## Model validation for 850 tokens : 95.88235294117648% similarity, with 815 matched token, and 35 token mismatch\n",
      "## Model validation for 900 tokens : 96.22222222222221% similarity, with 866 matched token, and 34 token mismatch\n",
      "## Model validation for 950 tokens : 95.57894736842105% similarity, with 908 matched token, and 42 token mismatch\n",
      "## Model validation for 1000 tokens : 95.1% similarity, with 951 matched token, and 49 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 1100 tokens : 95.18181818181817% similarity, with 1047 matched token, and 53 token mismatch\n",
      "## Model validation for 1150 tokens : 94.86956521739131% similarity, with 1091 matched token, and 59 token mismatch\n",
      "## Model validation for 1200 tokens : 94.33333333333334% similarity, with 1132 matched token, and 68 token mismatch\n",
      "## Model validation for 1250 tokens : 93.36% similarity, with 1167 matched token, and 83 token mismatch\n",
      "## Model validation for 1300 tokens : 92.84615384615384% similarity, with 1207 matched token, and 93 token mismatch\n",
      "## Model validation for 1350 tokens : 93.03703703703704% similarity, with 1256 matched token, and 94 token mismatch\n",
      "## Model validation for 1400 tokens : 93.21428571428572% similarity, with 1305 matched token, and 95 token mismatch\n",
      "## Model validation for 1450 tokens : 92.55172413793103% similarity, with 1342 matched token, and 108 token mismatch\n",
      "## Model validation for 1500 tokens : 92.0% similarity, with 1380 matched token, and 120 token mismatch\n",
      "## Model validation for 1550 tokens : 91.80645161290323% similarity, with 1423 matched token, and 127 token mismatch\n",
      "## Model validation for 1600 tokens : 91.4375% similarity, with 1463 matched token, and 137 token mismatch\n",
      "## Model validation for 1650 tokens : 89.93939393939394% similarity, with 1484 matched token, and 166 token mismatch\n",
      "## Model validation for 1700 tokens : 88.94117647058823% similarity, with 1512 matched token, and 188 token mismatch\n",
      "## Model validation for 1750 tokens : 87.6% similarity, with 1533 matched token, and 217 token mismatch\n",
      "## Model validation for 1800 tokens : 85.83333333333333% similarity, with 1545 matched token, and 255 token mismatch\n",
      "## Model validation for 1850 tokens : 83.45945945945947% similarity, with 1544 matched token, and 306 token mismatch\n",
      "## Model validation for 1900 tokens : 82.05263157894737% similarity, with 1559 matched token, and 341 token mismatch\n",
      "## Model validation for 1950 tokens : 78.76923076923077% similarity, with 1536 matched token, and 414 token mismatch\n",
      "## Model validation for 2000 tokens : 76.2% similarity, with 1524 matched token, and 476 token mismatch\n",
      "## Model validation for 2050 tokens : 75.1219512195122% similarity, with 1540 matched token, and 510 token mismatch\n",
      "## Model validation for 2100 tokens : 72.28571428571429% similarity, with 1518 matched token, and 582 token mismatch\n",
      "## Model validation for 2150 tokens : 69.34883720930233% similarity, with 1491 matched token, and 659 token mismatch\n",
      "## Model validation for 2200 tokens : 66.81818181818183% similarity, with 1470 matched token, and 730 token mismatch\n",
      "## Model validation for 2250 tokens : 64.93333333333334% similarity, with 1461 matched token, and 789 token mismatch\n",
      "## Model validation for 2300 tokens : 63.0% similarity, with 1449 matched token, and 851 token mismatch\n",
      "## Model validation for 2350 tokens : 61.40425531914894% similarity, with 1443 matched token, and 907 token mismatch\n",
      "## Model validation for 2400 tokens : 59.958333333333336% similarity, with 1439 matched token, and 961 token mismatch\n",
      "## Model validation for 2450 tokens : 58.57142857142858% similarity, with 1435 matched token, and 1015 token mismatch\n",
      "## Model validation for 2500 tokens : 56.96% similarity, with 1424 matched token, and 1076 token mismatch\n",
      "## Model validation for 2550 tokens : 55.25490196078431% similarity, with 1409 matched token, and 1141 token mismatch\n",
      "## Model validation for 2600 tokens : 53.61538461538462% similarity, with 1394 matched token, and 1206 token mismatch\n",
      "## Model validation for 2650 tokens : 52.33962264150943% similarity, with 1387 matched token, and 1263 token mismatch\n",
      "## Model validation for 2700 tokens : 50.66666666666667% similarity, with 1368 matched token, and 1332 token mismatch\n",
      "## Model validation for 2750 tokens : 48.981818181818184% similarity, with 1347 matched token, and 1403 token mismatch\n",
      "## Model validation for 2800 tokens : 47.17857142857143% similarity, with 1321 matched token, and 1479 token mismatch\n",
      "## Model validation for 2850 tokens : 45.08771929824562% similarity, with 1285 matched token, and 1565 token mismatch\n",
      "## Model validation for 2900 tokens : 44.0% similarity, with 1276 matched token, and 1624 token mismatch\n",
      "## Model validation for 2950 tokens : 42.06779661016949% similarity, with 1241 matched token, and 1709 token mismatch\n",
      "## Model validation for 3000 tokens : 40.6% similarity, with 1218 matched token, and 1782 token mismatch\n",
      "## Model validation for 3050 tokens : 39.278688524590166% similarity, with 1198 matched token, and 1852 token mismatch\n",
      "## Model validation for 3100 tokens : 37.41935483870968% similarity, with 1160 matched token, and 1940 token mismatch\n",
      "## Model validation for 3150 tokens : 35.492063492063494% similarity, with 1118 matched token, and 2032 token mismatch\n",
      "## Model validation for 3200 tokens : 33.9375% similarity, with 1086 matched token, and 2114 token mismatch\n",
      "## Model validation for 3250 tokens : 32.61538461538461% similarity, with 1060 matched token, and 2190 token mismatch\n",
      "## Model validation for 3300 tokens : 30.636363636363633% similarity, with 1011 matched token, and 2289 token mismatch\n",
      "## Model validation for 3350 tokens : 28.955223880597014% similarity, with 970 matched token, and 2380 token mismatch\n",
      "## Model validation for 3400 tokens : 27.0% similarity, with 918 matched token, and 2482 token mismatch\n",
      "## Model validation for 3450 tokens : 26.347826086956523% similarity, with 909 matched token, and 2541 token mismatch\n",
      "## Model validation for 3500 tokens : 24.571428571428573% similarity, with 860 matched token, and 2640 token mismatch\n",
      "## Model validation for 3550 tokens : 23.43661971830986% similarity, with 832 matched token, and 2718 token mismatch\n",
      "## Model validation for 3600 tokens : 21.972222222222225% similarity, with 791 matched token, and 2809 token mismatch\n",
      "## Model validation for 3650 tokens : 21.26027397260274% similarity, with 776 matched token, and 2874 token mismatch\n",
      "## Model validation for 3700 tokens : 20.2972972972973% similarity, with 751 matched token, and 2949 token mismatch\n",
      "## Model validation for 3750 tokens : 19.413333333333334% similarity, with 728 matched token, and 3022 token mismatch\n",
      "## Model validation for 3800 tokens : 18.5% similarity, with 703 matched token, and 3097 token mismatch\n",
      "## Model validation for 3850 tokens : 17.636363636363637% similarity, with 679 matched token, and 3171 token mismatch\n",
      "## Model validation for 3900 tokens : 16.564102564102566% similarity, with 646 matched token, and 3254 token mismatch\n",
      "## Model validation for 3950 tokens : 15.620253164556964% similarity, with 617 matched token, and 3333 token mismatch\n",
      "## Model validation for 4000 tokens : 14.825% similarity, with 593 matched token, and 3407 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k.pth\" \"none\" 1100 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

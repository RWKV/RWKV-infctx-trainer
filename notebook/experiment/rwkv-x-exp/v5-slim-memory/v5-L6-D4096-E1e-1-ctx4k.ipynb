{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5-slim / embedding init-range 1e-01 / 4k\n",
    "\n",
    "- 6 layers\n",
    "- 4096 embedding size\n",
    "\n",
    "Going through the modified memory training for v5 models, across various initial embedding model weights\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Additional dependencies for eval stuff\n",
    "!pip install -q aiocsv aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "\n",
    "RWKV_WAVENET_LAYERS=1\n",
    "\n",
    "EMBED_SCALE=0.1\n",
    "EMBED_SCALE_LABEL=str(EMBED_SCALE).replace(\".\", \"_\")\n",
    "\n",
    "LAYER_COUNT=6\n",
    "EMBED_DIM=4096\n",
    "\n",
    "WANDB_PREFIX=f\"v5-L{LAYER_COUNT}-D{EMBED_DIM}-E{EMBED_SCALE}\"\n",
    "FILENAME_PREFIX=f\"v5-L{LAYER_COUNT}-D{EMBED_DIM}-E{EMBED_SCALE_LABEL}\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 4096\n",
      "Output model path: ../model/L6-D4096-E0_1-neox-v5base-init.pth\n",
      "Vocab size: 50277\n",
      "Emb scale: 0.1\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "50277 4096  -0.1 emb.weight\n",
      "4096  4096  1.0  blocks.0.att.receptance.weight\n",
      "4096  4096  1.0  blocks.0.att.key.weight\n",
      "4096  4096  1.0  blocks.0.att.value.weight\n",
      "4096  4096  0    blocks.0.att.output.weight\n",
      "16384 4096  1.0  blocks.0.ffn.key.weight\n",
      "4096  4096  0    blocks.0.ffn.receptance.weight\n",
      "4096  16384 0    blocks.0.ffn.value.weight\n",
      "4096  4096  1.0  blocks.1.att.receptance.weight\n",
      "4096  4096  1.0  blocks.1.att.key.weight\n",
      "4096  4096  1.0  blocks.1.att.value.weight\n",
      "4096  4096  0    blocks.1.att.output.weight\n",
      "16384 4096  1.0  blocks.1.ffn.key.weight\n",
      "4096  4096  0    blocks.1.ffn.receptance.weight\n",
      "4096  16384 0    blocks.1.ffn.value.weight\n",
      "4096  4096  1.0  blocks.2.att.receptance.weight\n",
      "4096  4096  1.0  blocks.2.att.key.weight\n",
      "4096  4096  1.0  blocks.2.att.value.weight\n",
      "4096  4096  0    blocks.2.att.output.weight\n",
      "16384 4096  1.0  blocks.2.ffn.key.weight\n",
      "4096  4096  0    blocks.2.ffn.receptance.weight\n",
      "4096  16384 0    blocks.2.ffn.value.weight\n",
      "4096  4096  1.0  blocks.3.att.receptance.weight\n",
      "4096  4096  1.0  blocks.3.att.key.weight\n",
      "4096  4096  1.0  blocks.3.att.value.weight\n",
      "4096  4096  0    blocks.3.att.output.weight\n",
      "16384 4096  1.0  blocks.3.ffn.key.weight\n",
      "4096  4096  0    blocks.3.ffn.receptance.weight\n",
      "4096  16384 0    blocks.3.ffn.value.weight\n",
      "4096  4096  1.0  blocks.4.att.receptance.weight\n",
      "4096  4096  1.0  blocks.4.att.key.weight\n",
      "4096  4096  1.0  blocks.4.att.value.weight\n",
      "4096  4096  0    blocks.4.att.output.weight\n",
      "16384 4096  1.0  blocks.4.ffn.key.weight\n",
      "4096  4096  0    blocks.4.ffn.receptance.weight\n",
      "4096  16384 0    blocks.4.ffn.value.weight\n",
      "4096  4096  1.0  blocks.5.att.receptance.weight\n",
      "4096  4096  1.0  blocks.5.att.key.weight\n",
      "4096  4096  1.0  blocks.5.att.value.weight\n",
      "4096  4096  0    blocks.5.att.output.weight\n",
      "16384 4096  1.0  blocks.5.ffn.key.weight\n",
      "4096  4096  0    blocks.5.ffn.receptance.weight\n",
      "4096  16384 0    blocks.5.ffn.value.weight\n",
      "50277 4096  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 6 --n_embd 4096 \\\n",
    "        --emb-scale \"{EMBED_SCALE}\" \\\n",
    "        --vocab_size neox --skip-if-exists \\\n",
    "        \"../model/L{LAYER_COUNT}-D{EMBED_DIM}-E{EMBED_SCALE_LABEL}-neox-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enwiki Stage 1 : Foundation 4k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 433/433 [00:00<00:00, 4.46MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                              | 0.00/261M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 37.9k/261M [00:00<12:32, 347kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 143k/261M [00:00<07:53, 550kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 567k/261M [00:00<02:15, 1.93MB/s]\u001b[A\n",
      "Downloading data:   0%|                     | 1.26M/261M [00:00<01:11, 3.65MB/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 1.69M/261M [00:00<01:07, 3.82MB/s]\u001b[A\n",
      "Downloading data:   2%|â–Ž                    | 4.02M/261M [00:00<00:25, 10.0MB/s]\u001b[A\n",
      "Downloading data:   3%|â–‹                    | 7.80M/261M [00:00<00:13, 18.7MB/s]\u001b[A\n",
      "Downloading data:   4%|â–‰                    | 11.5M/261M [00:00<00:11, 22.0MB/s]\u001b[A\n",
      "Downloading data:   6%|â–ˆâ–                   | 15.2M/261M [00:01<00:09, 26.4MB/s]\u001b[A\n",
      "Downloading data:   8%|â–ˆâ–Œ                   | 19.7M/261M [00:01<00:07, 30.2MB/s]\u001b[A\n",
      "Downloading data:   9%|â–ˆâ–‰                   | 24.1M/261M [00:01<00:06, 34.0MB/s]\u001b[A\n",
      "Downloading data:  11%|â–ˆâ–ˆâ–Ž                  | 28.0M/261M [00:01<00:06, 35.1MB/s]\u001b[A\n",
      "Downloading data:  13%|â–ˆâ–ˆâ–‹                  | 34.0M/261M [00:01<00:05, 42.3MB/s]\u001b[A\n",
      "Downloading data:  15%|â–ˆâ–ˆâ–ˆ                  | 38.3M/261M [00:01<00:05, 42.0MB/s]\u001b[A\n",
      "Downloading data:  16%|â–ˆâ–ˆâ–ˆâ–                 | 42.6M/261M [00:01<00:05, 41.7MB/s]\u001b[A\n",
      "Downloading data:  18%|â–ˆâ–ˆâ–ˆâ–Š                 | 46.8M/261M [00:01<00:05, 40.0MB/s]\u001b[A\n",
      "Downloading data:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 51.2M/261M [00:01<00:05, 41.2MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 55.7M/261M [00:01<00:04, 42.2MB/s]\u001b[A\n",
      "Downloading data:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 60.1M/261M [00:02<00:04, 42.8MB/s]\u001b[A\n",
      "Downloading data:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 64.8M/261M [00:02<00:04, 43.9MB/s]\u001b[A\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 69.2M/261M [00:02<00:04, 40.8MB/s]\u001b[A\n",
      "Downloading data:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 73.3M/261M [00:02<00:04, 41.0MB/s]\u001b[A\n",
      "Downloading data:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 79.5M/261M [00:02<00:04, 43.3MB/s]\u001b[A\n",
      "Downloading data:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 83.9M/261M [00:02<00:04, 38.3MB/s]\u001b[A\n",
      "Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 87.8M/261M [00:02<00:04, 36.6MB/s]\u001b[A\n",
      "Downloading data:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 92.4M/261M [00:02<00:04, 37.3MB/s]\u001b[A\n",
      "Downloading data:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 99.1M/261M [00:03<00:03, 41.0MB/s]\u001b[A\n",
      "Downloading data:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 103M/261M [00:03<00:03, 41.0MB/s]\u001b[A\n",
      "Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 108M/261M [00:03<00:03, 42.0MB/s]\u001b[A\n",
      "Downloading data:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 112M/261M [00:03<00:03, 38.9MB/s]\u001b[A\n",
      "Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 116M/261M [00:03<00:03, 40.8MB/s]\u001b[A\n",
      "Downloading data:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 122M/261M [00:03<00:03, 45.2MB/s]\u001b[A\n",
      "Downloading data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 127M/261M [00:03<00:03, 42.4MB/s]\u001b[A\n",
      "Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 132M/261M [00:03<00:02, 43.9MB/s]\u001b[A\n",
      "Downloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 136M/261M [00:03<00:02, 43.7MB/s]\u001b[A\n",
      "Downloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 141M/261M [00:03<00:02, 44.3MB/s]\u001b[A\n",
      "Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 145M/261M [00:04<00:02, 41.5MB/s]\u001b[A\n",
      "Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 149M/261M [00:04<00:02, 41.7MB/s]\u001b[A\n",
      "Downloading data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 154M/261M [00:04<00:02, 43.7MB/s]\u001b[A\n",
      "Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 159M/261M [00:04<00:02, 44.8MB/s]\u001b[A\n",
      "Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 163M/261M [00:04<00:02, 37.4MB/s]\u001b[A\n",
      "Downloading data:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 168M/261M [00:04<00:02, 38.7MB/s]\u001b[A\n",
      "Downloading data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 172M/261M [00:04<00:02, 38.7MB/s]\u001b[A\n",
      "Downloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 177M/261M [00:04<00:01, 41.7MB/s]\u001b[A\n",
      "Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 182M/261M [00:05<00:01, 43.2MB/s]\u001b[A\n",
      "Downloading data:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 187M/261M [00:05<00:01, 43.3MB/s]\u001b[A\n",
      "Downloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 191M/261M [00:05<00:01, 39.7MB/s]\u001b[A\n",
      "Downloading data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 195M/261M [00:05<00:01, 40.8MB/s]\u001b[A\n",
      "Downloading data:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 200M/261M [00:05<00:01, 42.3MB/s]\u001b[A\n",
      "Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 205M/261M [00:05<00:01, 45.1MB/s]\u001b[A\n",
      "Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 210M/261M [00:05<00:01, 42.4MB/s]\u001b[A\n",
      "Downloading data:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214M/261M [00:05<00:01, 42.9MB/s]\u001b[A\n",
      "Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 219M/261M [00:05<00:01, 39.9MB/s]\u001b[A\n",
      "Downloading data:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 224M/261M [00:06<00:00, 43.3MB/s]\u001b[A\n",
      "Downloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 228M/261M [00:06<00:00, 43.3MB/s]\u001b[A\n",
      "Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 233M/261M [00:06<00:00, 44.6MB/s]\u001b[A\n",
      "Downloading data:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 238M/261M [00:06<00:00, 41.3MB/s]\u001b[A\n",
      "Downloading data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 242M/261M [00:06<00:00, 37.5MB/s]\u001b[A\n",
      "Downloading data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 247M/261M [00:06<00:00, 41.1MB/s]\u001b[A\n",
      "Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 251M/261M [00:06<00:00, 31.7MB/s]\u001b[A\n",
      "Downloading data:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255M/261M [00:06<00:00, 32.1MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261M/261M [00:07<00:00, 36.7MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|â–Ž                    | 3.27M/257M [00:00<00:09, 26.3MB/s]\u001b[A\n",
      "Downloading data:   4%|â–Š                    | 9.44M/257M [00:00<00:05, 42.1MB/s]\u001b[A\n",
      "Downloading data:   7%|â–ˆâ–                   | 17.6M/257M [00:00<00:04, 58.7MB/s]\u001b[A\n",
      "Downloading data:  10%|â–ˆâ–ˆ                   | 24.9M/257M [00:00<00:03, 64.0MB/s]\u001b[A\n",
      "Downloading data:  13%|â–ˆâ–ˆâ–‹                  | 33.1M/257M [00:00<00:03, 70.2MB/s]\u001b[A\n",
      "Downloading data:  16%|â–ˆâ–ˆâ–ˆâ–Ž                 | 40.3M/257M [00:00<00:03, 64.9MB/s]\u001b[A\n",
      "Downloading data:  18%|â–ˆâ–ˆâ–ˆâ–Š                 | 46.9M/257M [00:00<00:03, 61.8MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 54.6M/257M [00:00<00:03, 66.1MB/s]\u001b[A\n",
      "Downloading data:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 61.3M/257M [00:01<00:03, 62.9MB/s]\u001b[A\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 70.1M/257M [00:01<00:02, 69.9MB/s]\u001b[A\n",
      "Downloading data:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 77.4M/257M [00:01<00:02, 71.0MB/s]\u001b[A\n",
      "Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 84.6M/257M [00:01<00:02, 61.6MB/s]\u001b[A\n",
      "Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 93.3M/257M [00:01<00:02, 68.3MB/s]\u001b[A\n",
      "Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 100M/257M [00:01<00:02, 66.9MB/s]\u001b[A\n",
      "Downloading data:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 111M/257M [00:01<00:01, 76.3MB/s]\u001b[A\n",
      "Downloading data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 118M/257M [00:01<00:01, 72.1MB/s]\u001b[A\n",
      "Downloading data:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 126M/257M [00:01<00:01, 72.6MB/s]\u001b[A\n",
      "Downloading data:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 135M/257M [00:01<00:01, 77.7MB/s]\u001b[A\n",
      "Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 143M/257M [00:02<00:01, 74.1MB/s]\u001b[A\n",
      "Downloading data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 150M/257M [00:02<00:01, 70.9MB/s]\u001b[A\n",
      "Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 158M/257M [00:02<00:01, 62.9MB/s]\u001b[A\n",
      "Downloading data:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 167M/257M [00:02<00:01, 70.2MB/s]\u001b[A\n",
      "Downloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 174M/257M [00:02<00:01, 70.3MB/s]\u001b[A\n",
      "Downloading data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 182M/257M [00:02<00:01, 70.1MB/s]\u001b[A\n",
      "Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 189M/257M [00:02<00:00, 69.0MB/s]\u001b[A\n",
      "Downloading data:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 196M/257M [00:02<00:00, 63.4MB/s]\u001b[A\n",
      "Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 204M/257M [00:03<00:00, 62.3MB/s]\u001b[A\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 212M/257M [00:03<00:00, 66.5MB/s]\u001b[A\n",
      "Downloading data:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 221M/257M [00:03<00:00, 71.4MB/s]\u001b[A\n",
      "Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228M/257M [00:03<00:00, 66.7MB/s]\u001b[A\n",
      "Downloading data:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 235M/257M [00:03<00:00, 65.9MB/s]\u001b[A\n",
      "Downloading data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 243M/257M [00:03<00:00, 70.8MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 257M/257M [00:03<00:00, 67.5MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/260M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 3.08M/260M [00:00<00:09, 28.0MB/s]\u001b[A\n",
      "Downloading data:   3%|â–Œ                    | 6.87M/260M [00:00<00:07, 33.6MB/s]\u001b[A\n",
      "Downloading data:   5%|â–‰                    | 11.7M/260M [00:00<00:06, 39.5MB/s]\u001b[A\n",
      "Downloading data:   7%|â–ˆâ–                   | 17.9M/260M [00:00<00:05, 48.0MB/s]\u001b[A\n",
      "Downloading data:   9%|â–ˆâ–‰                   | 24.6M/260M [00:00<00:04, 54.7MB/s]\u001b[A\n",
      "Downloading data:  12%|â–ˆâ–ˆâ–                  | 30.1M/260M [00:00<00:04, 48.5MB/s]\u001b[A\n",
      "Downloading data:  15%|â–ˆâ–ˆâ–ˆâ–                 | 38.6M/260M [00:00<00:04, 54.6MB/s]\u001b[A\n",
      "Downloading data:  19%|â–ˆâ–ˆâ–ˆâ–‰                 | 48.3M/260M [00:00<00:03, 66.4MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 55.5M/260M [00:00<00:03, 65.3MB/s]\u001b[A\n",
      "Downloading data:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 62.2M/260M [00:01<00:03, 64.1MB/s]\u001b[A\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 70.5M/260M [00:01<00:02, 69.5MB/s]\u001b[A\n",
      "Downloading data:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 77.6M/260M [00:01<00:02, 62.5MB/s]\u001b[A\n",
      "Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 85.3M/260M [00:01<00:02, 64.1MB/s]\u001b[A\n",
      "Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 92.4M/260M [00:01<00:02, 65.9MB/s]\u001b[A\n",
      "Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 100M/260M [00:01<00:02, 70.0MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 108M/260M [00:01<00:02, 72.6MB/s]\u001b[A\n",
      "Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 116M/260M [00:01<00:02, 62.9MB/s]\u001b[A\n",
      "Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 124M/260M [00:02<00:02, 65.3MB/s]\u001b[A\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 133M/260M [00:02<00:01, 71.2MB/s]\u001b[A\n",
      "Downloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 140M/260M [00:02<00:01, 65.7MB/s]\u001b[A\n",
      "Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 148M/260M [00:02<00:01, 68.3MB/s]\u001b[A\n",
      "Downloading data:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 155M/260M [00:02<00:01, 61.6MB/s]\u001b[A\n",
      "Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 165M/260M [00:02<00:01, 68.9MB/s]\u001b[A\n",
      "Downloading data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 172M/260M [00:02<00:01, 68.2MB/s]\u001b[A\n",
      "Downloading data:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 179M/260M [00:02<00:01, 65.5MB/s]\u001b[A\n",
      "Downloading data:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 187M/260M [00:02<00:01, 69.3MB/s]\u001b[A\n",
      "Downloading data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 194M/260M [00:03<00:00, 69.5MB/s]\u001b[A\n",
      "Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 202M/260M [00:03<00:00, 72.2MB/s]\u001b[A\n",
      "Downloading data:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 209M/260M [00:03<00:00, 64.9MB/s]\u001b[A\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 216M/260M [00:03<00:00, 66.0MB/s]\u001b[A\n",
      "Downloading data:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 224M/260M [00:03<00:00, 64.6MB/s]\u001b[A\n",
      "Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 232M/260M [00:03<00:00, 64.1MB/s]\u001b[A\n",
      "Downloading data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 241M/260M [00:03<00:00, 69.2MB/s]\u001b[A\n",
      "Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 248M/260M [00:03<00:00, 70.2MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 260M/260M [00:04<00:00, 64.7MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 52.2k/257M [00:00<14:51, 289kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 220k/257M [00:00<04:45, 901kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 609k/257M [00:00<02:38, 1.62MB/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 1.89M/257M [00:00<00:52, 4.87MB/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 3.04M/257M [00:00<00:37, 6.80MB/s]\u001b[A\n",
      "Downloading data:   3%|â–Œ                    | 7.11M/257M [00:00<00:14, 16.9MB/s]\u001b[A\n",
      "Downloading data:   4%|â–‰                    | 10.9M/257M [00:00<00:10, 22.5MB/s]\u001b[A\n",
      "Downloading data:   6%|â–ˆâ–Ž                   | 15.8M/257M [00:00<00:08, 30.2MB/s]\u001b[A\n",
      "Downloading data:   8%|â–ˆâ–‹                   | 21.3M/257M [00:01<00:06, 34.6MB/s]\u001b[A\n",
      "Downloading data:  10%|â–ˆâ–ˆ                   | 25.5M/257M [00:01<00:06, 36.8MB/s]\u001b[A\n",
      "Downloading data:  12%|â–ˆâ–ˆâ–                  | 30.2M/257M [00:01<00:06, 37.4MB/s]\u001b[A\n",
      "Downloading data:  13%|â–ˆâ–ˆâ–Š                  | 34.6M/257M [00:01<00:05, 39.3MB/s]\u001b[A\n",
      "Downloading data:  15%|â–ˆâ–ˆâ–ˆâ–                 | 39.4M/257M [00:01<00:05, 41.9MB/s]\u001b[A\n",
      "Downloading data:  17%|â–ˆâ–ˆâ–ˆâ–‹                 | 44.5M/257M [00:01<00:04, 44.5MB/s]\u001b[A\n",
      "Downloading data:  19%|â–ˆâ–ˆâ–ˆâ–‰                 | 49.0M/257M [00:01<00:04, 44.5MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 53.7M/257M [00:01<00:04, 41.4MB/s]\u001b[A\n",
      "Downloading data:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 57.9M/257M [00:01<00:05, 37.8MB/s]\u001b[A\n",
      "Downloading data:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 61.7M/257M [00:02<00:05, 37.8MB/s]\u001b[A\n",
      "Downloading data:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 65.6M/257M [00:02<00:06, 29.0MB/s]\u001b[A\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 69.9M/257M [00:02<00:05, 32.3MB/s]\u001b[A\n",
      "Downloading data:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 74.6M/257M [00:02<00:05, 35.9MB/s]\u001b[A\n",
      "Downloading data:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 78.9M/257M [00:02<00:04, 37.8MB/s]\u001b[A\n",
      "Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 84.1M/257M [00:02<00:04, 41.4MB/s]\u001b[A\n",
      "Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 88.4M/257M [00:02<00:04, 39.4MB/s]\u001b[A\n",
      "Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 93.0M/257M [00:02<00:03, 41.1MB/s]\u001b[A\n",
      "Downloading data:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 97.6M/257M [00:03<00:03, 42.5MB/s]\u001b[A\n",
      "Downloading data:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 102M/257M [00:03<00:03, 39.9MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 107M/257M [00:03<00:03, 42.9MB/s]\u001b[A\n",
      "Downloading data:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 112M/257M [00:03<00:03, 44.4MB/s]\u001b[A\n",
      "Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 116M/257M [00:03<00:03, 41.3MB/s]\u001b[A\n",
      "Downloading data:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 121M/257M [00:03<00:03, 41.3MB/s]\u001b[A\n",
      "Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 125M/257M [00:03<00:03, 39.0MB/s]\u001b[A\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 130M/257M [00:03<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 134M/257M [00:03<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 139M/257M [00:04<00:02, 44.7MB/s]\u001b[A\n",
      "Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 144M/257M [00:04<00:02, 40.1MB/s]\u001b[A\n",
      "Downloading data:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 149M/257M [00:04<00:02, 40.2MB/s]\u001b[A\n",
      "Downloading data:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 153M/257M [00:04<00:02, 41.6MB/s]\u001b[A\n",
      "Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 158M/257M [00:04<00:02, 43.0MB/s]\u001b[A\n",
      "Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 163M/257M [00:04<00:02, 44.7MB/s]\u001b[A\n",
      "Downloading data:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 167M/257M [00:04<00:02, 44.7MB/s]\u001b[A\n",
      "Downloading data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 172M/257M [00:04<00:02, 42.3MB/s]\u001b[A\n",
      "Downloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 176M/257M [00:04<00:01, 42.3MB/s]\u001b[A\n",
      "Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 181M/257M [00:05<00:01, 41.0MB/s]\u001b[A\n",
      "Downloading data:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 186M/257M [00:05<00:01, 43.3MB/s]\u001b[A\n",
      "Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 191M/257M [00:05<00:01, 40.3MB/s]\u001b[A\n",
      "Downloading data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 195M/257M [00:05<00:01, 40.7MB/s]\u001b[A\n",
      "Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 200M/257M [00:05<00:01, 42.6MB/s]\u001b[A\n",
      "Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 204M/257M [00:05<00:01, 40.9MB/s]\u001b[A\n",
      "Downloading data:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208M/257M [00:05<00:01, 40.5MB/s]\u001b[A\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 213M/257M [00:05<00:01, 42.8MB/s]\u001b[A\n",
      "Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 217M/257M [00:05<00:00, 41.1MB/s]\u001b[A\n",
      "Downloading data:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 222M/257M [00:06<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 226M/257M [00:06<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 231M/257M [00:06<00:00, 42.1MB/s]\u001b[A\n",
      "Downloading data:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 236M/257M [00:06<00:00, 45.0MB/s]\u001b[A\n",
      "Downloading data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 241M/257M [00:06<00:00, 40.4MB/s]\u001b[A\n",
      "Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 247M/257M [00:06<00:00, 43.1MB/s]\u001b[A\n",
      "Downloading data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 251M/257M [00:06<00:00, 43.1MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 257M/257M [00:06<00:00, 37.9MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                     | 1.22M/257M [00:00<00:20, 12.2MB/s]\u001b[A\n",
      "Downloading data:   3%|â–Œ                    | 7.40M/257M [00:00<00:06, 37.8MB/s]\u001b[A\n",
      "Downloading data:   5%|â–ˆ                    | 13.5M/257M [00:00<00:05, 47.8MB/s]\u001b[A\n",
      "Downloading data:   8%|â–ˆâ–Œ                   | 19.8M/257M [00:00<00:04, 48.0MB/s]\u001b[A\n",
      "Downloading data:  10%|â–ˆâ–ˆ                   | 25.3M/257M [00:00<00:04, 50.2MB/s]\u001b[A\n",
      "Downloading data:  12%|â–ˆâ–ˆâ–Œ                  | 31.2M/257M [00:00<00:04, 52.4MB/s]\u001b[A\n",
      "Downloading data:  16%|â–ˆâ–ˆâ–ˆâ–Ž                 | 40.1M/257M [00:00<00:03, 62.6MB/s]\u001b[A\n",
      "Downloading data:  19%|â–ˆâ–ˆâ–ˆâ–‰                 | 48.4M/257M [00:00<00:03, 68.1MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 55.2M/257M [00:00<00:03, 65.7MB/s]\u001b[A\n",
      "Downloading data:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 64.0M/257M [00:01<00:02, 72.2MB/s]\u001b[A\n",
      "Downloading data:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 71.4M/257M [00:01<00:02, 70.2MB/s]\u001b[A\n",
      "Downloading data:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 78.5M/257M [00:01<00:02, 63.9MB/s]\u001b[A\n",
      "Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 85.2M/257M [00:01<00:02, 64.9MB/s]\u001b[A\n",
      "Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 92.0M/257M [00:01<00:02, 65.3MB/s]\u001b[A\n",
      "Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 101M/257M [00:01<00:02, 73.5MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 109M/257M [00:01<00:02, 70.6MB/s]\u001b[A\n",
      "Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 116M/257M [00:01<00:02, 65.6MB/s]\u001b[A\n",
      "Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 123M/257M [00:01<00:02, 64.8MB/s]\u001b[A\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 130M/257M [00:02<00:01, 64.0MB/s]\u001b[A\n",
      "Downloading data:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 136M/257M [00:02<00:01, 62.6MB/s]\u001b[A\n",
      "Downloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 146M/257M [00:02<00:01, 71.8MB/s]\u001b[A\n",
      "Downloading data:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 154M/257M [00:02<00:01, 75.1MB/s]\u001b[A\n",
      "Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 162M/257M [00:02<00:01, 68.4MB/s]\u001b[A\n",
      "Downloading data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 170M/257M [00:02<00:01, 71.8MB/s]\u001b[A\n",
      "Downloading data:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 177M/257M [00:02<00:01, 69.5MB/s]\u001b[A\n",
      "Downloading data:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 185M/257M [00:02<00:01, 71.0MB/s]\u001b[A\n",
      "Downloading data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 192M/257M [00:02<00:00, 71.6MB/s]\u001b[A\n",
      "Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 199M/257M [00:03<00:00, 70.3MB/s]\u001b[A\n",
      "Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 207M/257M [00:03<00:00, 64.3MB/s]\u001b[A\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 214M/257M [00:03<00:00, 63.5MB/s]\u001b[A\n",
      "Downloading data:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 224M/257M [00:03<00:00, 69.3MB/s]\u001b[A\n",
      "Downloading data:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 233M/257M [00:03<00:00, 75.3MB/s]\u001b[A\n",
      "Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 241M/257M [00:03<00:00, 72.7MB/s]\u001b[A\n",
      "Downloading data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 248M/257M [00:03<00:00, 67.9MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 257M/257M [00:03<00:00, 66.2MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/259M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|â–                    | 2.64M/259M [00:00<00:09, 26.4MB/s]\u001b[A\n",
      "Downloading data:   2%|â–Œ                    | 6.29M/259M [00:00<00:07, 32.3MB/s]\u001b[A\n",
      "Downloading data:   4%|â–‰                    | 11.3M/259M [00:00<00:06, 40.0MB/s]\u001b[A\n",
      "Downloading data:   7%|â–ˆâ–                   | 18.0M/259M [00:00<00:04, 50.3MB/s]\u001b[A\n",
      "Downloading data:   9%|â–ˆâ–‰                   | 24.5M/259M [00:00<00:04, 55.6MB/s]\u001b[A\n",
      "Downloading data:  12%|â–ˆâ–ˆâ–Œ                  | 31.6M/259M [00:00<00:03, 60.9MB/s]\u001b[A\n",
      "Downloading data:  15%|â–ˆâ–ˆâ–ˆ                  | 37.7M/259M [00:00<00:03, 57.4MB/s]\u001b[A\n",
      "Downloading data:  18%|â–ˆâ–ˆâ–ˆâ–‹                 | 45.8M/259M [00:00<00:03, 64.6MB/s]\u001b[A\n",
      "Downloading data:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–                | 55.5M/259M [00:00<00:02, 74.3MB/s]\u001b[A\n",
      "Downloading data:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 63.3M/259M [00:01<00:02, 75.6MB/s]\u001b[A\n",
      "Downloading data:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 70.9M/259M [00:01<00:02, 75.2MB/s]\u001b[A\n",
      "Downloading data:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 78.5M/259M [00:01<00:02, 75.1MB/s]\u001b[A\n",
      "Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 86.1M/259M [00:01<00:02, 75.5MB/s]\u001b[A\n",
      "Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 94.0M/259M [00:01<00:02, 76.5MB/s]\u001b[A\n",
      "Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 102M/259M [00:01<00:02, 74.5MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 109M/259M [00:01<00:02, 69.7MB/s]\u001b[A\n",
      "Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 116M/259M [00:01<00:02, 62.7MB/s]\u001b[A\n",
      "Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 125M/259M [00:01<00:01, 69.0MB/s]\u001b[A\n",
      "Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 133M/259M [00:01<00:01, 72.5MB/s]\u001b[A\n",
      "Downloading data:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 142M/259M [00:02<00:01, 76.6MB/s]\u001b[A\n",
      "Downloading data:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 150M/259M [00:02<00:01, 78.5MB/s]\u001b[A\n",
      "Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 158M/259M [00:02<00:01, 69.6MB/s]\u001b[A\n",
      "Downloading data:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 167M/259M [00:02<00:01, 75.0MB/s]\u001b[A\n",
      "Downloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 177M/259M [00:02<00:01, 81.0MB/s]\u001b[A\n",
      "Downloading data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 185M/259M [00:02<00:00, 77.3MB/s]\u001b[A\n",
      "Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 193M/259M [00:02<00:00, 77.2MB/s]\u001b[A\n",
      "Downloading data:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 201M/259M [00:02<00:00, 70.1MB/s]\u001b[A\n",
      "Downloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 208M/259M [00:03<00:00, 65.0MB/s]\u001b[A\n",
      "Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 215M/259M [00:03<00:00, 67.5MB/s]\u001b[A\n",
      "Downloading data:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 224M/259M [00:03<00:00, 71.9MB/s]\u001b[A\n",
      "Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 231M/259M [00:03<00:00, 72.0MB/s]\u001b[A\n",
      "Downloading data:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 239M/259M [00:03<00:00, 75.4MB/s]\u001b[A\n",
      "Downloading data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 247M/259M [00:03<00:00, 67.1MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 259M/259M [00:03<00:00, 69.2MB/s]\u001b[A\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.14s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 376.14it/s]\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.73it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/v5base-enwiki-4k.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-enwiki-4k.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-enwiki-4k/', '--model.load_model=../model/L6-D4096-E0_1-neox-v5base-init.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-enwiki-4k.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-enwiki-4k/', '--model.load_model=../model/L6-D4096-E0_1-neox-v5base-init.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1956874796\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1956874796\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_102854-jvzbucbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/jvzbucbx\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 64.00it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ddfe836637577ca9_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4d4a43715cf9c5ec_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e272537be34aded3_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-52422a63e6f04b92.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-2eab36f5d1f93a5f.arrow\n",
      "Saving the dataset (1/5 shards):  20%|â–| 16298/81487 [00:00<00:02, 26099.43 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/5 shards):  22%|â–| 18298/81487 [00:00<00:03, 19051.27 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/5 shards):  37%|â–Ž| 30298/81487 [00:01<00:01, 28100.40 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/5 shards):  40%|â–| 32596/81487 [00:01<00:01, 28100.40 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/5 shards):  42%|â–| 34596/81487 [00:01<00:02, 22790.64 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (3/5 shards):  60%|â–Œ| 48893/81487 [00:01<00:01, 30135.23 exam[rank: 6] Global seed set to 1956874796\n",
      "Saving the dataset (3/5 shards):  67%|â–‹| 54893/81487 [00:02<00:00, 27255.32 exam[rank: 3] Global seed set to 1956874796\n",
      "[rank: 7] Global seed set to 1956874796\n",
      "[rank: 4] Global seed set to 1956874796\n",
      "Saving the dataset (3/5 shards):  72%|â–‹| 58893/81487 [00:02<00:00, 29316.97 exam[rank: 5] Global seed set to 1956874796\n",
      "[rank: 2] Global seed set to 1956874796\n",
      "[rank: 1] Global seed set to 1956874796\n",
      "[rank: 0] Global seed set to 1956874796                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 10:29:19,155] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 10:29:43,360] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 10:29:43,768] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 10:29:43,806] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 10:29:43,866] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 10:29:43,964] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 10:29:44,066] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1956874796\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 10:29:44,733] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.268219470977783 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.340310096740723 seconds\n",
      "Time to load fused_adam op: 31.339895248413086 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.34050154685974 seconds\n",
      "Time to load fused_adam op: 31.341583490371704 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.342131853103638 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.3471839427948 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 31.349883556365967 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 15.951488494873047 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 16.024685621261597 seconds\n",
      "Time to load utils op: 16.023884296417236 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 16.02294683456421 seconds\n",
      "Time to load utils op: 16.02187991142273 seconds\n",
      "Time to load utils op: 16.022855043411255 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 16.026861667633057 seconds\n",
      "Time to load utils op: 16.02444887161255 seconds\n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0006580352783203125 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006380081176757812 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005979537963867188 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006489753723144531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006496906280517578 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006411075592041016 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0013070106506347656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009515285491943359 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 1000/10186 [42:08<6:27:07,  2.53s/it, v_num=ucbx, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:11:02<00:00,  2.54s/it, v_num=ucbx, train/loss=3\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|â–Ž                  | 1/52 [00:00<00:30,  1.65it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|â–‹                  | 2/52 [00:01<00:26,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 3/52 [00:01<00:23,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 4/52 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–Š                 | 5/52 [00:02<00:21,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 6/52 [00:02<00:21,  2.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–Œ                | 7/52 [00:03<00:20,  2.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 8/52 [00:03<00:19,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|â–ˆâ–ˆâ–ˆâ–Ž               | 9/52 [00:04<00:19,  2.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–              | 10/52 [00:04<00:18,  2.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–Š              | 11/52 [00:04<00:18,  2.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–             | 12/52 [00:05<00:17,  2.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 13/52 [00:05<00:17,  2.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 14/52 [00:06<00:16,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 15/52 [00:06<00:16,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 16/52 [00:06<00:15,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 17/52 [00:07<00:15,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 18/52 [00:07<00:14,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 19/52 [00:08<00:14,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 20/52 [00:08<00:14,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 21/52 [00:09<00:13,  2.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 22/52 [00:09<00:13,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 23/52 [00:10<00:12,  2.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 24/52 [00:10<00:12,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 25/52 [00:10<00:11,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 26/52 [00:11<00:11,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 27/52 [00:11<00:10,  2.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 28/52 [00:12<00:10,  2.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 29/52 [00:12<00:09,  2.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 30/52 [00:12<00:09,  2.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 31/52 [00:13<00:09,  2.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 32/52 [00:13<00:08,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 33/52 [00:14<00:08,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 34/52 [00:14<00:07,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 35/52 [00:15<00:07,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 36/52 [00:15<00:06,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 37/52 [00:15<00:06,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 38/52 [00:16<00:06,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/52 [00:16<00:05,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 40/52 [00:17<00:05,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/52 [00:17<00:04,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/52 [00:18<00:04,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 43/52 [00:18<00:03,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/52 [00:18<00:03,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/52 [00:19<00:03,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 46/52 [00:19<00:02,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 47/52 [00:20<00:02,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 48/52 [00:20<00:01,  2.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/52 [00:21<00:01,  2.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/52 [00:21<00:00,  2.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 51/52 [00:21<00:00,  2.33it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:11:32<00:00,  2.54s/it, v_num=ucbx, train/loss=3\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:11:32<00:00,  2.54s/it, v_num=ucbx, train/loss=3\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 10186/10186 [7:11:45<00:00,  2.54s/it, v_num=ucbx, train/loss=3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–†â–†â–…â–„â–…â–„â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.52836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/jvzbucbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_102854-jvzbucbx/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-enwiki-4k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-4k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/\" \\\n",
    "        --model.load_model=\"../model/L{LAYER_COUNT}-D{EMBED_DIM}-E{EMBED_SCALE_LABEL}-neox-v5base-init.pth\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-enwiki-4k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-enwiki-4k.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 19 19:22 ../model/v5-L6-D4096-E0_1-enwiki-4k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. He saw how the rice fields provided rice to them, while still other farmers had reached the mountains. He claimed that the great king, however, could not tell them about the mountain's world.\n",
      "\n",
      "In an early 20th-century visit to Beijing in 1805, Andrew Dana, then the ambassador to the National Academy of Sciences of China, stated that the mountain was the best in the world. He was deeply disturbed by the mountain's water, and urged to continue their journey, and eventually sent the army to the center. The army responded to their efforts by promising to raise the mountain's water, and the mountain was destroyed by fire in late 1819.\n",
      "\n",
      "By 1820, the mountain had grown to about 30,000Â km2 (4,000Â sqÂ mi). They were convinced that a road would be built. A commission was set up in 1860 to fund the project. He had plans to purchase a large parcel of land and build a railway, but the\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 0.0% similarity, with 0 matched token, and 5 token mismatch\n",
      "## Model validation for 10 tokens : 10.0% similarity, with 1 matched token, and 9 token mismatch\n",
      "## Model validation for 15 tokens : 0.0% similarity, with 0 matched token, and 15 token mismatch\n",
      "## Model validation for 20 tokens : 5.0% similarity, with 1 matched token, and 19 token mismatch\n",
      "## Model validation for 25 tokens : 4.0% similarity, with 1 matched token, and 24 token mismatch\n",
      "## Model validation for 30 tokens : 3.3333333333333335% similarity, with 1 matched token, and 29 token mismatch\n",
      "## Model validation for 35 tokens : 2.857142857142857% similarity, with 1 matched token, and 34 token mismatch\n",
      "## Model validation for 40 tokens : 0.0% similarity, with 0 matched token, and 40 token mismatch\n",
      "## Model validation for 45 tokens : 0.0% similarity, with 0 matched token, and 45 token mismatch\n",
      "## Model validation for 50 tokens : 0.0% similarity, with 0 matched token, and 50 token mismatch\n",
      "## Model validation for 55 tokens : 0.0% similarity, with 0 matched token, and 55 token mismatch\n",
      "## Model validation for 60 tokens : 0.0% similarity, with 0 matched token, and 60 token mismatch\n",
      "## Model validation for 65 tokens : 0.0% similarity, with 0 matched token, and 65 token mismatch\n",
      "## Model validation for 70 tokens : 0.0% similarity, with 0 matched token, and 70 token mismatch\n",
      "## Model validation for 75 tokens : 0.0% similarity, with 0 matched token, and 75 token mismatch\n",
      "## Model validation for 80 tokens : 0.0% similarity, with 0 matched token, and 80 token mismatch\n",
      "## Model validation for 85 tokens : 0.0% similarity, with 0 matched token, and 85 token mismatch\n",
      "## Model validation for 90 tokens : 1.1111111111111112% similarity, with 1 matched token, and 89 token mismatch\n",
      "## Model validation for 95 tokens : 1.0526315789473684% similarity, with 1 matched token, and 94 token mismatch\n",
      "## Model validation for 100 tokens : 1.0% similarity, with 1 matched token, and 99 token mismatch\n",
      "## Model validation for 105 tokens : 0.9523809523809524% similarity, with 1 matched token, and 104 token mismatch\n",
      "## Model validation for 110 tokens : 0.9090909090909091% similarity, with 1 matched token, and 109 token mismatch\n",
      "## Model validation for 115 tokens : 0.8695652173913043% similarity, with 1 matched token, and 114 token mismatch\n",
      "## Model validation for 120 tokens : 0.8333333333333334% similarity, with 1 matched token, and 119 token mismatch\n",
      "## Model validation for 125 tokens : 0.8% similarity, with 1 matched token, and 124 token mismatch\n",
      "## Model validation for 130 tokens : 0.7692307692307693% similarity, with 1 matched token, and 129 token mismatch\n",
      "## Model validation for 135 tokens : 0.7407407407407408% similarity, with 1 matched token, and 134 token mismatch\n",
      "## Model validation for 140 tokens : 0.7142857142857143% similarity, with 1 matched token, and 139 token mismatch\n",
      "## Model validation for 145 tokens : 1.3793103448275863% similarity, with 2 matched token, and 143 token mismatch\n",
      "## Model validation for 150 tokens : 1.3333333333333335% similarity, with 2 matched token, and 148 token mismatch\n",
      "## Model validation for 160 tokens : 1.25% similarity, with 2 matched token, and 158 token mismatch\n",
      "## Model validation for 170 tokens : 1.1764705882352942% similarity, with 2 matched token, and 168 token mismatch\n",
      "## Model validation for 180 tokens : 1.1111111111111112% similarity, with 2 matched token, and 178 token mismatch\n",
      "## Model validation for 190 tokens : 1.5789473684210527% similarity, with 3 matched token, and 187 token mismatch\n",
      "## Model validation for 200 tokens : 1.5% similarity, with 3 matched token, and 197 token mismatch\n",
      "## Model validation for 210 tokens : 1.4285714285714286% similarity, with 3 matched token, and 207 token mismatch\n",
      "## Model validation for 220 tokens : 1.3636363636363635% similarity, with 3 matched token, and 217 token mismatch\n",
      "## Model validation for 230 tokens : 1.3043478260869565% similarity, with 3 matched token, and 227 token mismatch\n",
      "## Model validation for 240 tokens : 1.25% similarity, with 3 matched token, and 237 token mismatch\n",
      "## Model validation for 250 tokens : 1.2% similarity, with 3 matched token, and 247 token mismatch\n",
      "## Model validation for 260 tokens : 1.153846153846154% similarity, with 3 matched token, and 257 token mismatch\n",
      "## Model validation for 270 tokens : 1.1111111111111112% similarity, with 3 matched token, and 267 token mismatch\n",
      "## Model validation for 280 tokens : 1.0714285714285714% similarity, with 3 matched token, and 277 token mismatch\n",
      "## Model validation for 290 tokens : 1.3793103448275863% similarity, with 4 matched token, and 286 token mismatch\n",
      "## Model validation for 300 tokens : 1.3333333333333335% similarity, with 4 matched token, and 296 token mismatch\n",
      "## Model validation for 325 tokens : 1.2307692307692308% similarity, with 4 matched token, and 321 token mismatch\n",
      "## Model validation for 350 tokens : 1.1428571428571428% similarity, with 4 matched token, and 346 token mismatch\n",
      "## Model validation for 375 tokens : 1.3333333333333335% similarity, with 5 matched token, and 370 token mismatch\n",
      "## Model validation for 400 tokens : 1.25% similarity, with 5 matched token, and 395 token mismatch\n",
      "## Model validation for 425 tokens : 1.1764705882352942% similarity, with 5 matched token, and 420 token mismatch\n",
      "## Model validation for 450 tokens : 1.1111111111111112% similarity, with 5 matched token, and 445 token mismatch\n",
      "## Model validation for 475 tokens : 1.0526315789473684% similarity, with 5 matched token, and 470 token mismatch\n",
      "## Model validation for 500 tokens : 1.2% similarity, with 6 matched token, and 494 token mismatch\n",
      "## Model validation for 525 tokens : 1.1428571428571428% similarity, with 6 matched token, and 519 token mismatch\n",
      "## Model validation for 550 tokens : 1.2727272727272727% similarity, with 7 matched token, and 543 token mismatch\n",
      "## Model validation for 575 tokens : 1.2173913043478262% similarity, with 7 matched token, and 568 token mismatch\n",
      "## Model validation for 600 tokens : 1.1666666666666667% similarity, with 7 matched token, and 593 token mismatch\n",
      "## Model validation for 625 tokens : 1.28% similarity, with 8 matched token, and 617 token mismatch\n",
      "## Model validation for 650 tokens : 1.2307692307692308% similarity, with 8 matched token, and 642 token mismatch\n",
      "## Model validation for 675 tokens : 1.1851851851851851% similarity, with 8 matched token, and 667 token mismatch\n",
      "## Model validation for 700 tokens : 1.1428571428571428% similarity, with 8 matched token, and 692 token mismatch\n",
      "## Model validation for 750 tokens : 1.2% similarity, with 9 matched token, and 741 token mismatch\n",
      "## Model validation for 800 tokens : 1.125% similarity, with 9 matched token, and 791 token mismatch\n",
      "## Model validation for 850 tokens : 1.1764705882352942% similarity, with 10 matched token, and 840 token mismatch\n",
      "## Model validation for 900 tokens : 1.1111111111111112% similarity, with 10 matched token, and 890 token mismatch\n",
      "## Model validation for 950 tokens : 1.1578947368421053% similarity, with 11 matched token, and 939 token mismatch\n",
      "## Model validation for 1000 tokens : 1.2% similarity, with 12 matched token, and 988 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enwiki Stage 2 : Basic Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 652.40it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3a81f68e4498c60a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-36c9ee56cc63a264_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4f2b89ca1acce20d.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c7f7bd164be56255.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/v5base-enwiki-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-enwiki-instruct.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-enwiki-instruct/', '--model.load_model=../model/v5-L6-D4096-E0_1-enwiki-4k.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-enwiki-instruct.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-enwiki-instruct/', '--model.load_model=../model/v5-L6-D4096-E0_1-enwiki-4k.pth', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3645234942\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3645234942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_192513-5apzk49a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/5apzk49a\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 602.63it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3a81f68e4498c60a_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-36c9ee56cc63a264_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4f2b89ca1acce20d.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c7f7bd164be56255.arrow\n",
      "[rank: 0] Global seed set to 3645234942                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 19:25:33,038] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3645234942\n",
      "[rank: 2] Global seed set to 3645234942\n",
      "[rank: 4] Global seed set to 3645234942\n",
      "[rank: 6] Global seed set to 3645234942\n",
      "[rank: 3] Global seed set to 3645234942\n",
      "[rank: 5] Global seed set to 3645234942\n",
      "[rank: 7] Global seed set to 3645234942\n",
      "[rank: 1] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 19:26:01,587] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 19:26:02,197] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 19:26:02,343] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 19:26:02,551] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 19:26:02,752] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 19:26:02,951] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3645234942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 19:26:03,242] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0703115463256836 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10135436058044434 seconds\n",
      "Time to load fused_adam op: 0.1011197566986084 seconds\n",
      "Time to load fused_adam op: 0.1013040542602539 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10140800476074219 seconds\n",
      "Time to load fused_adam op: 0.10146188735961914 seconds\n",
      "Time to load fused_adam op: 0.10166192054748535 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10185718536376953 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06998610496520996 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10254073143005371 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10235095024108887 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10284781455993652 seconds\n",
      "Time to load utils op: 0.10269474983215332 seconds\n",
      "Time to load utils op: 0.1023252010345459 seconds\n",
      "Time to load utils op: 0.10222864151000977 seconds\n",
      "Time to load utils op: 0.1024026870727539 seconds\n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006093978881835938 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0006122589111328125 seconds\n",
      "Time to load utils op: 0.0006661415100097656 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006589889526367188 seconds\n",
      "Time to load utils op: 0.0006239414215087891 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00067138671875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0011534690856933594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009024143218994141 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 738, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 736, in rmtree\n",
      "    os.rmdir(path, dir_fd=dir_fd)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-_82xusl_'\n",
      "Epoch 0:  54%|â–Œ| 1000/1867 [25:21<21:59,  1.52s/it, v_num=k49a, train/loss=3.170/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [47:03<00:00,  1.51s/it, v_num=k49a, train/loss=4.560\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 1/10 [00:00<00:03,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–Š               | 2/10 [00:00<00:02,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 3/10 [00:00<00:01,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 4/10 [00:00<00:01,  4.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 5/10 [00:01<00:01,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 6/10 [00:01<00:00,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7/10 [00:01<00:00,  4.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/10 [00:01<00:00,  4.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9/10 [00:02<00:00,  4.44it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [47:14<00:00,  1.52s/it, v_num=k49a, train/loss=4.560\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [47:14<00:00,  1.52s/it, v_num=k49a, train/loss=4.560\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 1867/1867 [47:28<00:00,  1.53s/it, v_num=k49a, train/loss=4.560\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–‚â–‚â–…â–‚â–†â–â–‚â–„â–‚â–ƒâ–‚â–‚â–â–â–â–„â–â–â–‚â–â–‚â–â–ƒâ–‚â–ƒâ–„â–ƒâ–‡â–ƒâ–â–„â–‚â–‚â–ˆâ–„â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–†â–…â–„â–ˆâ–…â–‚â–‚â–†â–„â–†â–‡â–†â–„â–†â–ƒâ–ˆâ–ƒâ–‚â–†â–„â–„â–„â–„â–‡â–†â–„â–„â–…â–„â–ƒâ–…â–…â–…â–â–ƒâ–…â–…â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.08389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Enwiki-Instruct (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/5apzk49a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_192513-5apzk49a/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-enwiki-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-Instruct (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-instruct/\" \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-enwiki-4k.pth\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-enwiki-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-enwiki-instruct.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 19 20:14 ../model/v5-L6-D4096-E0_1-enwiki-instruct.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}-enwiki-instruct/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. One particular interesting aspect was that the artifacts and trade show world cultures were interdependent and dependent on the time of whom they were enjoying.\n",
      "\n",
      "A number of reasons is that the dragons first appeared on their own and described their significance.\n",
      "\n",
      "A trip to the forest, along with a number of rare artifacts and flower breeds, including the Chinese on planet Earth, was given to the Chinese and Japanese scholars.# Answer~.nThe Chinese's trail was a Chinese feature that made the Chinese a Chinese thinker.\n",
      "\n",
      "A glass panel was one of the best ways to get to the Japanese since it is widely believed that Chinese dragons might have a large library of written Chinese characters.\n",
      "\n",
      "A telescope of a Chinese's stone tablet contains a well known Chinese dragon named Hokkien.\n",
      "\n",
      "One Hundred Years Before the Mianying a map of the Chinese kingdom of Sumatra.\n",
      "\n",
      "A glass of A Chinese Chinese book, it's hard to find someone who has had a collection\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 40.0% similarity, with 2 matched token, and 3 token mismatch\n",
      "## Model validation for 10 tokens : 20.0% similarity, with 2 matched token, and 8 token mismatch\n",
      "## Model validation for 15 tokens : 13.333333333333334% similarity, with 2 matched token, and 13 token mismatch\n",
      "## Model validation for 20 tokens : 10.0% similarity, with 2 matched token, and 18 token mismatch\n",
      "## Model validation for 25 tokens : 4.0% similarity, with 1 matched token, and 24 token mismatch\n",
      "## Model validation for 30 tokens : 3.3333333333333335% similarity, with 1 matched token, and 29 token mismatch\n",
      "## Model validation for 35 tokens : 2.857142857142857% similarity, with 1 matched token, and 34 token mismatch\n",
      "## Model validation for 40 tokens : 2.5% similarity, with 1 matched token, and 39 token mismatch\n",
      "## Model validation for 45 tokens : 2.2222222222222223% similarity, with 1 matched token, and 44 token mismatch\n",
      "## Model validation for 50 tokens : 2.0% similarity, with 1 matched token, and 49 token mismatch\n",
      "## Model validation for 55 tokens : 1.8181818181818181% similarity, with 1 matched token, and 54 token mismatch\n",
      "## Model validation for 60 tokens : 1.6666666666666667% similarity, with 1 matched token, and 59 token mismatch\n",
      "## Model validation for 65 tokens : 1.5384615384615385% similarity, with 1 matched token, and 64 token mismatch\n",
      "## Model validation for 70 tokens : 1.4285714285714286% similarity, with 1 matched token, and 69 token mismatch\n",
      "## Model validation for 75 tokens : 1.3333333333333335% similarity, with 1 matched token, and 74 token mismatch\n",
      "## Model validation for 80 tokens : 1.25% similarity, with 1 matched token, and 79 token mismatch\n",
      "## Model validation for 85 tokens : 1.1764705882352942% similarity, with 1 matched token, and 84 token mismatch\n",
      "## Model validation for 90 tokens : 2.2222222222222223% similarity, with 2 matched token, and 88 token mismatch\n",
      "## Model validation for 95 tokens : 2.1052631578947367% similarity, with 2 matched token, and 93 token mismatch\n",
      "## Model validation for 100 tokens : 1.0% similarity, with 1 matched token, and 99 token mismatch\n",
      "## Model validation for 105 tokens : 0.9523809523809524% similarity, with 1 matched token, and 104 token mismatch\n",
      "## Model validation for 110 tokens : 0.9090909090909091% similarity, with 1 matched token, and 109 token mismatch\n",
      "## Model validation for 115 tokens : 0.8695652173913043% similarity, with 1 matched token, and 114 token mismatch\n",
      "## Model validation for 120 tokens : 0.8333333333333334% similarity, with 1 matched token, and 119 token mismatch\n",
      "## Model validation for 125 tokens : 0.8% similarity, with 1 matched token, and 124 token mismatch\n",
      "## Model validation for 130 tokens : 0.7692307692307693% similarity, with 1 matched token, and 129 token mismatch\n",
      "## Model validation for 135 tokens : 0.7407407407407408% similarity, with 1 matched token, and 134 token mismatch\n",
      "## Model validation for 140 tokens : 0.7142857142857143% similarity, with 1 matched token, and 139 token mismatch\n",
      "## Model validation for 145 tokens : 1.3793103448275863% similarity, with 2 matched token, and 143 token mismatch\n",
      "## Model validation for 150 tokens : 1.3333333333333335% similarity, with 2 matched token, and 148 token mismatch\n",
      "## Model validation for 160 tokens : 1.25% similarity, with 2 matched token, and 158 token mismatch\n",
      "## Model validation for 170 tokens : 1.1764705882352942% similarity, with 2 matched token, and 168 token mismatch\n",
      "## Model validation for 180 tokens : 1.1111111111111112% similarity, with 2 matched token, and 178 token mismatch\n",
      "## Model validation for 190 tokens : 1.5789473684210527% similarity, with 3 matched token, and 187 token mismatch\n",
      "## Model validation for 200 tokens : 1.5% similarity, with 3 matched token, and 197 token mismatch\n",
      "## Model validation for 210 tokens : 1.4285714285714286% similarity, with 3 matched token, and 207 token mismatch\n",
      "## Model validation for 220 tokens : 1.3636363636363635% similarity, with 3 matched token, and 217 token mismatch\n",
      "## Model validation for 230 tokens : 1.3043478260869565% similarity, with 3 matched token, and 227 token mismatch\n",
      "## Model validation for 240 tokens : 1.25% similarity, with 3 matched token, and 237 token mismatch\n",
      "## Model validation for 250 tokens : 0.8% similarity, with 2 matched token, and 248 token mismatch\n",
      "## Model validation for 260 tokens : 1.153846153846154% similarity, with 3 matched token, and 257 token mismatch\n",
      "## Model validation for 270 tokens : 1.1111111111111112% similarity, with 3 matched token, and 267 token mismatch\n",
      "## Model validation for 280 tokens : 1.0714285714285714% similarity, with 3 matched token, and 277 token mismatch\n",
      "## Model validation for 290 tokens : 1.0344827586206897% similarity, with 3 matched token, and 287 token mismatch\n",
      "## Model validation for 300 tokens : 1.0% similarity, with 3 matched token, and 297 token mismatch\n",
      "## Model validation for 325 tokens : 0.9230769230769231% similarity, with 3 matched token, and 322 token mismatch\n",
      "## Model validation for 350 tokens : 0.8571428571428572% similarity, with 3 matched token, and 347 token mismatch\n",
      "## Model validation for 375 tokens : 1.0666666666666667% similarity, with 4 matched token, and 371 token mismatch\n",
      "## Model validation for 400 tokens : 1.0% similarity, with 4 matched token, and 396 token mismatch\n",
      "## Model validation for 425 tokens : 0.9411764705882352% similarity, with 4 matched token, and 421 token mismatch\n",
      "## Model validation for 450 tokens : 0.8888888888888888% similarity, with 4 matched token, and 446 token mismatch\n",
      "## Model validation for 475 tokens : 0.8421052631578947% similarity, with 4 matched token, and 471 token mismatch\n",
      "## Model validation for 500 tokens : 1.0% similarity, with 5 matched token, and 495 token mismatch\n",
      "## Model validation for 525 tokens : 0.9523809523809524% similarity, with 5 matched token, and 520 token mismatch\n",
      "## Model validation for 550 tokens : 1.090909090909091% similarity, with 6 matched token, and 544 token mismatch\n",
      "## Model validation for 575 tokens : 1.0434782608695654% similarity, with 6 matched token, and 569 token mismatch\n",
      "## Model validation for 600 tokens : 1.0% similarity, with 6 matched token, and 594 token mismatch\n",
      "## Model validation for 625 tokens : 1.1199999999999999% similarity, with 7 matched token, and 618 token mismatch\n",
      "## Model validation for 650 tokens : 1.0769230769230769% similarity, with 7 matched token, and 643 token mismatch\n",
      "## Model validation for 675 tokens : 1.037037037037037% similarity, with 7 matched token, and 668 token mismatch\n",
      "## Model validation for 700 tokens : 1.0% similarity, with 7 matched token, and 693 token mismatch\n",
      "## Model validation for 750 tokens : 1.0666666666666667% similarity, with 8 matched token, and 742 token mismatch\n",
      "## Model validation for 800 tokens : 1.0% similarity, with 8 matched token, and 792 token mismatch\n",
      "## Model validation for 850 tokens : 1.0588235294117647% similarity, with 9 matched token, and 841 token mismatch\n",
      "## Model validation for 900 tokens : 1.0% similarity, with 9 matched token, and 891 token mismatch\n",
      "## Model validation for 950 tokens : 1.0526315789473684% similarity, with 10 matched token, and 940 token mismatch\n",
      "## Model validation for 1000 tokens : 1.0999999999999999% similarity, with 11 matched token, and 989 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-instruct.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ../dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ../dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ../dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ../dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ../dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ../dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ../dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x  2 root root 4.0K Aug 19 20:17 .\n",
      "drwxr-xr-x 10 root root  205 Aug 19 17:44 ..\n",
      "-rw-r--r--  1 root root 612K Aug 19 20:17 word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 2.8M Aug 19 20:17 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 729K Aug 19 20:17 word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 839K Aug 19 20:17 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root 855K Aug 19 20:17 word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 5.2M Aug 19 20:17 word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 967K Aug 19 20:17 word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug 19 20:17 word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 968K Aug 19 20:17 word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 1.6M Aug 19 20:17 word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug 19 20:17 word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 2.3M Aug 19 20:17 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-5-count.jsonl  5  5000 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-10-count.jsonl 10 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-15-count.jsonl 15 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-20-count.jsonl 20 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-25-count.jsonl 25 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-40-count.jsonl 40 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-50-count.jsonl 50 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-60-count.jsonl 80 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-100-count.jsonl 100 2500 &\n",
    "python ../memory_script/gen_limited_segmented_jsonl.py ../dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-instruct.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-instruct/', '--model.load_model=../model/v5-L6-D4096-E0_1-enwiki-instruct.pth', '--model.ctx_len=512', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-instruct.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-instruct/', '--model.load_model=../model/v5-L6-D4096-E0_1-enwiki-instruct.pth', '--model.ctx_len=512', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1659784300\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1659784300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_201727-mbh8xq0f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/mbh8xq0f\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-9b5039f372137c54/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3591.01it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 237.84it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-9b5039f372137c54/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 202.35it/s]\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 1659784300\n",
      "[rank: 1] Global seed set to 1659784300\n",
      "[rank: 6] Global seed set to 1659784300\n",
      "[rank: 4] Global seed set to 1659784300\n",
      "[rank: 5] Global seed set to 1659784300\n",
      "[rank: 7] Global seed set to 1659784300\n",
      "[rank: 3] Global seed set to 1659784300\n",
      "Filter (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35000/35000 [00:04<00:00, 12997.94 examples/s][rank: 5] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 20:18:17,619] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1659784300                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 20:18:18,507] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Saving the dataset (0/1 shards):   0%|         | 0/34965 [00:00<?, ? examples/s][rank: 4] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 20:18:18,844] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Saving the dataset (0/1 shards):  54%|â–Œ| 19000/34965 [00:00<00:00, 169562.38 exa[rank: 6] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 20:18:18,949] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 1659784300                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 20:18:19,064] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 20:18:19,114] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 20:18:19,121] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1659784300\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 20:18:19,415] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06554269790649414 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10141801834106445 seconds\n",
      "Time to load fused_adam op: 0.10131645202636719 seconds\n",
      "Time to load fused_adam op: 0.1013803482055664 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10141706466674805 seconds\n",
      "Time to load fused_adam op: 0.10136651992797852 seconds\n",
      "Time to load fused_adam op: 0.10169482231140137 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10146331787109375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07106184959411621 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10225272178649902 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1028146743774414 seconds\n",
      "Time to load utils op: 0.10271382331848145 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10313820838928223 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10239291191101074 seconds\n",
      "Time to load utils op: 0.10218000411987305 seconds\n",
      "Time to load utils op: 0.10215282440185547 seconds\n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006184577941894531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0006518363952636719 seconds\n",
      "Time to load utils op: 0.0007207393646240234 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006859302520751953 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006887912750244141 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007905960083007812 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0014944076538085938 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008413791656494141 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 738, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 736, in rmtree\n",
      "    os.rmdir(path, dir_fd=dir_fd)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-at1y3pru'\n",
      "Epoch 0:  18%|â–| 800/4371 [03:38<16:16,  3.66it/s, v_num=xq0f, train/loss=0.691]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [19:53<00:00,  3.66it/s, v_num=xq0f, train/loss=0.122\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|â–ˆâ–ˆâ–ˆâ–ˆ                | 1/5 [00:00<00:01,  2.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 2/5 [00:00<00:00,  3.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 3/5 [00:00<00:00,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4/5 [00:01<00:00,  3.61it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:02<00:00,  3.63it/s, v_num=xq0f, train/loss=0.122\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:02<00:00,  3.63it/s, v_num=xq0f, train/loss=0.122\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 4371/4371 [20:16<00:00,  3.59it/s, v_num=xq0f, train/loss=0.122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–„â–â–ˆâ–‚â–ˆâ–ƒâ–â–â–â–‚â–â–ƒâ–ƒâ–â–‚â–â–â–ƒâ–â–â–â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–„â–†â–ƒâ–†â–„â–‚â–â–‚â–ƒâ–â–…â–…â–â–‚â–†â–ƒâ–„â–â–ƒâ–…â–â–â–‚â–…â–â–â–„â–‚â–â–â–‚â–â–â–â–â–â–â–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 110\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.25781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.53979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Instruct (train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/mbh8xq0f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_201727-mbh8xq0f/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Instruct (train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-instruct/\" \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-enwiki-instruct.pth\" \\\n",
    "        --model.ctx_len=512 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-instruct.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 19 20:40 ../model/v5-L6-D4096-E0_1-mem-instruct.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-instruct/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-instruct.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 96.0% similarity, with 24 matched token, and 1 token mismatch\n",
      "## Model validation for 30 tokens : 96.66666666666667% similarity, with 29 matched token, and 1 token mismatch\n",
      "## Model validation for 35 tokens : 97.14285714285714% similarity, with 34 matched token, and 1 token mismatch\n",
      "## Model validation for 40 tokens : 95.0% similarity, with 38 matched token, and 2 token mismatch\n",
      "## Model validation for 45 tokens : 95.55555555555556% similarity, with 43 matched token, and 2 token mismatch\n",
      "## Model validation for 50 tokens : 94.0% similarity, with 47 matched token, and 3 token mismatch\n",
      "## Model validation for 55 tokens : 90.9090909090909% similarity, with 50 matched token, and 5 token mismatch\n",
      "## Model validation for 60 tokens : 86.66666666666667% similarity, with 52 matched token, and 8 token mismatch\n",
      "## Model validation for 65 tokens : 86.15384615384616% similarity, with 56 matched token, and 9 token mismatch\n",
      "## Model validation for 70 tokens : 81.42857142857143% similarity, with 57 matched token, and 13 token mismatch\n",
      "## Model validation for 75 tokens : 78.66666666666666% similarity, with 59 matched token, and 16 token mismatch\n",
      "## Model validation for 80 tokens : 72.5% similarity, with 58 matched token, and 22 token mismatch\n",
      "## Model validation for 85 tokens : 71.76470588235294% similarity, with 61 matched token, and 24 token mismatch\n",
      "## Model validation for 90 tokens : 67.77777777777779% similarity, with 61 matched token, and 29 token mismatch\n",
      "## Model validation for 95 tokens : 66.3157894736842% similarity, with 63 matched token, and 32 token mismatch\n",
      "## Model validation for 100 tokens : 63.0% similarity, with 63 matched token, and 37 token mismatch\n",
      "## Model validation for 105 tokens : 63.8095238095238% similarity, with 67 matched token, and 38 token mismatch\n",
      "## Model validation for 110 tokens : 57.27272727272727% similarity, with 63 matched token, and 47 token mismatch\n",
      "## Model validation for 115 tokens : 53.04347826086957% similarity, with 61 matched token, and 54 token mismatch\n",
      "## Model validation for 120 tokens : 54.166666666666664% similarity, with 65 matched token, and 55 token mismatch\n",
      "## Model validation for 125 tokens : 51.2% similarity, with 64 matched token, and 61 token mismatch\n",
      "## Model validation for 130 tokens : 48.46153846153846% similarity, with 63 matched token, and 67 token mismatch\n",
      "## Model validation for 135 tokens : 43.7037037037037% similarity, with 59 matched token, and 76 token mismatch\n",
      "## Model validation for 140 tokens : 44.285714285714285% similarity, with 62 matched token, and 78 token mismatch\n",
      "## Model validation for 145 tokens : 45.51724137931035% similarity, with 66 matched token, and 79 token mismatch\n",
      "## Model validation for 150 tokens : 46.0% similarity, with 69 matched token, and 81 token mismatch\n",
      "## Model validation for 160 tokens : 43.75% similarity, with 70 matched token, and 90 token mismatch\n",
      "## Model validation for 170 tokens : 35.294117647058826% similarity, with 60 matched token, and 110 token mismatch\n",
      "## Model validation for 180 tokens : 32.77777777777778% similarity, with 59 matched token, and 121 token mismatch\n",
      "## Model validation for 190 tokens : 29.47368421052631% similarity, with 56 matched token, and 134 token mismatch\n",
      "## Model validation for 200 tokens : 26.5% similarity, with 53 matched token, and 147 token mismatch\n",
      "## Model validation for 210 tokens : 23.809523809523807% similarity, with 50 matched token, and 160 token mismatch\n",
      "## Model validation for 220 tokens : 22.272727272727273% similarity, with 49 matched token, and 171 token mismatch\n",
      "## Model validation for 230 tokens : 20.434782608695652% similarity, with 47 matched token, and 183 token mismatch\n",
      "## Model validation for 240 tokens : 18.333333333333332% similarity, with 44 matched token, and 196 token mismatch\n",
      "## Model validation for 250 tokens : 16.0% similarity, with 40 matched token, and 210 token mismatch\n",
      "## Model validation for 260 tokens : 14.615384615384617% similarity, with 38 matched token, and 222 token mismatch\n",
      "## Model validation for 270 tokens : 13.333333333333334% similarity, with 36 matched token, and 234 token mismatch\n",
      "## Model validation for 280 tokens : 11.785714285714285% similarity, with 33 matched token, and 247 token mismatch\n",
      "## Model validation for 290 tokens : 12.068965517241379% similarity, with 35 matched token, and 255 token mismatch\n",
      "## Model validation for 300 tokens : 10.0% similarity, with 30 matched token, and 270 token mismatch\n",
      "## Model validation for 325 tokens : 9.230769230769232% similarity, with 30 matched token, and 295 token mismatch\n",
      "## Model validation for 350 tokens : 8.571428571428571% similarity, with 30 matched token, and 320 token mismatch\n",
      "## Model validation for 375 tokens : 8.266666666666666% similarity, with 31 matched token, and 344 token mismatch\n",
      "## Model validation for 400 tokens : 8.25% similarity, with 33 matched token, and 367 token mismatch\n",
      "## Model validation for 425 tokens : 7.0588235294117645% similarity, with 30 matched token, and 395 token mismatch\n",
      "## Model validation for 450 tokens : 6.222222222222222% similarity, with 28 matched token, and 422 token mismatch\n",
      "## Model validation for 475 tokens : 5.894736842105263% similarity, with 28 matched token, and 447 token mismatch\n",
      "## Model validation for 500 tokens : 6.6000000000000005% similarity, with 33 matched token, and 467 token mismatch\n",
      "## Model validation for 525 tokens : 5.714285714285714% similarity, with 30 matched token, and 495 token mismatch\n",
      "## Model validation for 550 tokens : 5.2727272727272725% similarity, with 29 matched token, and 521 token mismatch\n",
      "## Model validation for 575 tokens : 5.217391304347826% similarity, with 30 matched token, and 545 token mismatch\n",
      "## Model validation for 600 tokens : 4.5% similarity, with 27 matched token, and 573 token mismatch\n",
      "## Model validation for 625 tokens : 4.8% similarity, with 30 matched token, and 595 token mismatch\n",
      "## Model validation for 650 tokens : 4.153846153846154% similarity, with 27 matched token, and 623 token mismatch\n",
      "## Model validation for 675 tokens : 3.5555555555555554% similarity, with 24 matched token, and 651 token mismatch\n",
      "## Model validation for 700 tokens : 3.0% similarity, with 21 matched token, and 679 token mismatch\n",
      "## Model validation for 750 tokens : 3.4666666666666663% similarity, with 26 matched token, and 724 token mismatch\n",
      "## Model validation for 800 tokens : 3.0% similarity, with 24 matched token, and 776 token mismatch\n",
      "## Model validation for 850 tokens : 2.823529411764706% similarity, with 24 matched token, and 826 token mismatch\n",
      "## Model validation for 900 tokens : 2.7777777777777777% similarity, with 25 matched token, and 875 token mismatch\n",
      "## Model validation for 950 tokens : 2.4210526315789473% similarity, with 23 matched token, and 927 token mismatch\n",
      "## Model validation for 1000 tokens : 2.1999999999999997% similarity, with 22 matched token, and 978 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 3542 samples (20 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 667 samples (50 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 3157 samples (30 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 5201 samples (20 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 1771 samples (50 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 1341 samples (50 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 2639 samples (50 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ../dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ../dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 79M\n",
      "drwxr-xr-x  2 root root 4.0K Aug 19 20:41 .\n",
      "drwxr-xr-x 10 root root  205 Aug 19 17:44 ..\n",
      "-rw-r--r--  1 root root 986K Aug 19 20:41 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 1.2M Aug 19 20:41 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Aug 19 20:41 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root 1.7M Aug 19 20:41 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.9M Aug 19 20:41 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root 2.2M Aug 19 20:41 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root 2.4M Aug 19 20:41 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root 2.7M Aug 19 20:41 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root 727K Aug 19 20:41 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root 2.9M Aug 19 20:41 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 3.1M Aug 19 20:41 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root 3.4M Aug 19 20:41 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root 3.6M Aug 19 20:41 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root 3.8M Aug 19 20:41 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root 4.1M Aug 19 20:41 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 4.3M Aug 19 20:41 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root 4.5M Aug 19 20:41 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root 4.8M Aug 19 20:41 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root 5.0M Aug 19 20:41 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root 1.1M Aug 19 20:41 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug 19 20:41 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 852K Aug 19 20:41 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root 1.4M Aug 19 20:41 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root 1.1M Aug 19 20:41 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Aug 19 20:41 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root 1.5M Aug 19 20:41 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root 5.3M Aug 19 20:41 word-100-count.jsonl\n",
      "-rw-r--r--  1 root root 597K Aug 19 20:41 word-2-count.jsonl\n",
      "-rw-r--r--  1 root root  10M Aug 19 20:41 word-200-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-100-count.jsonl 100 5000 &\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-512/', '--model.lr_init=5e-4', '--model.lr_final=4e-4', '--data.max_token_size=512', '--model.ctx_len=512', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-instruct.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-512/', '--model.lr_init=5e-4', '--model.lr_final=4e-4', '--data.max_token_size=512', '--model.ctx_len=512', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-instruct.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2285007610\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2285007610\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_204108-qldx32l2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/qldx32l2\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 33703.19it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3b3a4d5768c9ac01/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2244.14it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 111.82it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 2285007610\n",
      "[rank: 5] Global seed set to 2285007610\n",
      "[rank: 1] Global seed set to 2285007610\n",
      "[rank: 7] Global seed set to 2285007610\n",
      "[rank: 4] Global seed set to 2285007610\n",
      "[rank: 6] Global seed set to 2285007610\n",
      "[rank: 2] Global seed set to 2285007610\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3b3a4d5768c9ac01/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 137.56it/s]\n",
      "[rank: 4] Global seed set to 2285007610                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 20:42:01,015] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 20:42:01,137] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 20:42:01,193] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 20:42:01,254] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 20:42:01,299] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 20:42:01,430] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2285007610\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 20:42:01,555] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 2285007610                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 20:42:17,956] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06847238540649414 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10142970085144043 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10158395767211914 seconds\n",
      "Time to load fused_adam op: 0.10130167007446289 seconds\n",
      "Time to load fused_adam op: 0.10151362419128418 seconds\n",
      "Time to load fused_adam op: 0.10155320167541504 seconds\n",
      "Time to load fused_adam op: 0.10146546363830566 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10146331787109375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06998252868652344 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10244560241699219 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10223555564880371 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10235452651977539 seconds\n",
      "Time to load utils op: 0.10176825523376465 seconds\n",
      "Time to load utils op: 0.10220217704772949 seconds\n",
      "Time to load utils op: 0.10304570198059082 seconds\n",
      "Time to load utils op: 0.10254454612731934 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0007815361022949219 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0008063316345214844 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007264614105224609 seconds\n",
      "Time to load utils op: 0.000659942626953125 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0006926059722900391 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0012049674987792969 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0012981891632080078 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008547306060791016 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/16024 [03:34<1:08:06,  3.73it/s, v_num=32l2, train/loss=0.0/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 16024/16024 [1:11:01<00:00,  3.76it/s, v_num=32l2, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆ                  | 1/17 [00:00<00:06,  2.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 2/17 [00:00<00:04,  3.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–Ž               | 3/17 [00:00<00:04,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 4/17 [00:01<00:03,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 5/17 [00:01<00:03,  3.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 6/17 [00:01<00:02,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 7/17 [00:01<00:02,  4.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 8/17 [00:01<00:02,  4.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 9/17 [00:02<00:01,  4.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 10/17 [00:02<00:01,  4.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 11/17 [00:02<00:01,  4.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 12/17 [00:02<00:01,  4.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13/17 [00:03<00:00,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 14/17 [00:03<00:00,  4.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/17 [00:03<00:00,  4.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/17 [00:03<00:00,  4.18it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16024/16024 [1:11:15<00:00,  3.75it/s, v_num=32l2, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 16024/16024 [1:11:15<00:00,  3.75it/s, v_num=32l2, train/loss=0`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 16024/16024 [1:11:15<00:00,  3.75it/s, v_num=32l2, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–‡â–â–ƒâ–‚â–â–‚â–ƒâ–ˆâ–‚â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–ƒâ–„â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.0094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-512 (train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/qldx32l2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_204108-qldx32l2/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-512 (train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-512/\" \\\n",
    "        --model.lr_init=5e-4 \\\n",
    "        --model.lr_final=4e-4 \\\n",
    "        --data.max_token_size=512 \\\n",
    "        --model.ctx_len=512 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-512/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-512.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 19 21:55 ../model/v5-L6-D4096-E0_1-mem-ctx-512.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-512/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 98.66666666666667% similarity, with 74 matched token, and 1 token mismatch\n",
      "## Model validation for 80 tokens : 97.5% similarity, with 78 matched token, and 2 token mismatch\n",
      "## Model validation for 85 tokens : 97.6470588235294% similarity, with 83 matched token, and 2 token mismatch\n",
      "## Model validation for 90 tokens : 96.66666666666667% similarity, with 87 matched token, and 3 token mismatch\n",
      "## Model validation for 95 tokens : 97.89473684210527% similarity, with 93 matched token, and 2 token mismatch\n",
      "## Model validation for 100 tokens : 98.0% similarity, with 98 matched token, and 2 token mismatch\n",
      "## Model validation for 105 tokens : 98.09523809523809% similarity, with 103 matched token, and 2 token mismatch\n",
      "## Model validation for 110 tokens : 98.18181818181819% similarity, with 108 matched token, and 2 token mismatch\n",
      "## Model validation for 115 tokens : 98.26086956521739% similarity, with 113 matched token, and 2 token mismatch\n",
      "## Model validation for 120 tokens : 98.33333333333333% similarity, with 118 matched token, and 2 token mismatch\n",
      "## Model validation for 125 tokens : 99.2% similarity, with 124 matched token, and 1 token mismatch\n",
      "## Model validation for 130 tokens : 97.6923076923077% similarity, with 127 matched token, and 3 token mismatch\n",
      "## Model validation for 135 tokens : 98.51851851851852% similarity, with 133 matched token, and 2 token mismatch\n",
      "## Model validation for 140 tokens : 97.14285714285714% similarity, with 136 matched token, and 4 token mismatch\n",
      "## Model validation for 145 tokens : 97.93103448275862% similarity, with 142 matched token, and 3 token mismatch\n",
      "## Model validation for 150 tokens : 97.33333333333334% similarity, with 146 matched token, and 4 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 97.6470588235294% similarity, with 166 matched token, and 4 token mismatch\n",
      "## Model validation for 180 tokens : 97.22222222222221% similarity, with 175 matched token, and 5 token mismatch\n",
      "## Model validation for 190 tokens : 95.78947368421052% similarity, with 182 matched token, and 8 token mismatch\n",
      "## Model validation for 200 tokens : 95.0% similarity, with 190 matched token, and 10 token mismatch\n",
      "## Model validation for 210 tokens : 93.33333333333333% similarity, with 196 matched token, and 14 token mismatch\n",
      "## Model validation for 220 tokens : 91.36363636363637% similarity, with 201 matched token, and 19 token mismatch\n",
      "## Model validation for 230 tokens : 90.8695652173913% similarity, with 209 matched token, and 21 token mismatch\n",
      "## Model validation for 240 tokens : 88.75% similarity, with 213 matched token, and 27 token mismatch\n",
      "## Model validation for 250 tokens : 86.4% similarity, with 216 matched token, and 34 token mismatch\n",
      "## Model validation for 260 tokens : 83.46153846153847% similarity, with 217 matched token, and 43 token mismatch\n",
      "## Model validation for 270 tokens : 78.14814814814814% similarity, with 211 matched token, and 59 token mismatch\n",
      "## Model validation for 280 tokens : 73.92857142857143% similarity, with 207 matched token, and 73 token mismatch\n",
      "## Model validation for 290 tokens : 70.0% similarity, with 203 matched token, and 87 token mismatch\n",
      "## Model validation for 300 tokens : 67.33333333333333% similarity, with 202 matched token, and 98 token mismatch\n",
      "## Model validation for 325 tokens : 60.0% similarity, with 195 matched token, and 130 token mismatch\n",
      "## Model validation for 350 tokens : 54.0% similarity, with 189 matched token, and 161 token mismatch\n",
      "## Model validation for 375 tokens : 49.333333333333336% similarity, with 185 matched token, and 190 token mismatch\n",
      "## Model validation for 400 tokens : 45.25% similarity, with 181 matched token, and 219 token mismatch\n",
      "## Model validation for 425 tokens : 42.35294117647059% similarity, with 180 matched token, and 245 token mismatch\n",
      "## Model validation for 450 tokens : 38.666666666666664% similarity, with 174 matched token, and 276 token mismatch\n",
      "## Model validation for 475 tokens : 37.05263157894737% similarity, with 176 matched token, and 299 token mismatch\n",
      "## Model validation for 500 tokens : 34.0% similarity, with 170 matched token, and 330 token mismatch\n",
      "## Model validation for 525 tokens : 31.61904761904762% similarity, with 166 matched token, and 359 token mismatch\n",
      "## Model validation for 550 tokens : 29.818181818181817% similarity, with 164 matched token, and 386 token mismatch\n",
      "## Model validation for 575 tokens : 28.347826086956523% similarity, with 163 matched token, and 412 token mismatch\n",
      "## Model validation for 600 tokens : 25.833333333333336% similarity, with 155 matched token, and 445 token mismatch\n",
      "## Model validation for 625 tokens : 23.68% similarity, with 148 matched token, and 477 token mismatch\n",
      "## Model validation for 650 tokens : 22.923076923076923% similarity, with 149 matched token, and 501 token mismatch\n",
      "## Model validation for 675 tokens : 21.777777777777775% similarity, with 147 matched token, and 528 token mismatch\n",
      "## Model validation for 700 tokens : 20.0% similarity, with 140 matched token, and 560 token mismatch\n",
      "## Model validation for 750 tokens : 16.666666666666664% similarity, with 125 matched token, and 625 token mismatch\n",
      "## Model validation for 800 tokens : 13.25% similarity, with 106 matched token, and 694 token mismatch\n",
      "## Model validation for 850 tokens : 11.294117647058824% similarity, with 96 matched token, and 754 token mismatch\n",
      "## Model validation for 900 tokens : 9.555555555555555% similarity, with 86 matched token, and 814 token mismatch\n",
      "## Model validation for 950 tokens : 7.894736842105263% similarity, with 75 matched token, and 875 token mismatch\n",
      "## Model validation for 1000 tokens : 6.2% similarity, with 62 matched token, and 938 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Low ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Low ctx size (1024), Scaling up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 15 max words, 400 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 400 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 400 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 400 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 400 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 400 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 587 samples (10 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 400 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 400 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 400 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 1056 samples (10 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 870 samples (10 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 2613 samples (10 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 958 samples (20 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 758 samples (10 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 1305 samples (10 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 1796 samples (10 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 400 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 1048 samples (20 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 800 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 800 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 800 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 878 samples (20 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 661 samples (10 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 820 samples (20 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 800 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 760 samples (20 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 800 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated a single JSONL file with 714 samples (20 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 5628 samples (10 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 800 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 556 samples (20 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 347 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 800 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 324 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 196 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 219 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 301 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 634 samples (20 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 800 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 146 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 412 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 369 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated a single JSONL file with 336 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 382 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 800 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 284 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 297 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 800 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 800 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 311 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 800 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 194 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 800 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 800 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 524 samples (20 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 800 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 800 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 359 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 272 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 800 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 213 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 273 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 800 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 655 samples (20 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 113 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 800 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 800 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 800 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 800 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 800 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 269 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 800 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 800 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 800 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 800 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 800 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 800 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 290 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 800 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 800 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 800 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 800 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 205 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 396 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 800 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 800 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 800 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated a single JSONL file with 278 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 800 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 800 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 800 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 800 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 800 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 800 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 317 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 800 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 800 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 800 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 800 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 800 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated a single JSONL file with 591 samples (20 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 800 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 800 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 800 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 800 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 800 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 800 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 800 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 800 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 800 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 800 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 800 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 800 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 800 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 800 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 800 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 800 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 800 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 800 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 800 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 800 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 800 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 800 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 800 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 800 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 800 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 800 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 800 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 800 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 800 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 800 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 800 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 800 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 800 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 800 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 800 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 800 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 800 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 800 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 800 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 800 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 800 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 800 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 800 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 800 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 800 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 800 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 800 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 800 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 800 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 800 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 800 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 800 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 800 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 800 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "## Done ##\n",
      "total 297M\n",
      "drwxr-xr-x  2 root root   12K Aug 19 21:55 .\n",
      "drwxr-xr-x 10 root root   205 Aug 19 17:44 ..\n",
      "-rw-r--r--  1 root root   80K Aug 19 21:55 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  861K Aug 19 21:55 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  897K Aug 19 21:55 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  934K Aug 19 21:55 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  981K Aug 19 21:55 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root 1011K Aug 19 21:55 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 21:55 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 21:55 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 21:55 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 21:55 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 21:55 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root   98K Aug 19 21:55 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 21:55 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 21:55 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 21:55 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 21:55 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 21:55 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 21:55 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 21:55 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 21:55 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 21:55 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 21:55 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  117K Aug 19 21:55 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 21:55 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 21:55 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 21:55 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 21:55 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 21:55 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 21:55 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 21:55 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 21:55 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 21:55 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 21:55 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  139K Aug 19 21:55 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 21:55 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 21:55 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 21:55 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 21:55 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 21:55 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 21:55 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 21:55 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 21:55 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 21:55 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 21:55 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  157K Aug 19 21:55 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 21:55 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 21:55 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 21:55 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 21:55 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 21:55 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 21:55 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 21:55 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 21:55 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 21:55 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 21:55 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  173K Aug 19 21:55 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 21:55 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 21:55 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 21:55 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 21:55 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 21:55 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 21:55 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 21:55 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 21:55 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 21:55 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 21:55 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  196K Aug 19 21:55 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 21:55 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 21:55 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 21:55 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 21:55 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 21:55 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 21:55 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 21:55 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 21:55 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 21:55 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 21:55 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  213K Aug 19 21:55 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 21:55 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 21:55 gen-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 21:55 gen-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 21:55 gen-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 21:55 gen-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 21:55 gen-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 21:55 gen-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 21:55 gen-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 21:55 gen-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 19 21:55 gen-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root   59K Aug 19 21:55 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root  467K Aug 19 21:55 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 19 21:55 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 19 21:55 gen-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 19 21:55 gen-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 19 21:55 gen-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 19 21:55 gen-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 19 21:55 gen-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 19 21:55 gen-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 19 21:55 gen-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 19 21:55 gen-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 19 21:55 gen-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root  507K Aug 19 21:55 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 19 21:55 gen-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  549K Aug 19 21:55 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  579K Aug 19 21:55 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  623K Aug 19 21:55 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  651K Aug 19 21:55 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  698K Aug 19 21:55 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  732K Aug 19 21:55 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  776K Aug 19 21:55 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  817K Aug 19 21:55 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  506K Aug 19 21:55 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  571K Aug 19 21:55 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  561K Aug 19 21:55 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  551K Aug 19 21:55 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  553K Aug 19 21:55 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  546K Aug 19 21:55 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  548K Aug 19 21:55 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  545K Aug 19 21:55 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  546K Aug 19 21:55 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  544K Aug 19 21:55 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  546K Aug 19 21:55 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  430K Aug 19 21:55 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  549K Aug 19 21:55 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  543K Aug 19 21:55 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  548K Aug 19 21:55 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  544K Aug 19 21:55 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  541K Aug 19 21:55 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  542K Aug 19 21:55 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  540K Aug 19 21:55 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  544K Aug 19 21:55 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  539K Aug 19 21:55 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  545K Aug 19 21:55 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  385K Aug 19 21:55 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  544K Aug 19 21:55 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  538K Aug 19 21:55 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  539K Aug 19 21:55 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  538K Aug 19 21:55 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  537K Aug 19 21:55 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 21:55 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  538K Aug 19 21:55 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 21:55 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  365K Aug 19 21:55 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 21:55 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 21:55 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 21:55 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  341K Aug 19 21:55 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 21:55 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 21:55 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 21:55 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 21:55 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 21:55 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 21:55 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  331K Aug 19 21:55 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 21:55 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  321K Aug 19 21:55 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 21:55 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 21:55 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 21:55 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root  317K Aug 19 21:55 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 21:55 shuffle-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 21:55 shuffle-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 21:55 shuffle-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 21:55 shuffle-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 21:55 shuffle-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root  814K Aug 19 21:55 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root  613K Aug 19 21:55 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 21:55 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 21:55 shuffle-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 21:55 shuffle-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 21:55 shuffle-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 21:55 shuffle-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root  599K Aug 19 21:55 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 21:55 shuffle-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  592K Aug 19 21:55 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  593K Aug 19 21:55 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  588K Aug 19 21:55 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  580K Aug 19 21:55 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  575K Aug 19 21:55 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  573K Aug 19 21:55 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  568K Aug 19 21:55 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  564K Aug 19 21:55 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root   49K Aug 19 21:55 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for lower word count - and shift the focus upwards\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 400 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 400 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 510 words dataset\n",
    "# \n",
    "for i in {50..550..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 800 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-1k/', '--model.lr_init=4e-4', '--model.lr_final=2e-4', '--data.max_token_size=1024', '--model.ctx_len=1024', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-512.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-1k/', '--model.lr_init=4e-4', '--model.lr_final=2e-4', '--data.max_token_size=1024', '--model.ctx_len=1024', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-512.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1320297280\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1320297280\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_215604-h80ji5x9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/h80ji5x9\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [00:00<00:00, 146528.80it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f8fa0c84b0277d7f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 342.06it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.59it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 4] Global seed set to 1320297280\n",
      "[rank: 6] Global seed set to 1320297280\n",
      "[rank: 7] Global seed set to 1320297280\n",
      "[rank: 3] Global seed set to 1320297280\n",
      "[rank: 5] Global seed set to 1320297280\n",
      "[rank: 1] Global seed set to 1320297280\n",
      "[rank: 2] Global seed set to 1320297280\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f8fa0c84b0277d7f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.77it/s]\n",
      "Map (num_proc=64):   1%|         | 1021/124585 [00:01<01:48, 1139.60 examples/s][rank: 7] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 21:56:55,622] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 21:56:55,700] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   4%|â–        | 5289/124585 [00:01<00:15, 7744.04 examples/s][rank: 4] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 21:56:56,042] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   8%|â–Œ       | 9443/124585 [00:02<00:09, 12640.82 examples/s][rank: 2] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 21:56:56,254] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 21:56:56,273] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 21:56:56,312] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   9%|â–‹      | 11615/124585 [00:02<00:07, 14351.07 examples/s][rank: 3] Global seed set to 1320297280\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 21:56:56,434] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 1320297280                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 21:57:20,141] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 2.000e-04 (0.0002)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07054853439331055 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10132265090942383 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10130953788757324 seconds\n",
      "Time to load fused_adam op: 0.10140657424926758 seconds\n",
      "Time to load fused_adam op: 0.10138750076293945 seconds\n",
      "Time to load fused_adam op: 0.10175967216491699 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10185956954956055 seconds\n",
      "Time to load fused_adam op: 0.10164046287536621 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06886792182922363 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10211563110351562 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10228443145751953 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10361361503601074 seconds\n",
      "Time to load utils op: 0.10233330726623535 seconds\n",
      "Time to load utils op: 0.10210657119750977 seconds\n",
      "Time to load utils op: 0.1021125316619873 seconds\n",
      "Time to load utils op: 0.1030881404876709 seconds\n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006003379821777344 seconds\n",
      "Time to load utils op: 0.0005600452423095703 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005786418914794922 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0005955696105957031 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006310939788818359 seconds\n",
      "Time to load utils op: 0.0006365776062011719 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006239414215087891 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008211135864257812 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/15536 [04:58<1:31:38,  2.68it/s, v_num=i5x9, train/loss=1.3/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 15536/15536 [1:34:18<00:00,  2.75it/s, v_num=i5x9, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆâ–                 | 1/16 [00:00<00:04,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 2/16 [00:00<00:03,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–Œ               | 3/16 [00:00<00:03,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 4/16 [00:01<00:03,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 5/16 [00:01<00:02,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 6/16 [00:01<00:02,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 7/16 [00:01<00:02,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 8/16 [00:02<00:02,  3.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 9/16 [00:02<00:01,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 10/16 [00:02<00:01,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/16 [00:02<00:01,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 12/16 [00:03<00:01,  3.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 13/16 [00:03<00:00,  3.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/16 [00:03<00:00,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 15/16 [00:03<00:00,  3.95it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 15536/15536 [1:34:30<00:00,  2.74it/s, v_num=i5x9, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 15536/15536 [1:34:30<00:00,  2.74it/s, v_num=i5x9, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 15536/15536 [1:34:42<00:00,  2.73it/s, v_num=i5x9, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–‚â–„â–‚â–ƒâ–â–„â–ƒâ–†â–‚â–„â–…â–‡â–‡â–†â–‚â–ˆâ–…â–†â–â–†â–‚â–â–‚â–†â–…â–â–ƒâ–â–†â–‡â–‡â–ƒâ–ƒâ–ƒâ–‡â–â–ˆâ–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–…â–â–‚â–â–„â–‚â–â–ƒâ–â–‚â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.06733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-1k (train-ctx=1k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/h80ji5x9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_215604-h80ji5x9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-1k (train-ctx=1k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-1k/\" \\\n",
    "        --model.lr_init=4e-4 \\\n",
    "        --model.lr_final=2e-4 \\\n",
    "        --data.max_token_size=1024 \\\n",
    "        --model.ctx_len=1024 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-512.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-1k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 19 23:33 ../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-1k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 97.33333333333334% similarity, with 73 matched token, and 2 token mismatch\n",
      "## Model validation for 80 tokens : 97.5% similarity, with 78 matched token, and 2 token mismatch\n",
      "## Model validation for 85 tokens : 97.6470588235294% similarity, with 83 matched token, and 2 token mismatch\n",
      "## Model validation for 90 tokens : 97.77777777777777% similarity, with 88 matched token, and 2 token mismatch\n",
      "## Model validation for 95 tokens : 98.94736842105263% similarity, with 94 matched token, and 1 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 99.33333333333333% similarity, with 149 matched token, and 1 token mismatch\n",
      "## Model validation for 160 tokens : 99.375% similarity, with 159 matched token, and 1 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.94736842105263% similarity, with 188 matched token, and 2 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 99.0909090909091% similarity, with 218 matched token, and 2 token mismatch\n",
      "## Model validation for 230 tokens : 98.69565217391305% similarity, with 227 matched token, and 3 token mismatch\n",
      "## Model validation for 240 tokens : 98.75% similarity, with 237 matched token, and 3 token mismatch\n",
      "## Model validation for 250 tokens : 98.8% similarity, with 247 matched token, and 3 token mismatch\n",
      "## Model validation for 260 tokens : 99.23076923076923% similarity, with 258 matched token, and 2 token mismatch\n",
      "## Model validation for 270 tokens : 98.51851851851852% similarity, with 266 matched token, and 4 token mismatch\n",
      "## Model validation for 280 tokens : 97.85714285714285% similarity, with 274 matched token, and 6 token mismatch\n",
      "## Model validation for 290 tokens : 98.27586206896551% similarity, with 285 matched token, and 5 token mismatch\n",
      "## Model validation for 300 tokens : 98.0% similarity, with 294 matched token, and 6 token mismatch\n",
      "## Model validation for 325 tokens : 96.92307692307692% similarity, with 315 matched token, and 10 token mismatch\n",
      "## Model validation for 350 tokens : 97.14285714285714% similarity, with 340 matched token, and 10 token mismatch\n",
      "## Model validation for 375 tokens : 95.73333333333333% similarity, with 359 matched token, and 16 token mismatch\n",
      "## Model validation for 400 tokens : 93.25% similarity, with 373 matched token, and 27 token mismatch\n",
      "## Model validation for 425 tokens : 92.94117647058823% similarity, with 395 matched token, and 30 token mismatch\n",
      "## Model validation for 450 tokens : 90.66666666666666% similarity, with 408 matched token, and 42 token mismatch\n",
      "## Model validation for 475 tokens : 88.42105263157895% similarity, with 420 matched token, and 55 token mismatch\n",
      "## Model validation for 500 tokens : 86.0% similarity, with 430 matched token, and 70 token mismatch\n",
      "## Model validation for 525 tokens : 82.85714285714286% similarity, with 435 matched token, and 90 token mismatch\n",
      "## Model validation for 550 tokens : 79.27272727272727% similarity, with 436 matched token, and 114 token mismatch\n",
      "## Model validation for 575 tokens : 76.8695652173913% similarity, with 442 matched token, and 133 token mismatch\n",
      "## Model validation for 600 tokens : 74.33333333333333% similarity, with 446 matched token, and 154 token mismatch\n",
      "## Model validation for 625 tokens : 71.04% similarity, with 444 matched token, and 181 token mismatch\n",
      "## Model validation for 650 tokens : 66.46153846153847% similarity, with 432 matched token, and 218 token mismatch\n",
      "## Model validation for 675 tokens : 60.44444444444444% similarity, with 408 matched token, and 267 token mismatch\n",
      "## Model validation for 700 tokens : 55.85714285714286% similarity, with 391 matched token, and 309 token mismatch\n",
      "## Model validation for 750 tokens : 48.93333333333334% similarity, with 367 matched token, and 383 token mismatch\n",
      "## Model validation for 800 tokens : 40.25% similarity, with 322 matched token, and 478 token mismatch\n",
      "## Model validation for 850 tokens : 36.11764705882353% similarity, with 307 matched token, and 543 token mismatch\n",
      "## Model validation for 900 tokens : 30.88888888888889% similarity, with 278 matched token, and 622 token mismatch\n",
      "## Model validation for 950 tokens : 27.57894736842105% similarity, with 262 matched token, and 688 token mismatch\n",
      "## Model validation for 1000 tokens : 23.200000000000003% similarity, with 232 matched token, and 768 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 4 : Low ctx size (2048), memory training\n",
    "\n",
    "- Tune 4: Low ctx size (2048), Scaling up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 100 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated a single JSONL file with 262 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 91 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated a single JSONL file with 546 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 108 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 67 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 175 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 100 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 133 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 73 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 100 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 100 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 200 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 200 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 100 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 200 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 200 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 200 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 200 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated a single JSONL file with 396 samples (20 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 295 samples (20 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 200 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated a single JSONL file with 32 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 200 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 100 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 143 samples (20 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 200 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 312 samples (20 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 200 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 282 samples (20 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 100 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 200 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 200 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 382 samples (20 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 200 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 323 samples (20 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 200 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 372 samples (20 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 200 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated a single JSONL file with 209 samples (20 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 273 samples (20 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 200 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 200 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated a single JSONL file with 361 samples (20 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (20 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 343 samples (20 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 200 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 200 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 200 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 200 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 200 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 200 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 200 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 200 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 200 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 200 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 200 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 200 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 200 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 200 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 200 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 200 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 200 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated a single JSONL file with 278 samples (20 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 186 samples (20 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 98 samples (20 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (20 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 200 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (20 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 336 samples (20 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 200 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 318 samples (20 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 200 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 200 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1035 max words - at ../dataset/shuffle-word-1035-count.jsonl\n",
      "Generated a single JSONL file with 198 samples (20 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 200 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 200 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1510 max words - at ../dataset/shuffle-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 200 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 200 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 200 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 200 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 200 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 200 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 200 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 200 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 217 samples (20 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 200 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1425 max words - at ../dataset/shuffle-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 200 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated a single JSONL file with 270 samples (20 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1055 max words - at ../dataset/shuffle-word-1055-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 200 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1335 max words - at ../dataset/shuffle-word-1335-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 200 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 200 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 200 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 200 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1850 max words - at ../dataset/shuffle-word-1850-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 200 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated a single JSONL file with 267 samples (20 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 200 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 200 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 200 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 200 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 200 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 200 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1185 max words - at ../dataset/shuffle-word-1185-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 200 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 200 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1075 max words - at ../dataset/shuffle-word-1075-count.jsonl\n",
      "Generated a single JSONL file with 409 samples (20 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1815 max words - at ../dataset/shuffle-word-1815-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 200 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 200 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 200 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 200 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 58 samples (20 token repeat) - 1175 max words - at ../dataset/shuffle-word-1175-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 200 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated a single JSONL file with 205 samples (20 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 200 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 200 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 200 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1575 max words - at ../dataset/shuffle-word-1575-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 200 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1095 max words - at ../dataset/shuffle-word-1095-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 200 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 200 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 200 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 200 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 200 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 200 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 200 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 200 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1895 max words - at ../dataset/shuffle-word-1895-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1525 max words - at ../dataset/shuffle-word-1525-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 200 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 200 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 200 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1215 max words - at ../dataset/shuffle-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1640 max words - at ../dataset/shuffle-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 200 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated a single JSONL file with 190 samples (20 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 200 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1540 max words - at ../dataset/shuffle-word-1540-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 200 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 200 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 200 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1205 max words - at ../dataset/shuffle-word-1205-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 200 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 200 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1945 max words - at ../dataset/shuffle-word-1945-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 200 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1695 max words - at ../dataset/shuffle-word-1695-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1980 max words - at ../dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1365 max words - at ../dataset/shuffle-word-1365-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 200 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1625 max words - at ../dataset/shuffle-word-1625-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 200 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 200 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 200 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 200 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1970 max words - at ../dataset/shuffle-word-1970-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 200 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 200 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1660 max words - at ../dataset/shuffle-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 58 samples (20 token repeat) - 1145 max words - at ../dataset/shuffle-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1670 max words - at ../dataset/shuffle-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1825 max words - at ../dataset/shuffle-word-1825-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1415 max words - at ../dataset/shuffle-word-1415-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 200 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1845 max words - at ../dataset/shuffle-word-1845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1935 max words - at ../dataset/shuffle-word-1935-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 200 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1165 max words - at ../dataset/shuffle-word-1165-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1595 max words - at ../dataset/shuffle-word-1595-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 200 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 200 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 200 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 1665 max words, 200 samples - at ../dataset/gen-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1790 max words - at ../dataset/shuffle-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1950 max words - at ../dataset/shuffle-word-1950-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 200 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1285 max words - at ../dataset/shuffle-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1905 max words - at ../dataset/shuffle-word-1905-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 200 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1065 max words - at ../dataset/shuffle-word-1065-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 200 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1925 max words - at ../dataset/shuffle-word-1925-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1125 max words - at ../dataset/shuffle-word-1125-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1615 max words - at ../dataset/shuffle-word-1615-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 200 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1990 max words - at ../dataset/shuffle-word-1990-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1960 max words - at ../dataset/shuffle-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1690 max words - at ../dataset/shuffle-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 200 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 200 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 200 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 200 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1830 max words - at ../dataset/shuffle-word-1830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1965 max words - at ../dataset/shuffle-word-1965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1765 max words - at ../dataset/shuffle-word-1765-count.jsonl\n",
      "Generated a single JSONL file with 291 samples (20 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1045 max words - at ../dataset/shuffle-word-1045-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1355 max words - at ../dataset/shuffle-word-1355-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 200 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 200 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 200 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 200 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 200 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1565 max words - at ../dataset/shuffle-word-1565-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1995 max words - at ../dataset/shuffle-word-1995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1805 max words - at ../dataset/shuffle-word-1805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1920 max words - at ../dataset/shuffle-word-1920-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 200 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 200 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 200 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1545 max words - at ../dataset/shuffle-word-1545-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1875 max words - at ../dataset/shuffle-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1580 max words - at ../dataset/shuffle-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1655 max words - at ../dataset/shuffle-word-1655-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 181 samples (20 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 200 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1985 max words - at ../dataset/shuffle-word-1985-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (20 token repeat) - 1235 max words - at ../dataset/shuffle-word-1235-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 200 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1885 max words - at ../dataset/shuffle-word-1885-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 200 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1105 max words - at ../dataset/shuffle-word-1105-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1085 max words - at ../dataset/shuffle-word-1085-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 200 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1930 max words - at ../dataset/shuffle-word-1930-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 200 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 200 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (20 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1685 max words - at ../dataset/shuffle-word-1685-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 200 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 200 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 200 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 200 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 200 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 200 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 200 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated a single JSONL file with 137 samples (20 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 200 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated JSONL file with - 1375 max words, 200 samples - at ../dataset/gen-word-1375-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 200 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1165 max words, 200 samples - at ../dataset/gen-word-1165-count.jsonl\n",
      "Generated JSONL file with - 1145 max words, 200 samples - at ../dataset/gen-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1135 max words - at ../dataset/shuffle-word-1135-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 200 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1975 max words - at ../dataset/shuffle-word-1975-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 200 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 200 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated a single JSONL file with 83 samples (20 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 200 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 1555 max words, 200 samples - at ../dataset/gen-word-1555-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 200 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 201 samples (20 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 200 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 200 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 200 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 200 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 200 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated JSONL file with - 1355 max words, 200 samples - at ../dataset/gen-word-1355-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 200 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 200 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1265 max words - at ../dataset/shuffle-word-1265-count.jsonl\n",
      "Generated a single JSONL file with 83 samples (20 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 200 samples - at ../dataset/gen-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (20 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 200 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated JSONL file with - 1175 max words, 200 samples - at ../dataset/gen-word-1175-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 200 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 200 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1745 max words, 200 samples - at ../dataset/gen-word-1745-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 200 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated JSONL file with - 1025 max words, 200 samples - at ../dataset/gen-word-1025-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 200 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 200 samples - at ../dataset/gen-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 200 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1495 max words, 200 samples - at ../dataset/gen-word-1495-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 200 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1375 max words - at ../dataset/shuffle-word-1375-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 200 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 200 samples - at ../dataset/gen-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 200 samples - at ../dataset/gen-word-1950-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 200 samples - at ../dataset/gen-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 200 samples - at ../dataset/gen-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 200 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated JSONL file with - 1135 max words, 200 samples - at ../dataset/gen-word-1135-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1015 max words - at ../dataset/shuffle-word-1015-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 200 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1005 max words - at ../dataset/shuffle-word-1005-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 200 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1025 max words - at ../dataset/shuffle-word-1025-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 200 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 200 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 200 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 200 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 200 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 200 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated JSONL file with - 1065 max words, 200 samples - at ../dataset/gen-word-1065-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 200 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated JSONL file with - 1115 max words, 200 samples - at ../dataset/gen-word-1115-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 200 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 200 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 1315 max words, 200 samples - at ../dataset/gen-word-1315-count.jsonl\n",
      "Generated JSONL file with - 1215 max words, 200 samples - at ../dataset/gen-word-1215-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 200 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated JSONL file with - 1365 max words, 200 samples - at ../dataset/gen-word-1365-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 200 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 200 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 200 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 200 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 200 samples - at ../dataset/gen-word-1750-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 200 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 200 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1745 max words - at ../dataset/shuffle-word-1745-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated a single JSONL file with 55 samples (20 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1445 max words - at ../dataset/shuffle-word-1445-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 200 samples - at ../dataset/gen-word-1680-count.jsonl\n",
      "Generated JSONL file with - 1835 max words, 200 samples - at ../dataset/gen-word-1835-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated JSONL file with - 1875 max words, 200 samples - at ../dataset/gen-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1680 max words - at ../dataset/shuffle-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1630 max words - at ../dataset/shuffle-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1735 max words, 200 samples - at ../dataset/gen-word-1735-count.jsonl\n",
      "Generated JSONL file with - 1125 max words, 200 samples - at ../dataset/gen-word-1125-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 200 samples - at ../dataset/gen-word-1890-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 200 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1840 max words - at ../dataset/shuffle-word-1840-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 200 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 200 samples - at ../dataset/gen-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1810 max words - at ../dataset/shuffle-word-1810-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1105 max words, 200 samples - at ../dataset/gen-word-1105-count.jsonl\n",
      "Generated JSONL file with - 1455 max words, 200 samples - at ../dataset/gen-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1795 max words - at ../dataset/shuffle-word-1795-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 200 samples - at ../dataset/gen-word-1780-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 200 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 200 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1325 max words - at ../dataset/shuffle-word-1325-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1550 max words - at ../dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1560 max words - at ../dataset/shuffle-word-1560-count.jsonl\n",
      "Generated JSONL file with - 1755 max words, 200 samples - at ../dataset/gen-word-1755-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 200 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1725 max words - at ../dataset/shuffle-word-1725-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1515 max words - at ../dataset/shuffle-word-1515-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 200 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 200 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 200 samples - at ../dataset/gen-word-1710-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 200 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 200 samples - at ../dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 200 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 200 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 200 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1075 max words, 200 samples - at ../dataset/gen-word-1075-count.jsonl\n",
      "Generated JSONL file with - 1935 max words, 200 samples - at ../dataset/gen-word-1935-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 200 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 1155 max words, 200 samples - at ../dataset/gen-word-1155-count.jsonl\n",
      "Generated JSONL file with - 1275 max words, 200 samples - at ../dataset/gen-word-1275-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 200 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 200 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 200 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated a single JSONL file with 43 samples (20 token repeat) - 1395 max words - at ../dataset/shuffle-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1650 max words - at ../dataset/shuffle-word-1650-count.jsonl\n",
      "Generated JSONL file with - 1005 max words, 200 samples - at ../dataset/gen-word-1005-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 200 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1555 max words - at ../dataset/shuffle-word-1555-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1635 max words - at ../dataset/shuffle-word-1635-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 200 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 200 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1720 max words - at ../dataset/shuffle-word-1720-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1590 max words - at ../dataset/shuffle-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 200 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1735 max words - at ../dataset/shuffle-word-1735-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 200 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1405 max words - at ../dataset/shuffle-word-1405-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1435 max words - at ../dataset/shuffle-word-1435-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 200 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated JSONL file with - 1015 max words, 200 samples - at ../dataset/gen-word-1015-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 200 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1775 max words - at ../dataset/shuffle-word-1775-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1785 max words - at ../dataset/shuffle-word-1785-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1780 max words - at ../dataset/shuffle-word-1780-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 200 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 1775 max words, 200 samples - at ../dataset/gen-word-1775-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1610 max words - at ../dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1155 max words - at ../dataset/shuffle-word-1155-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 200 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 200 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated JSONL file with - 1995 max words, 200 samples - at ../dataset/gen-word-1995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1475 max words - at ../dataset/shuffle-word-1475-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1225 max words - at ../dataset/shuffle-word-1225-count.jsonl\n",
      "Generated JSONL file with - 1765 max words, 200 samples - at ../dataset/gen-word-1765-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1585 max words - at ../dataset/shuffle-word-1585-count.jsonl\n",
      "Generated JSONL file with - 1325 max words, 200 samples - at ../dataset/gen-word-1325-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1505 max words - at ../dataset/shuffle-word-1505-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 200 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 1085 max words, 200 samples - at ../dataset/gen-word-1085-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 200 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1385 max words - at ../dataset/shuffle-word-1385-count.jsonl\n",
      "Generated JSONL file with - 1945 max words, 200 samples - at ../dataset/gen-word-1945-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 200 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1530 max words - at ../dataset/shuffle-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1890 max words - at ../dataset/shuffle-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1315 max words - at ../dataset/shuffle-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1740 max words - at ../dataset/shuffle-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1570 max words - at ../dataset/shuffle-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 200 samples - at ../dataset/gen-word-1810-count.jsonl\n",
      "Generated JSONL file with - 1035 max words, 200 samples - at ../dataset/gen-word-1035-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 200 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 200 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated JSONL file with - 1385 max words, 200 samples - at ../dataset/gen-word-1385-count.jsonl\n",
      "Generated JSONL file with - 1925 max words, 200 samples - at ../dataset/gen-word-1925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1730 max words - at ../dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1520 max words - at ../dataset/shuffle-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1455 max words - at ../dataset/shuffle-word-1455-count.jsonl\n",
      "Generated JSONL file with - 1985 max words, 200 samples - at ../dataset/gen-word-1985-count.jsonl\n",
      "Generated JSONL file with - 1055 max words, 200 samples - at ../dataset/gen-word-1055-count.jsonl\n",
      "Generated JSONL file with - 1245 max words, 200 samples - at ../dataset/gen-word-1245-count.jsonl\n",
      "Generated JSONL file with - 1575 max words, 200 samples - at ../dataset/gen-word-1575-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 200 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1115 max words - at ../dataset/shuffle-word-1115-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1710 max words - at ../dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1860 max words - at ../dataset/shuffle-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1045 max words, 200 samples - at ../dataset/gen-word-1045-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 200 samples - at ../dataset/gen-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated JSONL file with - 1905 max words, 200 samples - at ../dataset/gen-word-1905-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1835 max words - at ../dataset/shuffle-word-1835-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 200 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1195 max words, 200 samples - at ../dataset/gen-word-1195-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1305 max words - at ../dataset/shuffle-word-1305-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 200 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1495 max words - at ../dataset/shuffle-word-1495-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1255 max words - at ../dataset/shuffle-word-1255-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated JSONL file with - 1695 max words, 200 samples - at ../dataset/gen-word-1695-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 200 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 200 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1705 max words - at ../dataset/shuffle-word-1705-count.jsonl\n",
      "Generated JSONL file with - 1785 max words, 200 samples - at ../dataset/gen-word-1785-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1770 max words - at ../dataset/shuffle-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1185 max words, 200 samples - at ../dataset/gen-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1675 max words - at ../dataset/shuffle-word-1675-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1750 max words - at ../dataset/shuffle-word-1750-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 200 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 200 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1465 max words, 200 samples - at ../dataset/gen-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1665 max words - at ../dataset/shuffle-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1245 max words - at ../dataset/shuffle-word-1245-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1870 max words - at ../dataset/shuffle-word-1870-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 200 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 200 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1465 max words - at ../dataset/shuffle-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1760 max words - at ../dataset/shuffle-word-1760-count.jsonl\n",
      "Generated JSONL file with - 1345 max words, 200 samples - at ../dataset/gen-word-1345-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 200 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 200 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1715 max words - at ../dataset/shuffle-word-1715-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 200 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1535 max words - at ../dataset/shuffle-word-1535-count.jsonl\n",
      "Generated a single JSONL file with 58 samples (20 token repeat) - 1195 max words - at ../dataset/shuffle-word-1195-count.jsonl\n",
      "Generated JSONL file with - 1225 max words, 200 samples - at ../dataset/gen-word-1225-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1485 max words - at ../dataset/shuffle-word-1485-count.jsonl\n",
      "Generated JSONL file with - 1845 max words, 200 samples - at ../dataset/gen-word-1845-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 200 samples - at ../dataset/gen-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1915 max words - at ../dataset/shuffle-word-1915-count.jsonl\n",
      "Generated JSONL file with - 1445 max words, 200 samples - at ../dataset/gen-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1645 max words - at ../dataset/shuffle-word-1645-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1865 max words - at ../dataset/shuffle-word-1865-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 200 samples - at ../dataset/gen-word-1930-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 200 samples - at ../dataset/gen-word-1970-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 200 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated JSONL file with - 1725 max words, 200 samples - at ../dataset/gen-word-1725-count.jsonl\n",
      "Generated JSONL file with - 1825 max words, 200 samples - at ../dataset/gen-word-1825-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 200 samples - at ../dataset/gen-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1295 max words - at ../dataset/shuffle-word-1295-count.jsonl\n",
      "Generated JSONL file with - 1795 max words, 200 samples - at ../dataset/gen-word-1795-count.jsonl\n",
      "Generated JSONL file with - 1255 max words, 200 samples - at ../dataset/gen-word-1255-count.jsonl\n",
      "Generated JSONL file with - 1865 max words, 200 samples - at ../dataset/gen-word-1865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1955 max words - at ../dataset/shuffle-word-1955-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1880 max words - at ../dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1345 max words - at ../dataset/shuffle-word-1345-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1275 max words - at ../dataset/shuffle-word-1275-count.jsonl\n",
      "Generated JSONL file with - 1425 max words, 200 samples - at ../dataset/gen-word-1425-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1620 max words - at ../dataset/shuffle-word-1620-count.jsonl\n",
      "Generated JSONL file with - 1815 max words, 200 samples - at ../dataset/gen-word-1815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1940 max words - at ../dataset/shuffle-word-1940-count.jsonl\n",
      "Generated JSONL file with - 1965 max words, 200 samples - at ../dataset/gen-word-1965-count.jsonl\n",
      "Generated JSONL file with - 1415 max words, 200 samples - at ../dataset/gen-word-1415-count.jsonl\n",
      "Generated JSONL file with - 1095 max words, 200 samples - at ../dataset/gen-word-1095-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1855 max words - at ../dataset/shuffle-word-1855-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 200 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 200 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 200 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1285 max words, 200 samples - at ../dataset/gen-word-1285-count.jsonl\n",
      "Generated JSONL file with - 1475 max words, 200 samples - at ../dataset/gen-word-1475-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 200 samples - at ../dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 200 samples - at ../dataset/gen-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1605 max words - at ../dataset/shuffle-word-1605-count.jsonl\n",
      "Generated JSONL file with - 1595 max words, 200 samples - at ../dataset/gen-word-1595-count.jsonl\n",
      "Generated JSONL file with - 1565 max words, 200 samples - at ../dataset/gen-word-1565-count.jsonl\n",
      "Generated JSONL file with - 1535 max words, 200 samples - at ../dataset/gen-word-1535-count.jsonl\n",
      "Generated JSONL file with - 1605 max words, 200 samples - at ../dataset/gen-word-1605-count.jsonl\n",
      "Generated JSONL file with - 1635 max words, 200 samples - at ../dataset/gen-word-1635-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 200 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated JSONL file with - 1675 max words, 200 samples - at ../dataset/gen-word-1675-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 200 samples - at ../dataset/gen-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1910 max words - at ../dataset/shuffle-word-1910-count.jsonl\n",
      "Generated JSONL file with - 1715 max words, 200 samples - at ../dataset/gen-word-1715-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 200 samples - at ../dataset/gen-word-1850-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 200 samples - at ../dataset/gen-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1625 max words, 200 samples - at ../dataset/gen-word-1625-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 200 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 200 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated JSONL file with - 1855 max words, 200 samples - at ../dataset/gen-word-1855-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 200 samples - at ../dataset/gen-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1305 max words, 200 samples - at ../dataset/gen-word-1305-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 200 samples - at ../dataset/gen-word-1730-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 200 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1755 max words - at ../dataset/shuffle-word-1755-count.jsonl\n",
      "Generated JSONL file with - 1645 max words, 200 samples - at ../dataset/gen-word-1645-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 200 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 200 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 1205 max words, 200 samples - at ../dataset/gen-word-1205-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 200 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 1655 max words, 200 samples - at ../dataset/gen-word-1655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1820 max words - at ../dataset/shuffle-word-1820-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 200 samples - at ../dataset/gen-word-1590-count.jsonl\n",
      "Generated JSONL file with - 1515 max words, 200 samples - at ../dataset/gen-word-1515-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 200 samples - at ../dataset/gen-word-1640-count.jsonl\n",
      "Generated JSONL file with - 1585 max words, 200 samples - at ../dataset/gen-word-1585-count.jsonl\n",
      "Generated JSONL file with - 1235 max words, 200 samples - at ../dataset/gen-word-1235-count.jsonl\n",
      "Generated JSONL file with - 1265 max words, 200 samples - at ../dataset/gen-word-1265-count.jsonl\n",
      "Generated JSONL file with - 1405 max words, 200 samples - at ../dataset/gen-word-1405-count.jsonl\n",
      "Generated JSONL file with - 1505 max words, 200 samples - at ../dataset/gen-word-1505-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 200 samples - at ../dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 200 samples - at ../dataset/gen-word-1550-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 200 samples - at ../dataset/gen-word-1520-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 200 samples - at ../dataset/gen-word-1840-count.jsonl\n",
      "Generated JSONL file with - 1525 max words, 200 samples - at ../dataset/gen-word-1525-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 200 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 1335 max words, 200 samples - at ../dataset/gen-word-1335-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 200 samples - at ../dataset/gen-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1895 max words, 200 samples - at ../dataset/gen-word-1895-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 200 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 200 samples - at ../dataset/gen-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 200 samples - at ../dataset/gen-word-1580-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 200 samples - at ../dataset/gen-word-1530-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 200 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1435 max words, 200 samples - at ../dataset/gen-word-1435-count.jsonl\n",
      "Generated JSONL file with - 1975 max words, 200 samples - at ../dataset/gen-word-1975-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 200 samples - at ../dataset/gen-word-1740-count.jsonl\n",
      "Generated JSONL file with - 1395 max words, 200 samples - at ../dataset/gen-word-1395-count.jsonl\n",
      "Generated JSONL file with - 1295 max words, 200 samples - at ../dataset/gen-word-1295-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 200 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1545 max words, 200 samples - at ../dataset/gen-word-1545-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 200 samples - at ../dataset/gen-word-1830-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 200 samples - at ../dataset/gen-word-1670-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 200 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 200 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 200 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 1485 max words, 200 samples - at ../dataset/gen-word-1485-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 200 samples - at ../dataset/gen-word-1820-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 200 samples - at ../dataset/gen-word-1610-count.jsonl\n",
      "Generated JSONL file with - 1915 max words, 200 samples - at ../dataset/gen-word-1915-count.jsonl\n",
      "Generated JSONL file with - 1885 max words, 200 samples - at ../dataset/gen-word-1885-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 200 samples - at ../dataset/gen-word-1910-count.jsonl\n",
      "Generated JSONL file with - 1615 max words, 200 samples - at ../dataset/gen-word-1615-count.jsonl\n",
      "Generated JSONL file with - 1805 max words, 200 samples - at ../dataset/gen-word-1805-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 200 samples - at ../dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 1705 max words, 200 samples - at ../dataset/gen-word-1705-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 200 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 1685 max words, 200 samples - at ../dataset/gen-word-1685-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 200 samples - at ../dataset/gen-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 200 samples - at ../dataset/gen-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1955 max words, 200 samples - at ../dataset/gen-word-1955-count.jsonl\n",
      "## Done ##\n",
      "total 965M\n",
      "drwxr-xr-x  2 root root   36K Aug 19 23:34 .\n",
      "drwxr-xr-x 10 root root   205 Aug 19 17:44 ..\n",
      "-rw-r--r--  1 root root   20K Aug 19 23:34 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  107K Aug 19 23:34 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root  221K Aug 19 23:34 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 19 23:34 gen-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root  238K Aug 19 23:34 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root  240K Aug 19 23:34 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 19 23:34 gen-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root  255K Aug 19 23:34 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 19 23:34 gen-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root  260K Aug 19 23:34 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 19 23:34 gen-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root  273K Aug 19 23:34 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 19 23:34 gen-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 19 23:34 gen-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root  281K Aug 19 23:34 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root  294K Aug 19 23:34 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 19 23:34 gen-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root  297K Aug 19 23:34 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 19 23:34 gen-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root   25K Aug 19 23:34 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  311K Aug 19 23:34 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 19 23:34 gen-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root  313K Aug 19 23:34 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 19 23:34 gen-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root  331K Aug 19 23:34 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 19 23:34 gen-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root  338K Aug 19 23:34 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 19 23:34 gen-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root  349K Aug 19 23:34 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 19 23:34 gen-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root  360K Aug 19 23:34 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 19 23:34 gen-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root  364K Aug 19 23:34 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 19 23:34 gen-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root  379K Aug 19 23:34 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 19 23:34 gen-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root  388K Aug 19 23:34 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 19 23:34 gen-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root  399K Aug 19 23:34 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 19 23:34 gen-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 19 23:34 gen-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 19 23:34 gen-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root   31K Aug 19 23:34 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  413K Aug 19 23:34 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 19 23:34 gen-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root  421K Aug 19 23:34 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  434K Aug 19 23:34 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  442K Aug 19 23:34 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  450K Aug 19 23:34 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  457K Aug 19 23:34 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  464K Aug 19 23:34 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  477K Aug 19 23:34 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  484K Aug 19 23:34 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  494K Aug 19 23:34 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root   34K Aug 19 23:34 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 19 23:34 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 23:34 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 23:34 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  550K Aug 19 23:34 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  557K Aug 19 23:34 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  562K Aug 19 23:34 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  573K Aug 19 23:34 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  580K Aug 19 23:34 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  595K Aug 19 23:34 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root   39K Aug 19 23:34 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  593K Aug 19 23:34 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  618K Aug 19 23:34 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  625K Aug 19 23:34 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  635K Aug 19 23:34 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  647K Aug 19 23:34 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  652K Aug 19 23:34 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  662K Aug 19 23:34 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  669K Aug 19 23:34 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  678K Aug 19 23:34 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  691K Aug 19 23:34 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root   45K Aug 19 23:34 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  701K Aug 19 23:34 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  711K Aug 19 23:34 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  725K Aug 19 23:34 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  721K Aug 19 23:34 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  748K Aug 19 23:34 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  758K Aug 19 23:34 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  748K Aug 19 23:34 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  770K Aug 19 23:34 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  780K Aug 19 23:34 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  787K Aug 19 23:34 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root   49K Aug 19 23:34 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  798K Aug 19 23:34 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  806K Aug 19 23:34 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root  826K Aug 19 23:34 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root  814K Aug 19 23:34 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root  832K Aug 19 23:34 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root  840K Aug 19 23:34 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root  847K Aug 19 23:34 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root  871K Aug 19 23:34 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root  873K Aug 19 23:34 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root  875K Aug 19 23:34 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root   54K Aug 19 23:34 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  890K Aug 19 23:34 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root  912K Aug 19 23:34 gen-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root  911K Aug 19 23:34 gen-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root  922K Aug 19 23:34 gen-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root  929K Aug 19 23:34 gen-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root  951K Aug 19 23:34 gen-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root  950K Aug 19 23:34 gen-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root  979K Aug 19 23:34 gen-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root  966K Aug 19 23:34 gen-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root  978K Aug 19 23:34 gen-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root   15K Aug 19 23:34 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root   58K Aug 19 23:34 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  993K Aug 19 23:34 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root 1001K Aug 19 23:34 gen-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 1011K Aug 19 23:34 gen-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root 1016K Aug 19 23:34 gen-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root   63K Aug 19 23:34 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 19 23:34 gen-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root   70K Aug 19 23:34 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 19 23:34 gen-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root   72K Aug 19 23:34 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 19 23:34 gen-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root   78K Aug 19 23:34 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 19 23:34 gen-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root   84K Aug 19 23:34 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 19 23:34 gen-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root   86K Aug 19 23:34 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 19 23:34 gen-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root   91K Aug 19 23:34 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 19 23:34 gen-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root   96K Aug 19 23:34 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 19 23:34 gen-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root  104K Aug 19 23:34 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 19 23:34 gen-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 19 23:34 gen-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root   54K Aug 19 23:34 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root  557K Aug 19 23:34 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root  555K Aug 19 23:34 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root  556K Aug 19 23:34 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root  548K Aug 19 23:34 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root  554K Aug 19 23:34 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root  546K Aug 19 23:34 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 19 23:34 shuffle-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 19 23:34 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root  547K Aug 19 23:34 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root  547K Aug 19 23:34 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root  547K Aug 19 23:34 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root   43K Aug 19 23:34 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  546K Aug 19 23:34 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 19 23:34 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root  542K Aug 19 23:34 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root  549K Aug 19 23:34 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 19 23:34 shuffle-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root  549K Aug 19 23:34 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 19 23:34 shuffle-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root  547K Aug 19 23:34 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root  540K Aug 19 23:34 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root  541K Aug 19 23:34 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 19 23:34 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root  545K Aug 19 23:34 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 19 23:34 shuffle-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root  539K Aug 19 23:34 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root  545K Aug 19 23:34 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 19 23:34 shuffle-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root   40K Aug 19 23:34 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  538K Aug 19 23:34 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root  539K Aug 19 23:34 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 23:34 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  540K Aug 19 23:34 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 23:34 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 23:34 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  539K Aug 19 23:34 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root   34K Aug 19 23:34 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 23:34 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  538K Aug 19 23:34 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 23:34 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 23:34 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 23:34 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  536K Aug 19 23:34 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  534K Aug 19 23:34 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 23:34 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root   36K Aug 19 23:34 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  535K Aug 19 23:34 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 23:34 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 23:34 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root   33K Aug 19 23:34 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 23:34 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 23:34 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  533K Aug 19 23:34 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 19 23:34 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 23:34 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 23:34 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root  531K Aug 19 23:34 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root   32K Aug 19 23:34 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 23:34 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 23:34 shuffle-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root  532K Aug 19 23:34 shuffle-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root   84K Aug 19 23:34 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root   31K Aug 19 23:34 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 19 23:34 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 19 23:34 shuffle-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 19 23:34 shuffle-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root   31K Aug 19 23:34 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 19 23:34 shuffle-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 19 23:34 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 19 23:34 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 19 23:34 shuffle-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 19 23:34 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 19 23:34 shuffle-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 19 23:34 shuffle-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 19 23:34 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 19 23:34 shuffle-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 19 23:34 shuffle-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 19 23:34 shuffle-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 19 23:34 shuffle-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 19 23:34 shuffle-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root   13K Aug 19 23:34 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for lower word count - and shift the focus upwards\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 100 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 105+ - 1050 words dataset\n",
    "# \n",
    "for i in {105..2000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 200 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-2k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=2048', '--model.ctx_len=2048', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-2k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=2048', '--model.ctx_len=2048', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 579681169\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 579681169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230819_233434-0vfv3yvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/0vfv3yvr\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<00:00, 159739.33it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-38d1f557cd9e302c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 96.30it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 579681169\n",
      "[rank: 6] Global seed set to 579681169\n",
      "[rank: 4] Global seed set to 579681169\n",
      "[rank: 3] Global seed set to 579681169\n",
      "[rank: 1] Global seed set to 579681169\n",
      "[rank: 2] Global seed set to 579681169\n",
      "[rank: 7] Global seed set to 579681169\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-38d1f557cd9e302c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.89it/s]\n",
      "Map (num_proc=64):   5%|â–        | 5047/112057 [00:09<00:42, 2531.43 examples/s][rank: 6] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-19 23:35:25,258] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   5%|â–        | 5360/112057 [00:09<00:39, 2685.75 examples/s][rank: 7] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-19 23:35:25,351] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   5%|â–        | 5955/112057 [00:09<00:40, 2622.61 examples/s][rank: 1] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-19 23:35:25,534] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   6%|â–Œ        | 6229/112057 [00:09<00:39, 2646.88 examples/s][rank: 2] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-19 23:35:25,646] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-19 23:35:25,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-19 23:35:25,716] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   6%|â–Œ        | 6852/112057 [00:10<00:45, 2327.34 examples/s][rank: 5] Global seed set to 579681169\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-19 23:35:25,933] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 579681169                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-19 23:36:04,206] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06934714317321777 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1013338565826416 seconds\n",
      "Time to load fused_adam op: 0.10138082504272461 seconds\n",
      "Time to load fused_adam op: 0.10143160820007324 seconds\n",
      "Time to load fused_adam op: 0.10135960578918457 seconds\n",
      "Time to load fused_adam op: 0.10148262977600098 seconds\n",
      "Time to load fused_adam op: 0.10146141052246094 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10175681114196777 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07159233093261719 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10248208045959473 seconds\n",
      "Time to load utils op: 0.10231494903564453 seconds\n",
      "Time to load utils op: 0.10209298133850098 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10204672813415527 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10264039039611816 seconds\n",
      "Time to load utils op: 0.1029653549194336 seconds\n",
      "Time to load utils op: 0.10304117202758789 seconds\n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006058216094970703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000667572021484375 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006461143493652344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.001111745834350586 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0010569095611572266 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0015430450439453125 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0015625953674316406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0010156631469726562 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   8%| | 800/10061 [07:12<1:23:24,  1.85it/s, v_num=3yvr, train/loss=4.6/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 10061/10061 [1:28:50<00:00,  1.89it/s, v_num=3yvr, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|â–ˆâ–‹                 | 1/11 [00:00<00:03,  2.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 2/11 [00:00<00:03,  2.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3/11 [00:00<00:02,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 4/11 [00:01<00:02,  3.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 5/11 [00:01<00:01,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 6/11 [00:01<00:01,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 7/11 [00:02<00:01,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 8/11 [00:02<00:00,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 9/11 [00:02<00:00,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/11 [00:02<00:00,  3.62it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10061/10061 [1:29:02<00:00,  1.88it/s, v_num=3yvr, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 10061/10061 [1:29:02<00:00,  1.88it/s, v_num=3yvr, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 10061/10061 [1:29:16<00:00,  1.88it/s, v_num=3yvr, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–…â–‡â–‚â–‡â–ƒâ–‚â–‡â–â–â–‡â–ˆâ–â–…â–„â–ˆâ–†â–‚â–‚â–ˆâ–‚â–ƒâ–…â–…â–‡â–…â–†â–„â–…â–…â–†â–ƒâ–†â–‚â–‚â–â–…â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–‡â–ˆâ–‡â–â–„â–â–â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–‚â–â–‚â–‚â–ƒâ–â–â–â–â–â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.07129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 314\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.3455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-2k (train-ctx=2k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/0vfv3yvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230819_233434-0vfv3yvr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-2k (train-ctx=2k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-2k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=2048 \\\n",
    "        --model.ctx_len=2048 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-2k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-2k.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 20 01:06 ../model/v5-L6-D4096-E0_1-mem-ctx-2k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-2k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-2k.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-2k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 98.66666666666667% similarity, with 74 matched token, and 1 token mismatch\n",
      "## Model validation for 80 tokens : 98.75% similarity, with 79 matched token, and 1 token mismatch\n",
      "## Model validation for 85 tokens : 98.82352941176471% similarity, with 84 matched token, and 1 token mismatch\n",
      "## Model validation for 90 tokens : 98.88888888888889% similarity, with 89 matched token, and 1 token mismatch\n",
      "## Model validation for 95 tokens : 98.94736842105263% similarity, with 94 matched token, and 1 token mismatch\n",
      "## Model validation for 100 tokens : 99.0% similarity, with 99 matched token, and 1 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 99.0909090909091% similarity, with 109 matched token, and 1 token mismatch\n",
      "## Model validation for 115 tokens : 99.1304347826087% similarity, with 114 matched token, and 1 token mismatch\n",
      "## Model validation for 120 tokens : 99.16666666666667% similarity, with 119 matched token, and 1 token mismatch\n",
      "## Model validation for 125 tokens : 99.2% similarity, with 124 matched token, and 1 token mismatch\n",
      "## Model validation for 130 tokens : 99.23076923076923% similarity, with 129 matched token, and 1 token mismatch\n",
      "## Model validation for 135 tokens : 99.25925925925925% similarity, with 134 matched token, and 1 token mismatch\n",
      "## Model validation for 140 tokens : 99.28571428571429% similarity, with 139 matched token, and 1 token mismatch\n",
      "## Model validation for 145 tokens : 99.3103448275862% similarity, with 144 matched token, and 1 token mismatch\n",
      "## Model validation for 150 tokens : 98.66666666666667% similarity, with 148 matched token, and 2 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.94736842105263% similarity, with 188 matched token, and 2 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 99.0909090909091% similarity, with 218 matched token, and 2 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 99.16666666666667% similarity, with 238 matched token, and 2 token mismatch\n",
      "## Model validation for 250 tokens : 99.2% similarity, with 248 matched token, and 2 token mismatch\n",
      "## Model validation for 260 tokens : 98.84615384615385% similarity, with 257 matched token, and 3 token mismatch\n",
      "## Model validation for 270 tokens : 97.77777777777777% similarity, with 264 matched token, and 6 token mismatch\n",
      "## Model validation for 280 tokens : 97.85714285714285% similarity, with 274 matched token, and 6 token mismatch\n",
      "## Model validation for 290 tokens : 97.24137931034483% similarity, with 282 matched token, and 8 token mismatch\n",
      "## Model validation for 300 tokens : 97.0% similarity, with 291 matched token, and 9 token mismatch\n",
      "## Model validation for 325 tokens : 97.53846153846155% similarity, with 317 matched token, and 8 token mismatch\n",
      "## Model validation for 350 tokens : 97.42857142857143% similarity, with 341 matched token, and 9 token mismatch\n",
      "## Model validation for 375 tokens : 96.26666666666667% similarity, with 361 matched token, and 14 token mismatch\n",
      "## Model validation for 400 tokens : 95.75% similarity, with 383 matched token, and 17 token mismatch\n",
      "## Model validation for 425 tokens : 95.29411764705881% similarity, with 405 matched token, and 20 token mismatch\n",
      "## Model validation for 450 tokens : 93.77777777777779% similarity, with 422 matched token, and 28 token mismatch\n",
      "## Model validation for 475 tokens : 92.63157894736842% similarity, with 440 matched token, and 35 token mismatch\n",
      "## Model validation for 500 tokens : 91.4% similarity, with 457 matched token, and 43 token mismatch\n",
      "## Model validation for 525 tokens : 91.42857142857143% similarity, with 480 matched token, and 45 token mismatch\n",
      "## Model validation for 550 tokens : 91.27272727272727% similarity, with 502 matched token, and 48 token mismatch\n",
      "## Model validation for 575 tokens : 91.65217391304348% similarity, with 527 matched token, and 48 token mismatch\n",
      "## Model validation for 600 tokens : 89.33333333333333% similarity, with 536 matched token, and 64 token mismatch\n",
      "## Model validation for 625 tokens : 88.8% similarity, with 555 matched token, and 70 token mismatch\n",
      "## Model validation for 650 tokens : 87.84615384615385% similarity, with 571 matched token, and 79 token mismatch\n",
      "## Model validation for 675 tokens : 86.37037037037038% similarity, with 583 matched token, and 92 token mismatch\n",
      "## Model validation for 700 tokens : 85.42857142857143% similarity, with 598 matched token, and 102 token mismatch\n",
      "## Model validation for 750 tokens : 82.39999999999999% similarity, with 618 matched token, and 132 token mismatch\n",
      "## Model validation for 800 tokens : 81.125% similarity, with 649 matched token, and 151 token mismatch\n",
      "## Model validation for 850 tokens : 78.23529411764706% similarity, with 665 matched token, and 185 token mismatch\n",
      "## Model validation for 900 tokens : 75.0% similarity, with 675 matched token, and 225 token mismatch\n",
      "## Model validation for 950 tokens : 71.89473684210526% similarity, with 683 matched token, and 267 token mismatch\n",
      "## Model validation for 1000 tokens : 67.60000000000001% similarity, with 676 matched token, and 324 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-2k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 5 : Ramping up the ctx size (4096), memory training\n",
    "\n",
    "- Tune 5: Mid ctx size (4096), Scaling up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 100 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 177 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 266 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 100 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 91 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 100 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 100 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 104 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 55 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 134 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 58 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated a single JSONL file with 46 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 557 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 100 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 100 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 28 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 100 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 100 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 100 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 100 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 100 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 100 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 100 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 100 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated a single JSONL file with 82 samples (20 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 100 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 100 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 100 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 100 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 100 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 100 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 100 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 100 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 200 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 100 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 200 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 200 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 100 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 200 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 100 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 200 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 200 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 100 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 100 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 100 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 100 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 100 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1020 max words - at ../dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1125 max words - at ../dataset/shuffle-word-1125-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 100 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 100 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 100 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 100 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 100 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 100 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 100 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 100 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 200 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 100 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 100 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 100 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 100 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 100 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 100 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 200 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 100 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 100 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 100 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 100 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 100 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 100 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 100 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 100 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1470 max words - at ../dataset/shuffle-word-1470-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1350 max words - at ../dataset/shuffle-word-1350-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 100 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1190 max words - at ../dataset/shuffle-word-1190-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 200 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1170 max words - at ../dataset/shuffle-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1225 max words - at ../dataset/shuffle-word-1225-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1340 max words - at ../dataset/shuffle-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1850 max words - at ../dataset/shuffle-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 200 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 200 samples - at ../dataset/gen-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 200 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 200 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1275 max words - at ../dataset/shuffle-word-1275-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 100 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 200 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1760 max words - at ../dataset/shuffle-word-1760-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 200 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1465 max words - at ../dataset/shuffle-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1250 max words - at ../dataset/shuffle-word-1250-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 200 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 200 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 100 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 200 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1295 max words - at ../dataset/shuffle-word-1295-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1380 max words - at ../dataset/shuffle-word-1380-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1590 max words - at ../dataset/shuffle-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1510 max words - at ../dataset/shuffle-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1385 max words - at ../dataset/shuffle-word-1385-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 100 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (20 token repeat) - 1290 max words - at ../dataset/shuffle-word-1290-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 200 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 200 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1475 max words - at ../dataset/shuffle-word-1475-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 200 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 200 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 200 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1490 max words - at ../dataset/shuffle-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 200 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2120 max words - at ../dataset/shuffle-word-2120-count.jsonl\n",
      "Generated a single JSONL file with 65 samples (20 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1685 max words - at ../dataset/shuffle-word-1685-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1175 max words - at ../dataset/shuffle-word-1175-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1745 max words - at ../dataset/shuffle-word-1745-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 200 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1080 max words - at ../dataset/shuffle-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 55 samples (20 token repeat) - 1255 max words - at ../dataset/shuffle-word-1255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1430 max words - at ../dataset/shuffle-word-1430-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 200 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 200 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1835 max words - at ../dataset/shuffle-word-1835-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 200 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 200 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1245 max words - at ../dataset/shuffle-word-1245-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 200 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 200 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1985 max words - at ../dataset/shuffle-word-1985-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 100 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 200 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2195 max words - at ../dataset/shuffle-word-2195-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 200 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated JSONL file with - 1205 max words, 200 samples - at ../dataset/gen-word-1205-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1460 max words - at ../dataset/shuffle-word-1460-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 200 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated JSONL file with - 1015 max words, 200 samples - at ../dataset/gen-word-1015-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2285 max words - at ../dataset/shuffle-word-2285-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1120 max words - at ../dataset/shuffle-word-1120-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1635 max words - at ../dataset/shuffle-word-1635-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 200 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 100 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1650 max words - at ../dataset/shuffle-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1310 max words - at ../dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1115 max words - at ../dataset/shuffle-word-1115-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 100 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 200 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2320 max words - at ../dataset/shuffle-word-2320-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 200 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2345 max words - at ../dataset/shuffle-word-2345-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1445 max words - at ../dataset/shuffle-word-1445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1505 max words - at ../dataset/shuffle-word-1505-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2225 max words - at ../dataset/shuffle-word-2225-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1365 max words - at ../dataset/shuffle-word-1365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1930 max words - at ../dataset/shuffle-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (20 token repeat) - 2330 max words - at ../dataset/shuffle-word-2330-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 100 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 100 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2220 max words - at ../dataset/shuffle-word-2220-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 200 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 100 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1545 max words - at ../dataset/shuffle-word-1545-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1710 max words - at ../dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 200 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 200 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2270 max words - at ../dataset/shuffle-word-2270-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2280 max words - at ../dataset/shuffle-word-2280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1765 max words - at ../dataset/shuffle-word-1765-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 100 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2260 max words - at ../dataset/shuffle-word-2260-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1675 max words - at ../dataset/shuffle-word-1675-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2065 max words - at ../dataset/shuffle-word-2065-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2085 max words - at ../dataset/shuffle-word-2085-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1420 max words - at ../dataset/shuffle-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1950 max words - at ../dataset/shuffle-word-1950-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2365 max words - at ../dataset/shuffle-word-2365-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 100 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2035 max words - at ../dataset/shuffle-word-2035-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1520 max words - at ../dataset/shuffle-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2380 max words - at ../dataset/shuffle-word-2380-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2200 max words - at ../dataset/shuffle-word-2200-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1995 max words - at ../dataset/shuffle-word-1995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1515 max words - at ../dataset/shuffle-word-1515-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1955 max words - at ../dataset/shuffle-word-1955-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1645 max words - at ../dataset/shuffle-word-1645-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1530 max words - at ../dataset/shuffle-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1935 max words - at ../dataset/shuffle-word-1935-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1630 max words - at ../dataset/shuffle-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2140 max words - at ../dataset/shuffle-word-2140-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1485 max words - at ../dataset/shuffle-word-1485-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 100 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 200 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1820 max words - at ../dataset/shuffle-word-1820-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 200 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2045 max words - at ../dataset/shuffle-word-2045-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1840 max words - at ../dataset/shuffle-word-1840-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1795 max words - at ../dataset/shuffle-word-1795-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2295 max words - at ../dataset/shuffle-word-2295-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1335 max words - at ../dataset/shuffle-word-1335-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 100 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1620 max words - at ../dataset/shuffle-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1435 max words - at ../dataset/shuffle-word-1435-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 100 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 100 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 200 samples - at ../dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 100 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2335 max words - at ../dataset/shuffle-word-2335-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2165 max words - at ../dataset/shuffle-word-2165-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2325 max words - at ../dataset/shuffle-word-2325-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 200 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2210 max words - at ../dataset/shuffle-word-2210-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2055 max words - at ../dataset/shuffle-word-2055-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2375 max words - at ../dataset/shuffle-word-2375-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2695 max words - at ../dataset/shuffle-word-2695-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2405 max words - at ../dataset/shuffle-word-2405-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2415 max words - at ../dataset/shuffle-word-2415-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2015 max words - at ../dataset/shuffle-word-2015-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 100 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 200 samples - at ../dataset/gen-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 100 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 200 samples - at ../dataset/gen-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2390 max words - at ../dataset/shuffle-word-2390-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2125 max words - at ../dataset/shuffle-word-2125-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2180 max words - at ../dataset/shuffle-word-2180-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 200 samples - at ../dataset/gen-word-1250-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 200 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2115 max words - at ../dataset/shuffle-word-2115-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2500 max words - at ../dataset/shuffle-word-2500-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (20 token repeat) - 2510 max words - at ../dataset/shuffle-word-2510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2235 max words - at ../dataset/shuffle-word-2235-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2290 max words - at ../dataset/shuffle-word-2290-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 200 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2110 max words - at ../dataset/shuffle-word-2110-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2630 max words - at ../dataset/shuffle-word-2630-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 200 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2340 max words - at ../dataset/shuffle-word-2340-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (20 token repeat) - 2570 max words - at ../dataset/shuffle-word-2570-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 100 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2190 max words - at ../dataset/shuffle-word-2190-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2355 max words - at ../dataset/shuffle-word-2355-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 100 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 200 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated JSONL file with - 1145 max words, 200 samples - at ../dataset/gen-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2155 max words - at ../dataset/shuffle-word-2155-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 200 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (20 token repeat) - 2485 max words - at ../dataset/shuffle-word-2485-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1395 max words - at ../dataset/shuffle-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 200 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 200 samples - at ../dataset/gen-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2440 max words - at ../dataset/shuffle-word-2440-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2400 max words - at ../dataset/shuffle-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2275 max words - at ../dataset/shuffle-word-2275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1605 max words - at ../dataset/shuffle-word-1605-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1410 max words - at ../dataset/shuffle-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 200 samples - at ../dataset/gen-word-1860-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 200 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1595 max words - at ../dataset/shuffle-word-1595-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 200 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1370 max words - at ../dataset/shuffle-word-1370-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 100 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1925 max words - at ../dataset/shuffle-word-1925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2245 max words - at ../dataset/shuffle-word-2245-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 200 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1330 max words - at ../dataset/shuffle-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1450 max words - at ../dataset/shuffle-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (20 token repeat) - 2545 max words - at ../dataset/shuffle-word-2545-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1805 max words - at ../dataset/shuffle-word-1805-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1865 max words - at ../dataset/shuffle-word-1865-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1375 max words - at ../dataset/shuffle-word-1375-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2205 max words - at ../dataset/shuffle-word-2205-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1815 max words - at ../dataset/shuffle-word-1815-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 200 samples - at ../dataset/gen-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1325 max words - at ../dataset/shuffle-word-1325-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1580 max words - at ../dataset/shuffle-word-1580-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 200 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1625 max words - at ../dataset/shuffle-word-1625-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2435 max words - at ../dataset/shuffle-word-2435-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1770 max words - at ../dataset/shuffle-word-1770-count.jsonl\n",
      "Generated a single JSONL file with 36 samples (20 token repeat) - 2465 max words - at ../dataset/shuffle-word-2465-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1320 max words - at ../dataset/shuffle-word-1320-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2215 max words - at ../dataset/shuffle-word-2215-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1360 max words - at ../dataset/shuffle-word-1360-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 100 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1715 max words - at ../dataset/shuffle-word-1715-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 200 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2385 max words - at ../dataset/shuffle-word-2385-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1570 max words - at ../dataset/shuffle-word-1570-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (20 token repeat) - 2420 max words - at ../dataset/shuffle-word-2420-count.jsonl\n",
      "Generated JSONL file with - 1065 max words, 200 samples - at ../dataset/gen-word-1065-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1315 max words - at ../dataset/shuffle-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1615 max words - at ../dataset/shuffle-word-1615-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2915 max words - at ../dataset/shuffle-word-2915-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (20 token repeat) - 2540 max words - at ../dataset/shuffle-word-2540-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2370 max words - at ../dataset/shuffle-word-2370-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 200 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2495 max words - at ../dataset/shuffle-word-2495-count.jsonl\n",
      "Generated JSONL file with - 1935 max words, 200 samples - at ../dataset/gen-word-1935-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1405 max words - at ../dataset/shuffle-word-1405-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2445 max words - at ../dataset/shuffle-word-2445-count.jsonl\n",
      "Generated JSONL file with - 1095 max words, 200 samples - at ../dataset/gen-word-1095-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 200 samples - at ../dataset/gen-word-1970-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2095 max words - at ../dataset/shuffle-word-2095-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1390 max words - at ../dataset/shuffle-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2250 max words - at ../dataset/shuffle-word-2250-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2490 max words - at ../dataset/shuffle-word-2490-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 200 samples - at ../dataset/gen-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1680 max words - at ../dataset/shuffle-word-1680-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 200 samples - at ../dataset/gen-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2410 max words - at ../dataset/shuffle-word-2410-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2255 max words - at ../dataset/shuffle-word-2255-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1560 max words - at ../dataset/shuffle-word-1560-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 200 samples - at ../dataset/gen-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1160 max words - at ../dataset/shuffle-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1455 max words - at ../dataset/shuffle-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1690 max words - at ../dataset/shuffle-word-1690-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 100 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1495 max words - at ../dataset/shuffle-word-1495-count.jsonl\n",
      "Generated JSONL file with - 1865 max words, 200 samples - at ../dataset/gen-word-1865-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 100 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 100 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 1755 max words, 200 samples - at ../dataset/gen-word-1755-count.jsonl\n",
      "Generated JSONL file with - 1115 max words, 200 samples - at ../dataset/gen-word-1115-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1550 max words - at ../dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 42 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1735 max words - at ../dataset/shuffle-word-1735-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated JSONL file with - 1355 max words, 200 samples - at ../dataset/gen-word-1355-count.jsonl\n",
      "Generated a single JSONL file with 57 samples (20 token repeat) - 1280 max words - at ../dataset/shuffle-word-1280-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2240 max words - at ../dataset/shuffle-word-2240-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2170 max words - at ../dataset/shuffle-word-2170-count.jsonl\n",
      "Generated JSONL file with - 1485 max words, 200 samples - at ../dataset/gen-word-1485-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1535 max words - at ../dataset/shuffle-word-1535-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3290 max words - at ../dataset/shuffle-word-3290-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3080 max words - at ../dataset/shuffle-word-3080-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2185 max words - at ../dataset/shuffle-word-2185-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated JSONL file with - 2185 max words, 200 samples - at ../dataset/gen-word-2185-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 200 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2105 max words - at ../dataset/shuffle-word-2105-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 200 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 200 samples - at ../dataset/gen-word-1470-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 200 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1810 max words - at ../dataset/shuffle-word-1810-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 200 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2070 max words - at ../dataset/shuffle-word-2070-count.jsonl\n",
      "Generated JSONL file with - 1445 max words, 200 samples - at ../dataset/gen-word-1445-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 200 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated JSONL file with - 1455 max words, 200 samples - at ../dataset/gen-word-1455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2090 max words - at ../dataset/shuffle-word-2090-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 100 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 200 samples - at ../dataset/gen-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1855 max words - at ../dataset/shuffle-word-1855-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 200 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3210 max words - at ../dataset/shuffle-word-3210-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1875 max words - at ../dataset/shuffle-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 200 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3170 max words - at ../dataset/shuffle-word-3170-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 200 samples - at ../dataset/gen-word-1750-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (20 token repeat) - 2560 max words - at ../dataset/shuffle-word-2560-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 100 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2315 max words - at ../dataset/shuffle-word-2315-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 200 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 1585 max words, 200 samples - at ../dataset/gen-word-1585-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2610 max words - at ../dataset/shuffle-word-2610-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3460 max words - at ../dataset/shuffle-word-3460-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2790 max words - at ../dataset/shuffle-word-2790-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated JSONL file with - 1695 max words, 200 samples - at ../dataset/gen-word-1695-count.jsonl\n",
      "Generated JSONL file with - 1855 max words, 200 samples - at ../dataset/gen-word-1855-count.jsonl\n",
      "Generated JSONL file with - 1535 max words, 200 samples - at ../dataset/gen-word-1535-count.jsonl\n",
      "Generated JSONL file with - 1605 max words, 200 samples - at ../dataset/gen-word-1605-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 200 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1795 max words, 200 samples - at ../dataset/gen-word-1795-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3305 max words - at ../dataset/shuffle-word-3305-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2840 max words - at ../dataset/shuffle-word-2840-count.jsonl\n",
      "Generated JSONL file with - 1515 max words, 200 samples - at ../dataset/gen-word-1515-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1610 max words - at ../dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 38 samples (20 token repeat) - 2450 max words - at ../dataset/shuffle-word-2450-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 100 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated JSONL file with - 1175 max words, 200 samples - at ../dataset/gen-word-1175-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 100 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 200 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 200 samples - at ../dataset/gen-word-1460-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3455 max words - at ../dataset/shuffle-word-3455-count.jsonl\n",
      "Generated JSONL file with - 1945 max words, 200 samples - at ../dataset/gen-word-1945-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 100 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1015 max words - at ../dataset/shuffle-word-1015-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2835 max words - at ../dataset/shuffle-word-2835-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3570 max words - at ../dataset/shuffle-word-3570-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3555 max words - at ../dataset/shuffle-word-3555-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3535 max words - at ../dataset/shuffle-word-3535-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3165 max words - at ../dataset/shuffle-word-3165-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 200 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1965 max words - at ../dataset/shuffle-word-1965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3255 max words - at ../dataset/shuffle-word-3255-count.jsonl\n",
      "Generated a single JSONL file with 24 samples (20 token repeat) - 2660 max words - at ../dataset/shuffle-word-2660-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3575 max words - at ../dataset/shuffle-word-3575-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (20 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1525 max words - at ../dataset/shuffle-word-1525-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 200 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 200 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3125 max words - at ../dataset/shuffle-word-3125-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 200 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3110 max words - at ../dataset/shuffle-word-3110-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3240 max words - at ../dataset/shuffle-word-3240-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated a single JSONL file with 64 samples (20 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3250 max words - at ../dataset/shuffle-word-3250-count.jsonl\n",
      "Generated JSONL file with - 1775 max words, 200 samples - at ../dataset/gen-word-1775-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1640 max words - at ../dataset/shuffle-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3215 max words - at ../dataset/shuffle-word-3215-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 200 samples - at ../dataset/gen-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3335 max words - at ../dataset/shuffle-word-3335-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3150 max words - at ../dataset/shuffle-word-3150-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1215 max words - at ../dataset/shuffle-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3265 max words - at ../dataset/shuffle-word-3265-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 100 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1095 max words - at ../dataset/shuffle-word-1095-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3520 max words - at ../dataset/shuffle-word-3520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1775 max words - at ../dataset/shuffle-word-1775-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 200 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 200 samples - at ../dataset/gen-word-1440-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3340 max words - at ../dataset/shuffle-word-3340-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1440 max words - at ../dataset/shuffle-word-1440-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2460 max words - at ../dataset/shuffle-word-2460-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 200 samples - at ../dataset/gen-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 79 samples (20 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated JSONL file with - 1545 max words, 200 samples - at ../dataset/gen-word-1545-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3135 max words - at ../dataset/shuffle-word-3135-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3435 max words - at ../dataset/shuffle-word-3435-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1035 max words - at ../dataset/shuffle-word-1035-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3625 max words - at ../dataset/shuffle-word-3625-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1230 max words - at ../dataset/shuffle-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3260 max words - at ../dataset/shuffle-word-3260-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3245 max words - at ../dataset/shuffle-word-3245-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3300 max words - at ../dataset/shuffle-word-3300-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2010 max words - at ../dataset/shuffle-word-2010-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3505 max words - at ../dataset/shuffle-word-3505-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2160 max words - at ../dataset/shuffle-word-2160-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1885 max words - at ../dataset/shuffle-word-1885-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3120 max words - at ../dataset/shuffle-word-3120-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (20 token repeat) - 2360 max words - at ../dataset/shuffle-word-2360-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 200 samples - at ../dataset/gen-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 51 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 200 samples - at ../dataset/gen-word-1840-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1880 max words - at ../dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 63 samples (20 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2675 max words - at ../dataset/shuffle-word-2675-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 200 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3425 max words - at ../dataset/shuffle-word-3425-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3665 max words - at ../dataset/shuffle-word-3665-count.jsonl\n",
      "Generated a single JSONL file with 62 samples (20 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 200 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3655 max words - at ../dataset/shuffle-word-3655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1540 max words - at ../dataset/shuffle-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3610 max words - at ../dataset/shuffle-word-3610-count.jsonl\n",
      "Generated JSONL file with - 1555 max words, 200 samples - at ../dataset/gen-word-1555-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1025 max words - at ../dataset/shuffle-word-1025-count.jsonl\n",
      "Generated JSONL file with - 1345 max words, 200 samples - at ../dataset/gen-word-1345-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3585 max words - at ../dataset/shuffle-word-3585-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 200 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1240 max words - at ../dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2175 max words - at ../dataset/shuffle-word-2175-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3600 max words - at ../dataset/shuffle-word-3600-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3530 max words - at ../dataset/shuffle-word-3530-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 100 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated JSONL file with - 1325 max words, 200 samples - at ../dataset/gen-word-1325-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 200 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3725 max words - at ../dataset/shuffle-word-3725-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3550 max words - at ../dataset/shuffle-word-3550-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2430 max words - at ../dataset/shuffle-word-2430-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3230 max words - at ../dataset/shuffle-word-3230-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1050 max words - at ../dataset/shuffle-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 80 samples (20 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 200 samples - at ../dataset/gen-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3390 max words - at ../dataset/shuffle-word-3390-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 100 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (20 token repeat) - 2350 max words - at ../dataset/shuffle-word-2350-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 200 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 200 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3205 max words - at ../dataset/shuffle-word-3205-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (20 token repeat) - 1210 max words - at ../dataset/shuffle-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1030 max words - at ../dataset/shuffle-word-1030-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 100 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 1725 max words, 200 samples - at ../dataset/gen-word-1725-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 200 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1055 max words - at ../dataset/shuffle-word-1055-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3380 max words - at ../dataset/shuffle-word-3380-count.jsonl\n",
      "Generated JSONL file with - 2460 max words, 200 samples - at ../dataset/gen-word-2460-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1735 max words, 200 samples - at ../dataset/gen-word-1735-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 100 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 2035 max words, 200 samples - at ../dataset/gen-word-2035-count.jsonl\n",
      "Generated JSONL file with - 2010 max words, 200 samples - at ../dataset/gen-word-2010-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 200 samples - at ../dataset/gen-word-1830-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1075 max words - at ../dataset/shuffle-word-1075-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1185 max words - at ../dataset/shuffle-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3450 max words - at ../dataset/shuffle-word-3450-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1670 max words - at ../dataset/shuffle-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3495 max words - at ../dataset/shuffle-word-3495-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 200 samples - at ../dataset/gen-word-1510-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 200 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3490 max words - at ../dataset/shuffle-word-3490-count.jsonl\n",
      "Generated JSONL file with - 1125 max words, 200 samples - at ../dataset/gen-word-1125-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (20 token repeat) - 2555 max words - at ../dataset/shuffle-word-2555-count.jsonl\n",
      "Generated JSONL file with - 1025 max words, 200 samples - at ../dataset/gen-word-1025-count.jsonl\n",
      "Generated JSONL file with - 1525 max words, 200 samples - at ../dataset/gen-word-1525-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3675 max words - at ../dataset/shuffle-word-3675-count.jsonl\n",
      "Generated JSONL file with - 2260 max words, 200 samples - at ../dataset/gen-word-2260-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3320 max words - at ../dataset/shuffle-word-3320-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3440 max words - at ../dataset/shuffle-word-3440-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3155 max words - at ../dataset/shuffle-word-3155-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3220 max words - at ../dataset/shuffle-word-3220-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3275 max words - at ../dataset/shuffle-word-3275-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3095 max words - at ../dataset/shuffle-word-3095-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1660 max words - at ../dataset/shuffle-word-1660-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3690 max words - at ../dataset/shuffle-word-3690-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1005 max words - at ../dataset/shuffle-word-1005-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2145 max words - at ../dataset/shuffle-word-2145-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3525 max words - at ../dataset/shuffle-word-3525-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3280 max words - at ../dataset/shuffle-word-3280-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3175 max words - at ../dataset/shuffle-word-3175-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1010 max words - at ../dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1355 max words - at ../dataset/shuffle-word-1355-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1085 max words - at ../dataset/shuffle-word-1085-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 200 samples - at ../dataset/gen-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 81 samples (20 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3100 max words - at ../dataset/shuffle-word-3100-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (20 token repeat) - 1130 max words - at ../dataset/shuffle-word-1130-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1135 max words - at ../dataset/shuffle-word-1135-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3395 max words - at ../dataset/shuffle-word-3395-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1110 max words - at ../dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 61 samples (20 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1555 max words - at ../dataset/shuffle-word-1555-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1265 max words - at ../dataset/shuffle-word-1265-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1480 max words - at ../dataset/shuffle-word-1480-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 200 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1090 max words - at ../dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3670 max words - at ../dataset/shuffle-word-3670-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1145 max words - at ../dataset/shuffle-word-1145-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3680 max words - at ../dataset/shuffle-word-3680-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3630 max words - at ../dataset/shuffle-word-3630-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 47 samples (20 token repeat) - 1270 max words - at ../dataset/shuffle-word-1270-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 200 samples - at ../dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 2145 max words, 200 samples - at ../dataset/gen-word-2145-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1065 max words - at ../dataset/shuffle-word-1065-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1425 max words - at ../dataset/shuffle-word-1425-count.jsonl\n",
      "Generated JSONL file with - 1815 max words, 200 samples - at ../dataset/gen-word-1815-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 200 samples - at ../dataset/gen-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3800 max words - at ../dataset/shuffle-word-3800-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 200 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1105 max words - at ../dataset/shuffle-word-1105-count.jsonl\n",
      "Generated JSONL file with - 2310 max words, 200 samples - at ../dataset/gen-word-2310-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1235 max words - at ../dataset/shuffle-word-1235-count.jsonl\n",
      "Generated JSONL file with - 1275 max words, 200 samples - at ../dataset/gen-word-1275-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1140 max words - at ../dataset/shuffle-word-1140-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2995 max words - at ../dataset/shuffle-word-2995-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1180 max words - at ../dataset/shuffle-word-1180-count.jsonl\n",
      "Generated JSONL file with - 1765 max words, 200 samples - at ../dataset/gen-word-1765-count.jsonl\n",
      "Generated JSONL file with - 2345 max words, 200 samples - at ../dataset/gen-word-2345-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3270 max words - at ../dataset/shuffle-word-3270-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (20 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3430 max words - at ../dataset/shuffle-word-3430-count.jsonl\n",
      "Generated a single JSONL file with 55 samples (20 token repeat) - 1220 max words - at ../dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1345 max words - at ../dataset/shuffle-word-1345-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3760 max words - at ../dataset/shuffle-word-3760-count.jsonl\n",
      "Generated JSONL file with - 1475 max words, 200 samples - at ../dataset/gen-word-1475-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 200 samples - at ../dataset/gen-word-1810-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3105 max words - at ../dataset/shuffle-word-3105-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3710 max words - at ../dataset/shuffle-word-3710-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (20 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3295 max words - at ../dataset/shuffle-word-3295-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3385 max words - at ../dataset/shuffle-word-3385-count.jsonl\n",
      "Generated JSONL file with - 2330 max words, 200 samples - at ../dataset/gen-word-2330-count.jsonl\n",
      "Generated a single JSONL file with 99 samples (20 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated JSONL file with - 1375 max words, 200 samples - at ../dataset/gen-word-1375-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 200 samples - at ../dataset/gen-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3475 max words - at ../dataset/shuffle-word-3475-count.jsonl\n",
      "Generated JSONL file with - 2140 max words, 200 samples - at ../dataset/gen-word-2140-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1060 max words - at ../dataset/shuffle-word-1060-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1045 max words - at ../dataset/shuffle-word-1045-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3485 max words - at ../dataset/shuffle-word-3485-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 200 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3235 max words - at ../dataset/shuffle-word-3235-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3445 max words - at ../dataset/shuffle-word-3445-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3480 max words - at ../dataset/shuffle-word-3480-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1040 max words - at ../dataset/shuffle-word-1040-count.jsonl\n",
      "Generated JSONL file with - 2290 max words, 200 samples - at ../dataset/gen-word-2290-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3510 max words - at ../dataset/shuffle-word-3510-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 200 samples - at ../dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 2530 max words, 200 samples - at ../dataset/gen-word-2530-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 200 samples - at ../dataset/gen-word-1540-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3660 max words - at ../dataset/shuffle-word-3660-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1150 max words - at ../dataset/shuffle-word-1150-count.jsonl\n",
      "Generated JSONL file with - 2370 max words, 200 samples - at ../dataset/gen-word-2370-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3470 max words - at ../dataset/shuffle-word-3470-count.jsonl\n",
      "Generated JSONL file with - 2245 max words, 200 samples - at ../dataset/gen-word-2245-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 200 samples - at ../dataset/gen-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3695 max words - at ../dataset/shuffle-word-3695-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 200 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1070 max words - at ../dataset/shuffle-word-1070-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 200 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1575 max words - at ../dataset/shuffle-word-1575-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 200 samples - at ../dataset/gen-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3720 max words - at ../dataset/shuffle-word-3720-count.jsonl\n",
      "Generated JSONL file with - 2360 max words, 200 samples - at ../dataset/gen-word-2360-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 200 samples - at ../dataset/gen-word-1820-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3730 max words - at ../dataset/shuffle-word-3730-count.jsonl\n",
      "Generated JSONL file with - 2285 max words, 200 samples - at ../dataset/gen-word-2285-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 200 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 200 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3500 max words - at ../dataset/shuffle-word-3500-count.jsonl\n",
      "Generated JSONL file with - 2220 max words, 200 samples - at ../dataset/gen-word-2220-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3920 max words - at ../dataset/shuffle-word-3920-count.jsonl\n",
      "Generated JSONL file with - 1435 max words, 200 samples - at ../dataset/gen-word-1435-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3355 max words - at ../dataset/shuffle-word-3355-count.jsonl\n",
      "Generated JSONL file with - 1415 max words, 200 samples - at ../dataset/gen-word-1415-count.jsonl\n",
      "Generated JSONL file with - 1215 max words, 200 samples - at ../dataset/gen-word-1215-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2030 max words - at ../dataset/shuffle-word-2030-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 200 samples - at ../dataset/gen-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3650 max words - at ../dataset/shuffle-word-3650-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1740 max words - at ../dataset/shuffle-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3830 max words - at ../dataset/shuffle-word-3830-count.jsonl\n",
      "Generated JSONL file with - 2395 max words, 200 samples - at ../dataset/gen-word-2395-count.jsonl\n",
      "Generated JSONL file with - 1595 max words, 200 samples - at ../dataset/gen-word-1595-count.jsonl\n",
      "Generated JSONL file with - 2210 max words, 200 samples - at ../dataset/gen-word-2210-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3645 max words - at ../dataset/shuffle-word-3645-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 200 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3465 max words - at ../dataset/shuffle-word-3465-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 200 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3785 max words - at ../dataset/shuffle-word-3785-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3420 max words - at ../dataset/shuffle-word-3420-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3735 max words - at ../dataset/shuffle-word-3735-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 200 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated JSONL file with - 2305 max words, 200 samples - at ../dataset/gen-word-2305-count.jsonl\n",
      "Generated JSONL file with - 2120 max words, 200 samples - at ../dataset/gen-word-2120-count.jsonl\n",
      "Generated JSONL file with - 1645 max words, 200 samples - at ../dataset/gen-word-1645-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2130 max words - at ../dataset/shuffle-word-2130-count.jsonl\n",
      "Generated JSONL file with - 1245 max words, 200 samples - at ../dataset/gen-word-1245-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3910 max words - at ../dataset/shuffle-word-3910-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3685 max words - at ../dataset/shuffle-word-3685-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1750 max words - at ../dataset/shuffle-word-1750-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1975 max words - at ../dataset/shuffle-word-1975-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1665 max words - at ../dataset/shuffle-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1725 max words - at ../dataset/shuffle-word-1725-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2005 max words - at ../dataset/shuffle-word-2005-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1825 max words - at ../dataset/shuffle-word-1825-count.jsonl\n",
      "Generated JSONL file with - 1425 max words, 200 samples - at ../dataset/gen-word-1425-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 200 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated JSONL file with - 2335 max words, 200 samples - at ../dataset/gen-word-2335-count.jsonl\n",
      "Generated JSONL file with - 2365 max words, 200 samples - at ../dataset/gen-word-2365-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1780 max words - at ../dataset/shuffle-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3595 max words - at ../dataset/shuffle-word-3595-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1915 max words - at ../dataset/shuffle-word-1915-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3765 max words - at ../dataset/shuffle-word-3765-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1785 max words - at ../dataset/shuffle-word-1785-count.jsonl\n",
      "Generated JSONL file with - 2030 max words, 200 samples - at ../dataset/gen-word-2030-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1920 max words - at ../dataset/shuffle-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3620 max words - at ../dataset/shuffle-word-3620-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 200 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 200 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated JSONL file with - 2295 max words, 200 samples - at ../dataset/gen-word-2295-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 200 samples - at ../dataset/gen-word-1730-count.jsonl\n",
      "Generated JSONL file with - 1875 max words, 200 samples - at ../dataset/gen-word-1875-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1585 max words - at ../dataset/shuffle-word-1585-count.jsonl\n",
      "Generated JSONL file with - 1805 max words, 200 samples - at ../dataset/gen-word-1805-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 200 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated JSONL file with - 2420 max words, 200 samples - at ../dataset/gen-word-2420-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3615 max words - at ../dataset/shuffle-word-3615-count.jsonl\n",
      "Generated JSONL file with - 1315 max words, 200 samples - at ../dataset/gen-word-1315-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3745 max words - at ../dataset/shuffle-word-3745-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1895 max words - at ../dataset/shuffle-word-1895-count.jsonl\n",
      "Generated JSONL file with - 2390 max words, 200 samples - at ../dataset/gen-word-2390-count.jsonl\n",
      "Generated a single JSONL file with 48 samples (20 token repeat) - 1285 max words - at ../dataset/shuffle-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2020 max words - at ../dataset/shuffle-word-2020-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 200 samples - at ../dataset/gen-word-1910-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2300 max words - at ../dataset/shuffle-word-2300-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 200 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated JSONL file with - 2325 max words, 200 samples - at ../dataset/gen-word-2325-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3515 max words - at ../dataset/shuffle-word-3515-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2645 max words - at ../dataset/shuffle-word-2645-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 200 samples - at ../dataset/gen-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1755 max words - at ../dataset/shuffle-word-1755-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 200 samples - at ../dataset/gen-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1945 max words - at ../dataset/shuffle-word-1945-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2265 max words - at ../dataset/shuffle-word-2265-count.jsonl\n",
      "Generated JSONL file with - 1075 max words, 200 samples - at ../dataset/gen-word-1075-count.jsonl\n",
      "Generated JSONL file with - 1295 max words, 200 samples - at ../dataset/gen-word-1295-count.jsonl\n",
      "Generated JSONL file with - 1385 max words, 200 samples - at ../dataset/gen-word-1385-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 200 samples - at ../dataset/gen-word-1120-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2635 max words - at ../dataset/shuffle-word-2635-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 200 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 1165 max words, 200 samples - at ../dataset/gen-word-1165-count.jsonl\n",
      "Generated JSONL file with - 3630 max words, 200 samples - at ../dataset/gen-word-3630-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1195 max words - at ../dataset/shuffle-word-1195-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3900 max words - at ../dataset/shuffle-word-3900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3750 max words - at ../dataset/shuffle-word-3750-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 200 samples - at ../dataset/gen-word-1130-count.jsonl\n",
      "Generated JSONL file with - 2455 max words, 200 samples - at ../dataset/gen-word-2455-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1305 max words - at ../dataset/shuffle-word-1305-count.jsonl\n",
      "Generated JSONL file with - 2650 max words, 200 samples - at ../dataset/gen-word-2650-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1155 max words - at ../dataset/shuffle-word-1155-count.jsonl\n",
      "Generated JSONL file with - 1465 max words, 200 samples - at ../dataset/gen-word-1465-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1165 max words - at ../dataset/shuffle-word-1165-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 200 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 200 samples - at ../dataset/gen-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2025 max words - at ../dataset/shuffle-word-2025-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2080 max words - at ../dataset/shuffle-word-2080-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 200 samples - at ../dataset/gen-word-1990-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 200 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 200 samples - at ../dataset/gen-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1790 max words - at ../dataset/shuffle-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1915 max words, 200 samples - at ../dataset/gen-word-1915-count.jsonl\n",
      "Generated JSONL file with - 2225 max words, 200 samples - at ../dataset/gen-word-2225-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 200 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1695 max words - at ../dataset/shuffle-word-1695-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2620 max words - at ../dataset/shuffle-word-2620-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1830 max words - at ../dataset/shuffle-word-1830-count.jsonl\n",
      "Generated JSONL file with - 2160 max words, 200 samples - at ../dataset/gen-word-2160-count.jsonl\n",
      "Generated JSONL file with - 2020 max words, 200 samples - at ../dataset/gen-word-2020-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 200 samples - at ../dataset/gen-word-1430-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1990 max words - at ../dataset/shuffle-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 200 samples - at ../dataset/gen-word-1210-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 200 samples - at ../dataset/gen-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3935 max words - at ../dataset/shuffle-word-3935-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2060 max words - at ../dataset/shuffle-word-2060-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 200 samples - at ../dataset/gen-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3915 max words - at ../dataset/shuffle-word-3915-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1905 max words - at ../dataset/shuffle-word-1905-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 200 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 2075 max words, 200 samples - at ../dataset/gen-word-2075-count.jsonl\n",
      "Generated a single JSONL file with 49 samples (20 token repeat) - 1205 max words - at ../dataset/shuffle-word-1205-count.jsonl\n",
      "Generated JSONL file with - 3005 max words, 200 samples - at ../dataset/gen-word-3005-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 200 samples - at ../dataset/gen-word-1230-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 200 samples - at ../dataset/gen-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2135 max words - at ../dataset/shuffle-word-2135-count.jsonl\n",
      "Generated JSONL file with - 2215 max words, 200 samples - at ../dataset/gen-word-2215-count.jsonl\n",
      "Generated JSONL file with - 2055 max words, 200 samples - at ../dataset/gen-word-2055-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 200 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 200 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated a single JSONL file with 52 samples (20 token repeat) - 1260 max words - at ../dataset/shuffle-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3895 max words - at ../dataset/shuffle-word-3895-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3640 max words - at ../dataset/shuffle-word-3640-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 200 samples - at ../dataset/gen-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2150 max words - at ../dataset/shuffle-word-2150-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 200 samples - at ../dataset/gen-word-2100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3890 max words - at ../dataset/shuffle-word-3890-count.jsonl\n",
      "Generated a single JSONL file with 36 samples (20 token repeat) - 2455 max words - at ../dataset/shuffle-word-2455-count.jsonl\n",
      "Generated JSONL file with - 2615 max words, 200 samples - at ../dataset/gen-word-2615-count.jsonl\n",
      "Generated JSONL file with - 1085 max words, 200 samples - at ../dataset/gen-word-1085-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3700 max words - at ../dataset/shuffle-word-3700-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 200 samples - at ../dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 1565 max words, 200 samples - at ../dataset/gen-word-1565-count.jsonl\n",
      "Generated JSONL file with - 1825 max words, 200 samples - at ../dataset/gen-word-1825-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3635 max words - at ../dataset/shuffle-word-3635-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1910 max words - at ../dataset/shuffle-word-1910-count.jsonl\n",
      "Generated JSONL file with - 2525 max words, 200 samples - at ../dataset/gen-word-2525-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1415 max words - at ../dataset/shuffle-word-1415-count.jsonl\n",
      "Generated JSONL file with - 2550 max words, 200 samples - at ../dataset/gen-word-2550-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1655 max words - at ../dataset/shuffle-word-1655-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1860 max words - at ../dataset/shuffle-word-1860-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3605 max words - at ../dataset/shuffle-word-3605-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3740 max words - at ../dataset/shuffle-word-3740-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 200 samples - at ../dataset/gen-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (20 token repeat) - 2505 max words - at ../dataset/shuffle-word-2505-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1720 max words - at ../dataset/shuffle-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 200 samples - at ../dataset/gen-word-1360-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2075 max words - at ../dataset/shuffle-word-2075-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1870 max words - at ../dataset/shuffle-word-1870-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1845 max words - at ../dataset/shuffle-word-1845-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 200 samples - at ../dataset/gen-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1970 max words - at ../dataset/shuffle-word-1970-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 200 samples - at ../dataset/gen-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3825 max words - at ../dataset/shuffle-word-3825-count.jsonl\n",
      "Generated JSONL file with - 2085 max words, 200 samples - at ../dataset/gen-word-2085-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated JSONL file with - 2130 max words, 200 samples - at ../dataset/gen-word-2130-count.jsonl\n",
      "Generated JSONL file with - 1265 max words, 200 samples - at ../dataset/gen-word-1265-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1960 max words - at ../dataset/shuffle-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 30 samples (20 token repeat) - 2515 max words - at ../dataset/shuffle-word-2515-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2050 max words - at ../dataset/shuffle-word-2050-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2100 max words - at ../dataset/shuffle-word-2100-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2040 max words - at ../dataset/shuffle-word-2040-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (20 token repeat) - 2525 max words - at ../dataset/shuffle-word-2525-count.jsonl\n",
      "Generated JSONL file with - 1495 max words, 200 samples - at ../dataset/gen-word-1495-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (20 token repeat) - 2520 max words - at ../dataset/shuffle-word-2520-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1890 max words - at ../dataset/shuffle-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2475 max words - at ../dataset/shuffle-word-2475-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2305 max words - at ../dataset/shuffle-word-2305-count.jsonl\n",
      "Generated JSONL file with - 2315 max words, 200 samples - at ../dataset/gen-word-2315-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2230 max words - at ../dataset/shuffle-word-2230-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3975 max words - at ../dataset/shuffle-word-3975-count.jsonl\n",
      "Generated JSONL file with - 2205 max words, 200 samples - at ../dataset/gen-word-2205-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (20 token repeat) - 2550 max words - at ../dataset/shuffle-word-2550-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (20 token repeat) - 2600 max words - at ../dataset/shuffle-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 36 samples (20 token repeat) - 2480 max words - at ../dataset/shuffle-word-2480-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1940 max words - at ../dataset/shuffle-word-1940-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2690 max words - at ../dataset/shuffle-word-2690-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 200 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1980 max words - at ../dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1730 max words - at ../dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3980 max words - at ../dataset/shuffle-word-3980-count.jsonl\n",
      "Generated JSONL file with - 1955 max words, 200 samples - at ../dataset/gen-word-1955-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1565 max words - at ../dataset/shuffle-word-1565-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 200 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2740 max words - at ../dataset/shuffle-word-2740-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2705 max words - at ../dataset/shuffle-word-2705-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2680 max words - at ../dataset/shuffle-word-2680-count.jsonl\n",
      "Generated JSONL file with - 1395 max words, 200 samples - at ../dataset/gen-word-1395-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2710 max words - at ../dataset/shuffle-word-2710-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2745 max words - at ../dataset/shuffle-word-2745-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2880 max words - at ../dataset/shuffle-word-2880-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2310 max words - at ../dataset/shuffle-word-2310-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2700 max words - at ../dataset/shuffle-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1705 max words - at ../dataset/shuffle-word-1705-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (20 token repeat) - 2585 max words - at ../dataset/shuffle-word-2585-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2815 max words - at ../dataset/shuffle-word-2815-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 200 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2755 max words - at ../dataset/shuffle-word-2755-count.jsonl\n",
      "Generated JSONL file with - 3380 max words, 200 samples - at ../dataset/gen-word-3380-count.jsonl\n",
      "Generated a single JSONL file with 24 samples (20 token repeat) - 2640 max words - at ../dataset/shuffle-word-2640-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2760 max words - at ../dataset/shuffle-word-2760-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2810 max words - at ../dataset/shuffle-word-2810-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2565 max words - at ../dataset/shuffle-word-2565-count.jsonl\n",
      "Generated JSONL file with - 2560 max words, 200 samples - at ../dataset/gen-word-2560-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (20 token repeat) - 2575 max words - at ../dataset/shuffle-word-2575-count.jsonl\n",
      "Generated JSONL file with - 2170 max words, 200 samples - at ../dataset/gen-word-2170-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2805 max words - at ../dataset/shuffle-word-2805-count.jsonl\n",
      "Generated a single JSONL file with 33 samples (20 token repeat) - 2580 max words - at ../dataset/shuffle-word-2580-count.jsonl\n",
      "Generated JSONL file with - 2165 max words, 200 samples - at ../dataset/gen-word-2165-count.jsonl\n",
      "Generated JSONL file with - 1305 max words, 200 samples - at ../dataset/gen-word-1305-count.jsonl\n",
      "Generated JSONL file with - 2150 max words, 200 samples - at ../dataset/gen-word-2150-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2715 max words - at ../dataset/shuffle-word-2715-count.jsonl\n",
      "Generated JSONL file with - 2105 max words, 200 samples - at ../dataset/gen-word-2105-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2830 max words - at ../dataset/shuffle-word-2830-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 200 samples - at ../dataset/gen-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2780 max words - at ../dataset/shuffle-word-2780-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2800 max words - at ../dataset/shuffle-word-2800-count.jsonl\n",
      "Generated JSONL file with - 1505 max words, 200 samples - at ../dataset/gen-word-1505-count.jsonl\n",
      "Generated a single JSONL file with 22 samples (20 token repeat) - 2725 max words - at ../dataset/shuffle-word-2725-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 200 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 3265 max words, 200 samples - at ../dataset/gen-word-3265-count.jsonl\n",
      "Generated JSONL file with - 2485 max words, 200 samples - at ../dataset/gen-word-2485-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2795 max words - at ../dataset/shuffle-word-2795-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (20 token repeat) - 2615 max words - at ../dataset/shuffle-word-2615-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 200 samples - at ../dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 1675 max words, 200 samples - at ../dataset/gen-word-1675-count.jsonl\n",
      "Generated JSONL file with - 1255 max words, 200 samples - at ../dataset/gen-word-1255-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2890 max words - at ../dataset/shuffle-word-2890-count.jsonl\n",
      "Generated JSONL file with - 2155 max words, 200 samples - at ../dataset/gen-word-2155-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2765 max words - at ../dataset/shuffle-word-2765-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2770 max words - at ../dataset/shuffle-word-2770-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 200 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated a single JSONL file with 35 samples (20 token repeat) - 2425 max words - at ../dataset/shuffle-word-2425-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 200 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated a single JSONL file with 25 samples (20 token repeat) - 2650 max words - at ../dataset/shuffle-word-2650-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 200 samples - at ../dataset/gen-word-1170-count.jsonl\n",
      "Generated a single JSONL file with 25 samples (20 token repeat) - 2655 max words - at ../dataset/shuffle-word-2655-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2720 max words - at ../dataset/shuffle-word-2720-count.jsonl\n",
      "Generated JSONL file with - 1235 max words, 200 samples - at ../dataset/gen-word-1235-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2935 max words - at ../dataset/shuffle-word-2935-count.jsonl\n",
      "Generated JSONL file with - 1335 max words, 200 samples - at ../dataset/gen-word-1335-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2845 max words - at ../dataset/shuffle-word-2845-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2750 max words - at ../dataset/shuffle-word-2750-count.jsonl\n",
      "Generated JSONL file with - 2255 max words, 200 samples - at ../dataset/gen-word-2255-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2735 max words - at ../dataset/shuffle-word-2735-count.jsonl\n",
      "Generated a single JSONL file with 24 samples (20 token repeat) - 2685 max words - at ../dataset/shuffle-word-2685-count.jsonl\n",
      "Generated JSONL file with - 2510 max words, 200 samples - at ../dataset/gen-word-2510-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2395 max words - at ../dataset/shuffle-word-2395-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2970 max words - at ../dataset/shuffle-word-2970-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2785 max words - at ../dataset/shuffle-word-2785-count.jsonl\n",
      "Generated JSONL file with - 1225 max words, 200 samples - at ../dataset/gen-word-1225-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 200 samples - at ../dataset/gen-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1575 max words, 200 samples - at ../dataset/gen-word-1575-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 200 samples - at ../dataset/gen-word-1320-count.jsonl\n",
      "Generated JSONL file with - 2095 max words, 200 samples - at ../dataset/gen-word-2095-count.jsonl\n",
      "Generated JSONL file with - 2230 max words, 200 samples - at ../dataset/gen-word-2230-count.jsonl\n",
      "Generated a single JSONL file with 37 samples (20 token repeat) - 2470 max words - at ../dataset/shuffle-word-2470-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2730 max words - at ../dataset/shuffle-word-2730-count.jsonl\n",
      "Generated JSONL file with - 2555 max words, 200 samples - at ../dataset/gen-word-2555-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2870 max words - at ../dataset/shuffle-word-2870-count.jsonl\n",
      "Generated JSONL file with - 3430 max words, 200 samples - at ../dataset/gen-word-3430-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 200 samples - at ../dataset/gen-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2875 max words - at ../dataset/shuffle-word-2875-count.jsonl\n",
      "Generated JSONL file with - 3980 max words, 200 samples - at ../dataset/gen-word-3980-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2920 max words - at ../dataset/shuffle-word-2920-count.jsonl\n",
      "Generated JSONL file with - 2375 max words, 200 samples - at ../dataset/gen-word-2375-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2850 max words - at ../dataset/shuffle-word-2850-count.jsonl\n",
      "Generated JSONL file with - 1285 max words, 200 samples - at ../dataset/gen-word-1285-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2855 max words - at ../dataset/shuffle-word-2855-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (20 token repeat) - 2535 max words - at ../dataset/shuffle-word-2535-count.jsonl\n",
      "Generated a single JSONL file with 21 samples (20 token repeat) - 2775 max words - at ../dataset/shuffle-word-2775-count.jsonl\n",
      "Generated JSONL file with - 3750 max words, 200 samples - at ../dataset/gen-word-3750-count.jsonl\n",
      "Generated JSONL file with - 2320 max words, 200 samples - at ../dataset/gen-word-2320-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3160 max words - at ../dataset/shuffle-word-3160-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 200 samples - at ../dataset/gen-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1045 max words, 200 samples - at ../dataset/gen-word-1045-count.jsonl\n",
      "Generated a single JSONL file with 27 samples (20 token repeat) - 2530 max words - at ../dataset/shuffle-word-2530-count.jsonl\n",
      "Generated JSONL file with - 1055 max words, 200 samples - at ../dataset/gen-word-1055-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2980 max words - at ../dataset/shuffle-word-2980-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 200 samples - at ../dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 3775 max words, 200 samples - at ../dataset/gen-word-3775-count.jsonl\n",
      "Generated JSONL file with - 3205 max words, 200 samples - at ../dataset/gen-word-3205-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 200 samples - at ../dataset/gen-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2955 max words - at ../dataset/shuffle-word-2955-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2665 max words - at ../dataset/shuffle-word-2665-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 200 samples - at ../dataset/gen-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1715 max words, 200 samples - at ../dataset/gen-word-1715-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2670 max words - at ../dataset/shuffle-word-2670-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 200 samples - at ../dataset/gen-word-1960-count.jsonl\n",
      "Generated JSONL file with - 2410 max words, 200 samples - at ../dataset/gen-word-2410-count.jsonl\n",
      "Generated JSONL file with - 1105 max words, 200 samples - at ../dataset/gen-word-1105-count.jsonl\n",
      "Generated JSONL file with - 1635 max words, 200 samples - at ../dataset/gen-word-1635-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 200 samples - at ../dataset/gen-word-1480-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 200 samples - at ../dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1035 max words, 200 samples - at ../dataset/gen-word-1035-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3020 max words - at ../dataset/shuffle-word-3020-count.jsonl\n",
      "Generated JSONL file with - 2250 max words, 200 samples - at ../dataset/gen-word-2250-count.jsonl\n",
      "Generated JSONL file with - 3475 max words, 200 samples - at ../dataset/gen-word-3475-count.jsonl\n",
      "Generated JSONL file with - 1005 max words, 200 samples - at ../dataset/gen-word-1005-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3145 max words - at ../dataset/shuffle-word-3145-count.jsonl\n",
      "Generated JSONL file with - 1785 max words, 200 samples - at ../dataset/gen-word-1785-count.jsonl\n",
      "Generated JSONL file with - 2480 max words, 200 samples - at ../dataset/gen-word-2480-count.jsonl\n",
      "Generated JSONL file with - 2725 max words, 200 samples - at ../dataset/gen-word-2725-count.jsonl\n",
      "Generated JSONL file with - 1615 max words, 200 samples - at ../dataset/gen-word-1615-count.jsonl\n",
      "Generated JSONL file with - 2240 max words, 200 samples - at ../dataset/gen-word-2240-count.jsonl\n",
      "Generated JSONL file with - 1685 max words, 200 samples - at ../dataset/gen-word-1685-count.jsonl\n",
      "Generated JSONL file with - 1655 max words, 200 samples - at ../dataset/gen-word-1655-count.jsonl\n",
      "Generated JSONL file with - 2515 max words, 200 samples - at ../dataset/gen-word-2515-count.jsonl\n",
      "Generated JSONL file with - 1135 max words, 200 samples - at ../dataset/gen-word-1135-count.jsonl\n",
      "Generated JSONL file with - 3270 max words, 200 samples - at ../dataset/gen-word-3270-count.jsonl\n",
      "Generated JSONL file with - 2275 max words, 200 samples - at ../dataset/gen-word-2275-count.jsonl\n",
      "Generated JSONL file with - 1185 max words, 200 samples - at ../dataset/gen-word-1185-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2910 max words - at ../dataset/shuffle-word-2910-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2945 max words - at ../dataset/shuffle-word-2945-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3195 max words - at ../dataset/shuffle-word-3195-count.jsonl\n",
      "Generated JSONL file with - 2445 max words, 200 samples - at ../dataset/gen-word-2445-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2930 max words - at ../dataset/shuffle-word-2930-count.jsonl\n",
      "Generated JSONL file with - 3280 max words, 200 samples - at ../dataset/gen-word-3280-count.jsonl\n",
      "Generated JSONL file with - 2970 max words, 200 samples - at ../dataset/gen-word-2970-count.jsonl\n",
      "Generated a single JSONL file with 23 samples (20 token repeat) - 2605 max words - at ../dataset/shuffle-word-2605-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3040 max words - at ../dataset/shuffle-word-3040-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 200 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated JSONL file with - 1965 max words, 200 samples - at ../dataset/gen-word-1965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2895 max words - at ../dataset/shuffle-word-2895-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3130 max words - at ../dataset/shuffle-word-3130-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2950 max words - at ../dataset/shuffle-word-2950-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 200 samples - at ../dataset/gen-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2960 max words - at ../dataset/shuffle-word-2960-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3015 max words - at ../dataset/shuffle-word-3015-count.jsonl\n",
      "Generated JSONL file with - 2080 max words, 200 samples - at ../dataset/gen-word-2080-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2990 max words - at ../dataset/shuffle-word-2990-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2940 max words - at ../dataset/shuffle-word-2940-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 200 samples - at ../dataset/gen-word-1710-count.jsonl\n",
      "Generated JSONL file with - 3955 max words, 200 samples - at ../dataset/gen-word-3955-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2975 max words - at ../dataset/shuffle-word-2975-count.jsonl\n",
      "Generated JSONL file with - 1705 max words, 200 samples - at ../dataset/gen-word-1705-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3345 max words - at ../dataset/shuffle-word-3345-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2820 max words - at ../dataset/shuffle-word-2820-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 200 samples - at ../dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 3935 max words, 200 samples - at ../dataset/gen-word-3935-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 200 samples - at ../dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 3305 max words, 200 samples - at ../dataset/gen-word-3305-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2885 max words - at ../dataset/shuffle-word-2885-count.jsonl\n",
      "Generated JSONL file with - 1745 max words, 200 samples - at ../dataset/gen-word-1745-count.jsonl\n",
      "Generated JSONL file with - 3485 max words, 200 samples - at ../dataset/gen-word-3485-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 200 samples - at ../dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 2235 max words, 200 samples - at ../dataset/gen-word-2235-count.jsonl\n",
      "Generated JSONL file with - 1975 max words, 200 samples - at ../dataset/gen-word-1975-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 200 samples - at ../dataset/gen-word-1590-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 200 samples - at ../dataset/gen-word-1720-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3070 max words - at ../dataset/shuffle-word-3070-count.jsonl\n",
      "Generated JSONL file with - 3135 max words, 200 samples - at ../dataset/gen-word-3135-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3065 max words - at ../dataset/shuffle-word-3065-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3025 max words - at ../dataset/shuffle-word-3025-count.jsonl\n",
      "Generated JSONL file with - 3495 max words, 200 samples - at ../dataset/gen-word-3495-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 200 samples - at ../dataset/gen-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2625 max words - at ../dataset/shuffle-word-2625-count.jsonl\n",
      "Generated JSONL file with - 3070 max words, 200 samples - at ../dataset/gen-word-3070-count.jsonl\n",
      "Generated JSONL file with - 2195 max words, 200 samples - at ../dataset/gen-word-2195-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (20 token repeat) - 2595 max words - at ../dataset/shuffle-word-2595-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 200 samples - at ../dataset/gen-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3315 max words - at ../dataset/shuffle-word-3315-count.jsonl\n",
      "Generated JSONL file with - 3145 max words, 200 samples - at ../dataset/gen-word-3145-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 200 samples - at ../dataset/gen-word-1740-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 200 samples - at ../dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1195 max words, 200 samples - at ../dataset/gen-word-1195-count.jsonl\n",
      "Generated JSONL file with - 2280 max words, 200 samples - at ../dataset/gen-word-2280-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3035 max words - at ../dataset/shuffle-word-3035-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3200 max words - at ../dataset/shuffle-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 26 samples (20 token repeat) - 2590 max words - at ../dataset/shuffle-word-2590-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 200 samples - at ../dataset/gen-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2900 max words - at ../dataset/shuffle-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2945 max words, 200 samples - at ../dataset/gen-word-2945-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2865 max words - at ../dataset/shuffle-word-2865-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3060 max words - at ../dataset/shuffle-word-3060-count.jsonl\n",
      "Generated JSONL file with - 2435 max words, 200 samples - at ../dataset/gen-word-2435-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3050 max words - at ../dataset/shuffle-word-3050-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3030 max words - at ../dataset/shuffle-word-3030-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 200 samples - at ../dataset/gen-word-1950-count.jsonl\n",
      "Generated JSONL file with - 1155 max words, 200 samples - at ../dataset/gen-word-1155-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2905 max words - at ../dataset/shuffle-word-2905-count.jsonl\n",
      "Generated JSONL file with - 2645 max words, 200 samples - at ../dataset/gen-word-2645-count.jsonl\n",
      "Generated JSONL file with - 1665 max words, 200 samples - at ../dataset/gen-word-1665-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3360 max words - at ../dataset/shuffle-word-3360-count.jsonl\n",
      "Generated JSONL file with - 3260 max words, 200 samples - at ../dataset/gen-word-3260-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3140 max words - at ../dataset/shuffle-word-3140-count.jsonl\n",
      "Generated JSONL file with - 2050 max words, 200 samples - at ../dataset/gen-word-2050-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3400 max words - at ../dataset/shuffle-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2965 max words - at ../dataset/shuffle-word-2965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3940 max words - at ../dataset/shuffle-word-3940-count.jsonl\n",
      "Generated JSONL file with - 1925 max words, 200 samples - at ../dataset/gen-word-1925-count.jsonl\n",
      "Generated JSONL file with - 3530 max words, 200 samples - at ../dataset/gen-word-3530-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3085 max words - at ../dataset/shuffle-word-3085-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 200 samples - at ../dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 2175 max words, 200 samples - at ../dataset/gen-word-2175-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3580 max words - at ../dataset/shuffle-word-3580-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3410 max words - at ../dataset/shuffle-word-3410-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3755 max words - at ../dataset/shuffle-word-3755-count.jsonl\n",
      "Generated JSONL file with - 3125 max words, 200 samples - at ../dataset/gen-word-3125-count.jsonl\n",
      "Generated JSONL file with - 2090 max words, 200 samples - at ../dataset/gen-word-2090-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3055 max words - at ../dataset/shuffle-word-3055-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3590 max words - at ../dataset/shuffle-word-3590-count.jsonl\n",
      "Generated JSONL file with - 2125 max words, 200 samples - at ../dataset/gen-word-2125-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3545 max words - at ../dataset/shuffle-word-3545-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3540 max words - at ../dataset/shuffle-word-3540-count.jsonl\n",
      "Generated JSONL file with - 3045 max words, 200 samples - at ../dataset/gen-word-3045-count.jsonl\n",
      "Generated JSONL file with - 2190 max words, 200 samples - at ../dataset/gen-word-2190-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3115 max words - at ../dataset/shuffle-word-3115-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3365 max words - at ../dataset/shuffle-word-3365-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3005 max words - at ../dataset/shuffle-word-3005-count.jsonl\n",
      "Generated JSONL file with - 2470 max words, 200 samples - at ../dataset/gen-word-2470-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3715 max words - at ../dataset/shuffle-word-3715-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2925 max words - at ../dataset/shuffle-word-2925-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3185 max words - at ../dataset/shuffle-word-3185-count.jsonl\n",
      "Generated JSONL file with - 2380 max words, 200 samples - at ../dataset/gen-word-2380-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3045 max words - at ../dataset/shuffle-word-3045-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3805 max words - at ../dataset/shuffle-word-3805-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3780 max words - at ../dataset/shuffle-word-3780-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 200 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 2070 max words, 200 samples - at ../dataset/gen-word-2070-count.jsonl\n",
      "Generated JSONL file with - 1365 max words, 200 samples - at ../dataset/gen-word-1365-count.jsonl\n",
      "Generated JSONL file with - 3140 max words, 200 samples - at ../dataset/gen-word-3140-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3770 max words - at ../dataset/shuffle-word-3770-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3415 max words - at ../dataset/shuffle-word-3415-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3370 max words - at ../dataset/shuffle-word-3370-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3560 max words - at ../dataset/shuffle-word-3560-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 200 samples - at ../dataset/gen-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3775 max words - at ../dataset/shuffle-word-3775-count.jsonl\n",
      "Generated JSONL file with - 2180 max words, 200 samples - at ../dataset/gen-word-2180-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 200 samples - at ../dataset/gen-word-1270-count.jsonl\n",
      "Generated JSONL file with - 3400 max words, 200 samples - at ../dataset/gen-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3010 max words - at ../dataset/shuffle-word-3010-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2825 max words - at ../dataset/shuffle-word-2825-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3330 max words - at ../dataset/shuffle-word-3330-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3705 max words - at ../dataset/shuffle-word-3705-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3190 max words - at ../dataset/shuffle-word-3190-count.jsonl\n",
      "Generated JSONL file with - 2385 max words, 200 samples - at ../dataset/gen-word-2385-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3405 max words - at ../dataset/shuffle-word-3405-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3180 max words - at ../dataset/shuffle-word-3180-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3090 max words - at ../dataset/shuffle-word-3090-count.jsonl\n",
      "Generated JSONL file with - 2065 max words, 200 samples - at ../dataset/gen-word-2065-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2985 max words - at ../dataset/shuffle-word-2985-count.jsonl\n",
      "Generated JSONL file with - 3225 max words, 200 samples - at ../dataset/gen-word-3225-count.jsonl\n",
      "Generated JSONL file with - 1845 max words, 200 samples - at ../dataset/gen-word-1845-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3815 max words - at ../dataset/shuffle-word-3815-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3285 max words - at ../dataset/shuffle-word-3285-count.jsonl\n",
      "Generated JSONL file with - 2920 max words, 200 samples - at ../dataset/gen-word-2920-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3565 max words - at ../dataset/shuffle-word-3565-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 200 samples - at ../dataset/gen-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3325 max words - at ../dataset/shuffle-word-3325-count.jsonl\n",
      "Generated JSONL file with - 1405 max words, 200 samples - at ../dataset/gen-word-1405-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2860 max words - at ../dataset/shuffle-word-2860-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3225 max words - at ../dataset/shuffle-word-3225-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 200 samples - at ../dataset/gen-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3075 max words - at ../dataset/shuffle-word-3075-count.jsonl\n",
      "Generated JSONL file with - 1985 max words, 200 samples - at ../dataset/gen-word-1985-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3860 max words - at ../dataset/shuffle-word-3860-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3350 max words - at ../dataset/shuffle-word-3350-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3000 max words - at ../dataset/shuffle-word-3000-count.jsonl\n",
      "Generated JSONL file with - 2440 max words, 200 samples - at ../dataset/gen-word-2440-count.jsonl\n",
      "Generated JSONL file with - 2265 max words, 200 samples - at ../dataset/gen-word-2265-count.jsonl\n",
      "Generated JSONL file with - 2955 max words, 200 samples - at ../dataset/gen-word-2955-count.jsonl\n",
      "Generated JSONL file with - 2060 max words, 200 samples - at ../dataset/gen-word-2060-count.jsonl\n",
      "Generated JSONL file with - 1905 max words, 200 samples - at ../dataset/gen-word-1905-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 200 samples - at ../dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 3150 max words, 200 samples - at ../dataset/gen-word-3150-count.jsonl\n",
      "Generated JSONL file with - 1885 max words, 200 samples - at ../dataset/gen-word-1885-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3790 max words - at ../dataset/shuffle-word-3790-count.jsonl\n",
      "Generated JSONL file with - 3605 max words, 200 samples - at ../dataset/gen-word-3605-count.jsonl\n",
      "Generated JSONL file with - 2980 max words, 200 samples - at ../dataset/gen-word-2980-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3820 max words - at ../dataset/shuffle-word-3820-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3855 max words - at ../dataset/shuffle-word-3855-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3375 max words - at ../dataset/shuffle-word-3375-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3310 max words - at ../dataset/shuffle-word-3310-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3905 max words - at ../dataset/shuffle-word-3905-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3810 max words - at ../dataset/shuffle-word-3810-count.jsonl\n",
      "Generated JSONL file with - 2425 max words, 200 samples - at ../dataset/gen-word-2425-count.jsonl\n",
      "Generated JSONL file with - 1625 max words, 200 samples - at ../dataset/gen-word-1625-count.jsonl\n",
      "Generated JSONL file with - 2340 max words, 200 samples - at ../dataset/gen-word-2340-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3835 max words - at ../dataset/shuffle-word-3835-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3965 max words - at ../dataset/shuffle-word-3965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3950 max words - at ../dataset/shuffle-word-3950-count.jsonl\n",
      "Generated JSONL file with - 3455 max words, 200 samples - at ../dataset/gen-word-3455-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3845 max words - at ../dataset/shuffle-word-3845-count.jsonl\n",
      "Generated JSONL file with - 3810 max words, 200 samples - at ../dataset/gen-word-3810-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3960 max words - at ../dataset/shuffle-word-3960-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3930 max words - at ../dataset/shuffle-word-3930-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 200 samples - at ../dataset/gen-word-1920-count.jsonl\n",
      "Generated JSONL file with - 3615 max words, 200 samples - at ../dataset/gen-word-3615-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3945 max words - at ../dataset/shuffle-word-3945-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3985 max words - at ../dataset/shuffle-word-3985-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 200 samples - at ../dataset/gen-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 200 samples - at ../dataset/gen-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4000 max words - at ../dataset/shuffle-word-4000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3995 max words - at ../dataset/shuffle-word-3995-count.jsonl\n",
      "Generated JSONL file with - 1835 max words, 200 samples - at ../dataset/gen-word-1835-count.jsonl\n",
      "Generated JSONL file with - 3220 max words, 200 samples - at ../dataset/gen-word-3220-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 200 samples - at ../dataset/gen-word-1880-count.jsonl\n",
      "Generated JSONL file with - 2135 max words, 200 samples - at ../dataset/gen-word-2135-count.jsonl\n",
      "Generated JSONL file with - 2805 max words, 200 samples - at ../dataset/gen-word-2805-count.jsonl\n",
      "Generated JSONL file with - 3325 max words, 200 samples - at ../dataset/gen-word-3325-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 200 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3850 max words - at ../dataset/shuffle-word-3850-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 200 samples - at ../dataset/gen-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3925 max words - at ../dataset/shuffle-word-3925-count.jsonl\n",
      "Generated JSONL file with - 3995 max words, 200 samples - at ../dataset/gen-word-3995-count.jsonl\n",
      "Generated JSONL file with - 3175 max words, 200 samples - at ../dataset/gen-word-3175-count.jsonl\n",
      "Generated JSONL file with - 2450 max words, 200 samples - at ../dataset/gen-word-2450-count.jsonl\n",
      "Generated JSONL file with - 3285 max words, 200 samples - at ../dataset/gen-word-3285-count.jsonl\n",
      "Generated JSONL file with - 2765 max words, 200 samples - at ../dataset/gen-word-2765-count.jsonl\n",
      "Generated JSONL file with - 2835 max words, 200 samples - at ../dataset/gen-word-2835-count.jsonl\n",
      "Generated JSONL file with - 3410 max words, 200 samples - at ../dataset/gen-word-3410-count.jsonl\n",
      "Generated JSONL file with - 3245 max words, 200 samples - at ../dataset/gen-word-3245-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3795 max words - at ../dataset/shuffle-word-3795-count.jsonl\n",
      "Generated JSONL file with - 3730 max words, 200 samples - at ../dataset/gen-word-3730-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 200 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 2595 max words, 200 samples - at ../dataset/gen-word-2595-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3875 max words - at ../dataset/shuffle-word-3875-count.jsonl\n",
      "Generated JSONL file with - 2885 max words, 200 samples - at ../dataset/gen-word-2885-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3840 max words - at ../dataset/shuffle-word-3840-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3870 max words - at ../dataset/shuffle-word-3870-count.jsonl\n",
      "Generated JSONL file with - 3420 max words, 200 samples - at ../dataset/gen-word-3420-count.jsonl\n",
      "Generated JSONL file with - 2995 max words, 200 samples - at ../dataset/gen-word-2995-count.jsonl\n",
      "Generated JSONL file with - 2005 max words, 200 samples - at ../dataset/gen-word-2005-count.jsonl\n",
      "Generated JSONL file with - 3650 max words, 200 samples - at ../dataset/gen-word-3650-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3955 max words - at ../dataset/shuffle-word-3955-count.jsonl\n",
      "Generated JSONL file with - 3440 max words, 200 samples - at ../dataset/gen-word-3440-count.jsonl\n",
      "Generated JSONL file with - 2690 max words, 200 samples - at ../dataset/gen-word-2690-count.jsonl\n",
      "Generated JSONL file with - 2475 max words, 200 samples - at ../dataset/gen-word-2475-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3865 max words - at ../dataset/shuffle-word-3865-count.jsonl\n",
      "Generated JSONL file with - 2585 max words, 200 samples - at ../dataset/gen-word-2585-count.jsonl\n",
      "Generated JSONL file with - 3545 max words, 200 samples - at ../dataset/gen-word-3545-count.jsonl\n",
      "Generated JSONL file with - 2735 max words, 200 samples - at ../dataset/gen-word-2735-count.jsonl\n",
      "Generated JSONL file with - 2630 max words, 200 samples - at ../dataset/gen-word-2630-count.jsonl\n",
      "Generated JSONL file with - 3060 max words, 200 samples - at ../dataset/gen-word-3060-count.jsonl\n",
      "Generated JSONL file with - 3080 max words, 200 samples - at ../dataset/gen-word-3080-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3885 max words - at ../dataset/shuffle-word-3885-count.jsonl\n",
      "Generated JSONL file with - 3500 max words, 200 samples - at ../dataset/gen-word-3500-count.jsonl\n",
      "Generated JSONL file with - 2755 max words, 200 samples - at ../dataset/gen-word-2755-count.jsonl\n",
      "Generated JSONL file with - 2570 max words, 200 samples - at ../dataset/gen-word-2570-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3970 max words - at ../dataset/shuffle-word-3970-count.jsonl\n",
      "Generated JSONL file with - 3635 max words, 200 samples - at ../dataset/gen-word-3635-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3880 max words - at ../dataset/shuffle-word-3880-count.jsonl\n",
      "Generated JSONL file with - 3620 max words, 200 samples - at ../dataset/gen-word-3620-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3990 max words - at ../dataset/shuffle-word-3990-count.jsonl\n",
      "Generated JSONL file with - 2780 max words, 200 samples - at ../dataset/gen-word-2780-count.jsonl\n",
      "Generated JSONL file with - 2355 max words, 200 samples - at ../dataset/gen-word-2355-count.jsonl\n",
      "Generated JSONL file with - 2490 max words, 200 samples - at ../dataset/gen-word-2490-count.jsonl\n",
      "Generated JSONL file with - 2680 max words, 200 samples - at ../dataset/gen-word-2680-count.jsonl\n",
      "Generated JSONL file with - 3185 max words, 200 samples - at ../dataset/gen-word-3185-count.jsonl\n",
      "Generated JSONL file with - 2045 max words, 200 samples - at ../dataset/gen-word-2045-count.jsonl\n",
      "Generated JSONL file with - 3100 max words, 200 samples - at ../dataset/gen-word-3100-count.jsonl\n",
      "Generated JSONL file with - 3160 max words, 200 samples - at ../dataset/gen-word-3160-count.jsonl\n",
      "Generated JSONL file with - 3950 max words, 200 samples - at ../dataset/gen-word-3950-count.jsonl\n",
      "Generated JSONL file with - 2575 max words, 200 samples - at ../dataset/gen-word-2575-count.jsonl\n",
      "Generated JSONL file with - 3105 max words, 200 samples - at ../dataset/gen-word-3105-count.jsonl\n",
      "Generated JSONL file with - 3490 max words, 200 samples - at ../dataset/gen-word-3490-count.jsonl\n",
      "Generated JSONL file with - 2770 max words, 200 samples - at ../dataset/gen-word-2770-count.jsonl\n",
      "Generated JSONL file with - 3910 max words, 200 samples - at ../dataset/gen-word-3910-count.jsonl\n",
      "Generated JSONL file with - 3670 max words, 200 samples - at ../dataset/gen-word-3670-count.jsonl\n",
      "Generated JSONL file with - 3675 max words, 200 samples - at ../dataset/gen-word-3675-count.jsonl\n",
      "Generated JSONL file with - 2760 max words, 200 samples - at ../dataset/gen-word-2760-count.jsonl\n",
      "Generated JSONL file with - 3660 max words, 200 samples - at ../dataset/gen-word-3660-count.jsonl\n",
      "Generated JSONL file with - 1995 max words, 200 samples - at ../dataset/gen-word-1995-count.jsonl\n",
      "Generated JSONL file with - 3050 max words, 200 samples - at ../dataset/gen-word-3050-count.jsonl\n",
      "Generated JSONL file with - 2820 max words, 200 samples - at ../dataset/gen-word-2820-count.jsonl\n",
      "Generated JSONL file with - 3315 max words, 200 samples - at ../dataset/gen-word-3315-count.jsonl\n",
      "Generated JSONL file with - 2925 max words, 200 samples - at ../dataset/gen-word-2925-count.jsonl\n",
      "Generated JSONL file with - 1895 max words, 200 samples - at ../dataset/gen-word-1895-count.jsonl\n",
      "Generated JSONL file with - 2640 max words, 200 samples - at ../dataset/gen-word-2640-count.jsonl\n",
      "Generated JSONL file with - 3600 max words, 200 samples - at ../dataset/gen-word-3600-count.jsonl\n",
      "Generated JSONL file with - 2430 max words, 200 samples - at ../dataset/gen-word-2430-count.jsonl\n",
      "Generated JSONL file with - 3725 max words, 200 samples - at ../dataset/gen-word-3725-count.jsonl\n",
      "Generated JSONL file with - 3510 max words, 200 samples - at ../dataset/gen-word-3510-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 200 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 2890 max words, 200 samples - at ../dataset/gen-word-2890-count.jsonl\n",
      "Generated JSONL file with - 3685 max words, 200 samples - at ../dataset/gen-word-3685-count.jsonl\n",
      "Generated JSONL file with - 2350 max words, 200 samples - at ../dataset/gen-word-2350-count.jsonl\n",
      "Generated JSONL file with - 3565 max words, 200 samples - at ../dataset/gen-word-3565-count.jsonl\n",
      "Generated JSONL file with - 2850 max words, 200 samples - at ../dataset/gen-word-2850-count.jsonl\n",
      "Generated JSONL file with - 3785 max words, 200 samples - at ../dataset/gen-word-3785-count.jsonl\n",
      "Generated JSONL file with - 3345 max words, 200 samples - at ../dataset/gen-word-3345-count.jsonl\n",
      "Generated JSONL file with - 3535 max words, 200 samples - at ../dataset/gen-word-3535-count.jsonl\n",
      "Generated JSONL file with - 3820 max words, 200 samples - at ../dataset/gen-word-3820-count.jsonl\n",
      "Generated JSONL file with - 2860 max words, 200 samples - at ../dataset/gen-word-2860-count.jsonl\n",
      "Generated JSONL file with - 2830 max words, 200 samples - at ../dataset/gen-word-2830-count.jsonl\n",
      "Generated JSONL file with - 2600 max words, 200 samples - at ../dataset/gen-word-2600-count.jsonl\n",
      "Generated JSONL file with - 2015 max words, 200 samples - at ../dataset/gen-word-2015-count.jsonl\n",
      "Generated JSONL file with - 3300 max words, 200 samples - at ../dataset/gen-word-3300-count.jsonl\n",
      "Generated JSONL file with - 2540 max words, 200 samples - at ../dataset/gen-word-2540-count.jsonl\n",
      "Generated JSONL file with - 2580 max words, 200 samples - at ../dataset/gen-word-2580-count.jsonl\n",
      "Generated JSONL file with - 3310 max words, 200 samples - at ../dataset/gen-word-3310-count.jsonl\n",
      "Generated JSONL file with - 2900 max words, 200 samples - at ../dataset/gen-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2025 max words, 200 samples - at ../dataset/gen-word-2025-count.jsonl\n",
      "Generated JSONL file with - 2115 max words, 200 samples - at ../dataset/gen-word-2115-count.jsonl\n",
      "Generated JSONL file with - 2990 max words, 200 samples - at ../dataset/gen-word-2990-count.jsonl\n",
      "Generated JSONL file with - 3120 max words, 200 samples - at ../dataset/gen-word-3120-count.jsonl\n",
      "Generated JSONL file with - 3645 max words, 200 samples - at ../dataset/gen-word-3645-count.jsonl\n",
      "Generated JSONL file with - 3515 max words, 200 samples - at ../dataset/gen-word-3515-count.jsonl\n",
      "Generated JSONL file with - 3925 max words, 200 samples - at ../dataset/gen-word-3925-count.jsonl\n",
      "Generated JSONL file with - 2855 max words, 200 samples - at ../dataset/gen-word-2855-count.jsonl\n",
      "Generated JSONL file with - 3290 max words, 200 samples - at ../dataset/gen-word-3290-count.jsonl\n",
      "Generated JSONL file with - 3095 max words, 200 samples - at ../dataset/gen-word-3095-count.jsonl\n",
      "Generated JSONL file with - 2270 max words, 200 samples - at ../dataset/gen-word-2270-count.jsonl\n",
      "Generated JSONL file with - 2685 max words, 200 samples - at ../dataset/gen-word-2685-count.jsonl\n",
      "Generated JSONL file with - 3865 max words, 200 samples - at ../dataset/gen-word-3865-count.jsonl\n",
      "Generated JSONL file with - 3375 max words, 200 samples - at ../dataset/gen-word-3375-count.jsonl\n",
      "Generated JSONL file with - 2110 max words, 200 samples - at ../dataset/gen-word-2110-count.jsonl\n",
      "Generated JSONL file with - 2040 max words, 200 samples - at ../dataset/gen-word-2040-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 200 samples - at ../dataset/gen-word-2300-count.jsonl\n",
      "Generated JSONL file with - 2715 max words, 200 samples - at ../dataset/gen-word-2715-count.jsonl\n",
      "Generated JSONL file with - 2495 max words, 200 samples - at ../dataset/gen-word-2495-count.jsonl\n",
      "Generated JSONL file with - 2815 max words, 200 samples - at ../dataset/gen-word-2815-count.jsonl\n",
      "Generated JSONL file with - 2520 max words, 200 samples - at ../dataset/gen-word-2520-count.jsonl\n",
      "Generated JSONL file with - 3520 max words, 200 samples - at ../dataset/gen-word-3520-count.jsonl\n",
      "Generated JSONL file with - 3940 max words, 200 samples - at ../dataset/gen-word-3940-count.jsonl\n",
      "Generated JSONL file with - 3695 max words, 200 samples - at ../dataset/gen-word-3695-count.jsonl\n",
      "Generated JSONL file with - 3690 max words, 200 samples - at ../dataset/gen-word-3690-count.jsonl\n",
      "Generated JSONL file with - 2610 max words, 200 samples - at ../dataset/gen-word-2610-count.jsonl\n",
      "Generated JSONL file with - 2720 max words, 200 samples - at ../dataset/gen-word-2720-count.jsonl\n",
      "Generated JSONL file with - 3760 max words, 200 samples - at ../dataset/gen-word-3760-count.jsonl\n",
      "Generated JSONL file with - 3195 max words, 200 samples - at ../dataset/gen-word-3195-count.jsonl\n",
      "Generated JSONL file with - 3770 max words, 200 samples - at ../dataset/gen-word-3770-count.jsonl\n",
      "Generated JSONL file with - 3540 max words, 200 samples - at ../dataset/gen-word-3540-count.jsonl\n",
      "Generated JSONL file with - 3155 max words, 200 samples - at ../dataset/gen-word-3155-count.jsonl\n",
      "Generated JSONL file with - 3210 max words, 200 samples - at ../dataset/gen-word-3210-count.jsonl\n",
      "Generated JSONL file with - 2625 max words, 200 samples - at ../dataset/gen-word-2625-count.jsonl\n",
      "Generated JSONL file with - 3840 max words, 200 samples - at ../dataset/gen-word-3840-count.jsonl\n",
      "Generated JSONL file with - 3665 max words, 200 samples - at ../dataset/gen-word-3665-count.jsonl\n",
      "Generated JSONL file with - 2825 max words, 200 samples - at ../dataset/gen-word-2825-count.jsonl\n",
      "Generated JSONL file with - 3985 max words, 200 samples - at ../dataset/gen-word-3985-count.jsonl\n",
      "Generated JSONL file with - 2910 max words, 200 samples - at ../dataset/gen-word-2910-count.jsonl\n",
      "Generated JSONL file with - 3040 max words, 200 samples - at ../dataset/gen-word-3040-count.jsonl\n",
      "Generated JSONL file with - 2775 max words, 200 samples - at ../dataset/gen-word-2775-count.jsonl\n",
      "Generated JSONL file with - 3230 max words, 200 samples - at ../dataset/gen-word-3230-count.jsonl\n",
      "Generated JSONL file with - 3295 max words, 200 samples - at ../dataset/gen-word-3295-count.jsonl\n",
      "Generated JSONL file with - 3755 max words, 200 samples - at ../dataset/gen-word-3755-count.jsonl\n",
      "Generated JSONL file with - 3085 max words, 200 samples - at ../dataset/gen-word-3085-count.jsonl\n",
      "Generated JSONL file with - 3480 max words, 200 samples - at ../dataset/gen-word-3480-count.jsonl\n",
      "Generated JSONL file with - 3900 max words, 200 samples - at ../dataset/gen-word-3900-count.jsonl\n",
      "Generated JSONL file with - 3720 max words, 200 samples - at ../dataset/gen-word-3720-count.jsonl\n",
      "Generated JSONL file with - 2840 max words, 200 samples - at ../dataset/gen-word-2840-count.jsonl\n",
      "Generated JSONL file with - 3805 max words, 200 samples - at ../dataset/gen-word-3805-count.jsonl\n",
      "Generated JSONL file with - 2660 max words, 200 samples - at ../dataset/gen-word-2660-count.jsonl\n",
      "Generated JSONL file with - 2865 max words, 200 samples - at ../dataset/gen-word-2865-count.jsonl\n",
      "Generated JSONL file with - 2535 max words, 200 samples - at ../dataset/gen-word-2535-count.jsonl\n",
      "Generated JSONL file with - 2905 max words, 200 samples - at ../dataset/gen-word-2905-count.jsonl\n",
      "Generated JSONL file with - 3350 max words, 200 samples - at ../dataset/gen-word-3350-count.jsonl\n",
      "Generated JSONL file with - 3740 max words, 200 samples - at ../dataset/gen-word-3740-count.jsonl\n",
      "Generated JSONL file with - 3250 max words, 200 samples - at ../dataset/gen-word-3250-count.jsonl\n",
      "Generated JSONL file with - 3395 max words, 200 samples - at ../dataset/gen-word-3395-count.jsonl\n",
      "Generated JSONL file with - 3680 max words, 200 samples - at ../dataset/gen-word-3680-count.jsonl\n",
      "Generated JSONL file with - 2695 max words, 200 samples - at ../dataset/gen-word-2695-count.jsonl\n",
      "Generated JSONL file with - 3235 max words, 200 samples - at ../dataset/gen-word-3235-count.jsonl\n",
      "Generated JSONL file with - 3640 max words, 200 samples - at ../dataset/gen-word-3640-count.jsonl\n",
      "Generated JSONL file with - 2670 max words, 200 samples - at ../dataset/gen-word-2670-count.jsonl\n",
      "Generated JSONL file with - 3240 max words, 200 samples - at ../dataset/gen-word-3240-count.jsonl\n",
      "Generated JSONL file with - 3625 max words, 200 samples - at ../dataset/gen-word-3625-count.jsonl\n",
      "Generated JSONL file with - 3415 max words, 200 samples - at ../dataset/gen-word-3415-count.jsonl\n",
      "Generated JSONL file with - 3790 max words, 200 samples - at ../dataset/gen-word-3790-count.jsonl\n",
      "Generated JSONL file with - 2845 max words, 200 samples - at ../dataset/gen-word-2845-count.jsonl\n",
      "Generated JSONL file with - 2740 max words, 200 samples - at ../dataset/gen-word-2740-count.jsonl\n",
      "Generated JSONL file with - 2935 max words, 200 samples - at ../dataset/gen-word-2935-count.jsonl\n",
      "Generated JSONL file with - 2590 max words, 200 samples - at ../dataset/gen-word-2590-count.jsonl\n",
      "Generated JSONL file with - 3090 max words, 200 samples - at ../dataset/gen-word-3090-count.jsonl\n",
      "Generated JSONL file with - 3470 max words, 200 samples - at ../dataset/gen-word-3470-count.jsonl\n",
      "Generated JSONL file with - 3465 max words, 200 samples - at ../dataset/gen-word-3465-count.jsonl\n",
      "Generated JSONL file with - 2545 max words, 200 samples - at ../dataset/gen-word-2545-count.jsonl\n",
      "Generated JSONL file with - 2700 max words, 200 samples - at ../dataset/gen-word-2700-count.jsonl\n",
      "Generated JSONL file with - 3705 max words, 200 samples - at ../dataset/gen-word-3705-count.jsonl\n",
      "Generated JSONL file with - 2675 max words, 200 samples - at ../dataset/gen-word-2675-count.jsonl\n",
      "Generated JSONL file with - 3030 max words, 200 samples - at ../dataset/gen-word-3030-count.jsonl\n",
      "Generated JSONL file with - 3020 max words, 200 samples - at ../dataset/gen-word-3020-count.jsonl\n",
      "Generated JSONL file with - 2710 max words, 200 samples - at ../dataset/gen-word-2710-count.jsonl\n",
      "Generated JSONL file with - 2405 max words, 200 samples - at ../dataset/gen-word-2405-count.jsonl\n",
      "Generated JSONL file with - 2730 max words, 200 samples - at ../dataset/gen-word-2730-count.jsonl\n",
      "Generated JSONL file with - 2415 max words, 200 samples - at ../dataset/gen-word-2415-count.jsonl\n",
      "Generated JSONL file with - 2975 max words, 200 samples - at ../dataset/gen-word-2975-count.jsonl\n",
      "Generated JSONL file with - 2635 max words, 200 samples - at ../dataset/gen-word-2635-count.jsonl\n",
      "Generated JSONL file with - 3990 max words, 200 samples - at ../dataset/gen-word-3990-count.jsonl\n",
      "Generated JSONL file with - 2705 max words, 200 samples - at ../dataset/gen-word-2705-count.jsonl\n",
      "Generated JSONL file with - 3700 max words, 200 samples - at ../dataset/gen-word-3700-count.jsonl\n",
      "Generated JSONL file with - 2505 max words, 200 samples - at ../dataset/gen-word-2505-count.jsonl\n",
      "Generated JSONL file with - 3585 max words, 200 samples - at ../dataset/gen-word-3585-count.jsonl\n",
      "Generated JSONL file with - 2465 max words, 200 samples - at ../dataset/gen-word-2465-count.jsonl\n",
      "Generated JSONL file with - 3000 max words, 200 samples - at ../dataset/gen-word-3000-count.jsonl\n",
      "Generated JSONL file with - 2745 max words, 200 samples - at ../dataset/gen-word-2745-count.jsonl\n",
      "Generated JSONL file with - 2915 max words, 200 samples - at ../dataset/gen-word-2915-count.jsonl\n",
      "Generated JSONL file with - 3165 max words, 200 samples - at ../dataset/gen-word-3165-count.jsonl\n",
      "Generated JSONL file with - 2810 max words, 200 samples - at ../dataset/gen-word-2810-count.jsonl\n",
      "Generated JSONL file with - 3715 max words, 200 samples - at ../dataset/gen-word-3715-count.jsonl\n",
      "Generated JSONL file with - 2620 max words, 200 samples - at ../dataset/gen-word-2620-count.jsonl\n",
      "Generated JSONL file with - 3735 max words, 200 samples - at ../dataset/gen-word-3735-count.jsonl\n",
      "Generated JSONL file with - 3590 max words, 200 samples - at ../dataset/gen-word-3590-count.jsonl\n",
      "Generated JSONL file with - 3555 max words, 200 samples - at ../dataset/gen-word-3555-count.jsonl\n",
      "Generated JSONL file with - 2605 max words, 200 samples - at ../dataset/gen-word-2605-count.jsonl\n",
      "Generated JSONL file with - 3710 max words, 200 samples - at ../dataset/gen-word-3710-count.jsonl\n",
      "Generated JSONL file with - 3450 max words, 200 samples - at ../dataset/gen-word-3450-count.jsonl\n",
      "Generated JSONL file with - 3405 max words, 200 samples - at ../dataset/gen-word-3405-count.jsonl\n",
      "Generated JSONL file with - 3075 max words, 200 samples - at ../dataset/gen-word-3075-count.jsonl\n",
      "Generated JSONL file with - 3435 max words, 200 samples - at ../dataset/gen-word-3435-count.jsonl\n",
      "Generated JSONL file with - 3275 max words, 200 samples - at ../dataset/gen-word-3275-count.jsonl\n",
      "Generated JSONL file with - 2965 max words, 200 samples - at ../dataset/gen-word-2965-count.jsonl\n",
      "Generated JSONL file with - 2565 max words, 200 samples - at ../dataset/gen-word-2565-count.jsonl\n",
      "Generated JSONL file with - 2655 max words, 200 samples - at ../dataset/gen-word-2655-count.jsonl\n",
      "Generated JSONL file with - 2800 max words, 200 samples - at ../dataset/gen-word-2800-count.jsonl\n",
      "Generated JSONL file with - 3460 max words, 200 samples - at ../dataset/gen-word-3460-count.jsonl\n",
      "Generated JSONL file with - 3930 max words, 200 samples - at ../dataset/gen-word-3930-count.jsonl\n",
      "Generated JSONL file with - 3505 max words, 200 samples - at ../dataset/gen-word-3505-count.jsonl\n",
      "Generated JSONL file with - 3655 max words, 200 samples - at ../dataset/gen-word-3655-count.jsonl\n",
      "Generated JSONL file with - 3385 max words, 200 samples - at ../dataset/gen-word-3385-count.jsonl\n",
      "Generated JSONL file with - 2665 max words, 200 samples - at ../dataset/gen-word-2665-count.jsonl\n",
      "Generated JSONL file with - 3915 max words, 200 samples - at ../dataset/gen-word-3915-count.jsonl\n",
      "Generated JSONL file with - 3425 max words, 200 samples - at ../dataset/gen-word-3425-count.jsonl\n",
      "Generated JSONL file with - 3780 max words, 200 samples - at ../dataset/gen-word-3780-count.jsonl\n",
      "Generated JSONL file with - 3010 max words, 200 samples - at ../dataset/gen-word-3010-count.jsonl\n",
      "Generated JSONL file with - 3875 max words, 200 samples - at ../dataset/gen-word-3875-count.jsonl\n",
      "Generated JSONL file with - 3110 max words, 200 samples - at ../dataset/gen-word-3110-count.jsonl\n",
      "Generated JSONL file with - 3965 max words, 200 samples - at ../dataset/gen-word-3965-count.jsonl\n",
      "Generated JSONL file with - 3130 max words, 200 samples - at ../dataset/gen-word-3130-count.jsonl\n",
      "Generated JSONL file with - 3825 max words, 200 samples - at ../dataset/gen-word-3825-count.jsonl\n",
      "Generated JSONL file with - 2930 max words, 200 samples - at ../dataset/gen-word-2930-count.jsonl\n",
      "Generated JSONL file with - 3065 max words, 200 samples - at ../dataset/gen-word-3065-count.jsonl\n",
      "Generated JSONL file with - 3550 max words, 200 samples - at ../dataset/gen-word-3550-count.jsonl\n",
      "Generated JSONL file with - 2795 max words, 200 samples - at ../dataset/gen-word-2795-count.jsonl\n",
      "Generated JSONL file with - 3255 max words, 200 samples - at ../dataset/gen-word-3255-count.jsonl\n",
      "Generated JSONL file with - 3570 max words, 200 samples - at ../dataset/gen-word-3570-count.jsonl\n",
      "Generated JSONL file with - 3595 max words, 200 samples - at ../dataset/gen-word-3595-count.jsonl\n",
      "Generated JSONL file with - 3190 max words, 200 samples - at ../dataset/gen-word-3190-count.jsonl\n",
      "Generated JSONL file with - 2870 max words, 200 samples - at ../dataset/gen-word-2870-count.jsonl\n",
      "Generated JSONL file with - 3845 max words, 200 samples - at ../dataset/gen-word-3845-count.jsonl\n",
      "Generated JSONL file with - 2940 max words, 200 samples - at ../dataset/gen-word-2940-count.jsonl\n",
      "Generated JSONL file with - 3610 max words, 200 samples - at ../dataset/gen-word-3610-count.jsonl\n",
      "Generated JSONL file with - 3795 max words, 200 samples - at ../dataset/gen-word-3795-count.jsonl\n",
      "Generated JSONL file with - 2785 max words, 200 samples - at ../dataset/gen-word-2785-count.jsonl\n",
      "Generated JSONL file with - 3025 max words, 200 samples - at ../dataset/gen-word-3025-count.jsonl\n",
      "Generated JSONL file with - 3880 max words, 200 samples - at ../dataset/gen-word-3880-count.jsonl\n",
      "Generated JSONL file with - 4000 max words, 200 samples - at ../dataset/gen-word-4000-count.jsonl\n",
      "Generated JSONL file with - 3835 max words, 200 samples - at ../dataset/gen-word-3835-count.jsonl\n",
      "Generated JSONL file with - 3115 max words, 200 samples - at ../dataset/gen-word-3115-count.jsonl\n",
      "Generated JSONL file with - 3015 max words, 200 samples - at ../dataset/gen-word-3015-count.jsonl\n",
      "Generated JSONL file with - 2875 max words, 200 samples - at ../dataset/gen-word-2875-count.jsonl\n",
      "Generated JSONL file with - 3815 max words, 200 samples - at ../dataset/gen-word-3815-count.jsonl\n",
      "Generated JSONL file with - 2790 max words, 200 samples - at ../dataset/gen-word-2790-count.jsonl\n",
      "Generated JSONL file with - 2960 max words, 200 samples - at ../dataset/gen-word-2960-count.jsonl\n",
      "Generated JSONL file with - 3855 max words, 200 samples - at ../dataset/gen-word-3855-count.jsonl\n",
      "Generated JSONL file with - 2950 max words, 200 samples - at ../dataset/gen-word-2950-count.jsonl\n",
      "Generated JSONL file with - 2985 max words, 200 samples - at ../dataset/gen-word-2985-count.jsonl\n",
      "Generated JSONL file with - 3390 max words, 200 samples - at ../dataset/gen-word-3390-count.jsonl\n",
      "Generated JSONL file with - 2880 max words, 200 samples - at ../dataset/gen-word-2880-count.jsonl\n",
      "Generated JSONL file with - 2750 max words, 200 samples - at ../dataset/gen-word-2750-count.jsonl\n",
      "Generated JSONL file with - 3895 max words, 200 samples - at ../dataset/gen-word-3895-count.jsonl\n",
      "Generated JSONL file with - 3035 max words, 200 samples - at ../dataset/gen-word-3035-count.jsonl\n",
      "Generated JSONL file with - 3335 max words, 200 samples - at ../dataset/gen-word-3335-count.jsonl\n",
      "Generated JSONL file with - 2895 max words, 200 samples - at ../dataset/gen-word-2895-count.jsonl\n",
      "Generated JSONL file with - 3170 max words, 200 samples - at ../dataset/gen-word-3170-count.jsonl\n",
      "Generated JSONL file with - 3055 max words, 200 samples - at ../dataset/gen-word-3055-count.jsonl\n",
      "Generated JSONL file with - 3745 max words, 200 samples - at ../dataset/gen-word-3745-count.jsonl\n",
      "Generated JSONL file with - 3200 max words, 200 samples - at ../dataset/gen-word-3200-count.jsonl\n",
      "Generated JSONL file with - 3330 max words, 200 samples - at ../dataset/gen-word-3330-count.jsonl\n",
      "Generated JSONL file with - 3975 max words, 200 samples - at ../dataset/gen-word-3975-count.jsonl\n",
      "Generated JSONL file with - 3580 max words, 200 samples - at ../dataset/gen-word-3580-count.jsonl\n",
      "Generated JSONL file with - 3365 max words, 200 samples - at ../dataset/gen-word-3365-count.jsonl\n",
      "Generated JSONL file with - 3340 max words, 200 samples - at ../dataset/gen-word-3340-count.jsonl\n",
      "Generated JSONL file with - 3970 max words, 200 samples - at ../dataset/gen-word-3970-count.jsonl\n",
      "Generated JSONL file with - 3180 max words, 200 samples - at ../dataset/gen-word-3180-count.jsonl\n",
      "Generated JSONL file with - 3445 max words, 200 samples - at ../dataset/gen-word-3445-count.jsonl\n",
      "Generated JSONL file with - 3920 max words, 200 samples - at ../dataset/gen-word-3920-count.jsonl\n",
      "Generated JSONL file with - 3355 max words, 200 samples - at ../dataset/gen-word-3355-count.jsonl\n",
      "Generated JSONL file with - 3215 max words, 200 samples - at ../dataset/gen-word-3215-count.jsonl\n",
      "Generated JSONL file with - 3525 max words, 200 samples - at ../dataset/gen-word-3525-count.jsonl\n",
      "Generated JSONL file with - 3360 max words, 200 samples - at ../dataset/gen-word-3360-count.jsonl\n",
      "Generated JSONL file with - 3320 max words, 200 samples - at ../dataset/gen-word-3320-count.jsonl\n",
      "Generated JSONL file with - 3870 max words, 200 samples - at ../dataset/gen-word-3870-count.jsonl\n",
      "Generated JSONL file with - 3575 max words, 200 samples - at ../dataset/gen-word-3575-count.jsonl\n",
      "Generated JSONL file with - 3960 max words, 200 samples - at ../dataset/gen-word-3960-count.jsonl\n",
      "Generated JSONL file with - 3370 max words, 200 samples - at ../dataset/gen-word-3370-count.jsonl\n",
      "Generated JSONL file with - 3945 max words, 200 samples - at ../dataset/gen-word-3945-count.jsonl\n",
      "Generated JSONL file with - 3905 max words, 200 samples - at ../dataset/gen-word-3905-count.jsonl\n",
      "Generated JSONL file with - 3765 max words, 200 samples - at ../dataset/gen-word-3765-count.jsonl\n",
      "Generated JSONL file with - 3850 max words, 200 samples - at ../dataset/gen-word-3850-count.jsonl\n",
      "Generated JSONL file with - 3560 max words, 200 samples - at ../dataset/gen-word-3560-count.jsonl\n",
      "Generated JSONL file with - 3885 max words, 200 samples - at ../dataset/gen-word-3885-count.jsonl\n",
      "Generated JSONL file with - 3860 max words, 200 samples - at ../dataset/gen-word-3860-count.jsonl\n",
      "Generated JSONL file with - 3830 max words, 200 samples - at ../dataset/gen-word-3830-count.jsonl\n",
      "Generated JSONL file with - 3890 max words, 200 samples - at ../dataset/gen-word-3890-count.jsonl\n",
      "Generated JSONL file with - 3800 max words, 200 samples - at ../dataset/gen-word-3800-count.jsonl\n",
      "## Done ##\n",
      "total 3.4G\n",
      "drwxr-xr-x  2 root root   72K Aug 20 01:07 .\n",
      "drwxr-xr-x 10 root root   205 Aug 19 17:44 ..\n",
      "-rw-r--r--  1 root root   20K Aug 20 01:07 gen-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root  106K Aug 20 01:07 gen-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root  113K Aug 20 01:07 gen-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root  120K Aug 20 01:07 gen-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  2.1M Aug 20 01:07 gen-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  2.2M Aug 20 01:07 gen-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root  124K Aug 20 01:07 gen-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  2.3M Aug 20 01:07 gen-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root  130K Aug 20 01:07 gen-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root  132K Aug 20 01:07 gen-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  2.4M Aug 20 01:07 gen-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root  134K Aug 20 01:07 gen-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  2.5M Aug 20 01:07 gen-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root  143K Aug 20 01:07 gen-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  2.6M Aug 20 01:07 gen-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root  146K Aug 20 01:07 gen-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  2.7M Aug 20 01:07 gen-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root  154K Aug 20 01:07 gen-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  2.8M Aug 20 01:07 gen-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root   23K Aug 20 01:07 gen-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root  157K Aug 20 01:07 gen-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  2.9M Aug 20 01:07 gen-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root  161K Aug 20 01:07 gen-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  3.0M Aug 20 01:07 gen-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root  162K Aug 20 01:07 gen-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  3.1M Aug 20 01:07 gen-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root  170K Aug 20 01:07 gen-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  3.2M Aug 20 01:07 gen-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root  179K Aug 20 01:07 gen-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  3.3M Aug 20 01:07 gen-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root  179K Aug 20 01:07 gen-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  3.4M Aug 20 01:07 gen-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root  187K Aug 20 01:07 gen-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  3.5M Aug 20 01:07 gen-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root  191K Aug 20 01:07 gen-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  3.6M Aug 20 01:07 gen-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root  194K Aug 20 01:07 gen-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  3.7M Aug 20 01:07 gen-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root  199K Aug 20 01:07 gen-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  3.8M Aug 20 01:07 gen-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 gen-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root  204K Aug 20 01:07 gen-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2005-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2010-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2015-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2020-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2025-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2030-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2035-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2040-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2045-count.jsonl\n",
      "-rw-r--r--  1 root root  215K Aug 20 01:07 gen-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  3.9M Aug 20 01:07 gen-word-2050-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2055-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2060-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2065-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2070-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2075-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2080-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2085-count.jsonl\n",
      "-rw-r--r--  1 root root  4.0M Aug 20 01:07 gen-word-2090-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2095-count.jsonl\n",
      "-rw-r--r--  1 root root  213K Aug 20 01:07 gen-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2100-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2105-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2110-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2115-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2120-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2125-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2130-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2135-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2140-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2145-count.jsonl\n",
      "-rw-r--r--  1 root root  220K Aug 20 01:07 gen-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2150-count.jsonl\n",
      "-rw-r--r--  1 root root  4.1M Aug 20 01:07 gen-word-2155-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2160-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2165-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2170-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2175-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2180-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2185-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2190-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2195-count.jsonl\n",
      "-rw-r--r--  1 root root  224K Aug 20 01:07 gen-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2200-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2205-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2210-count.jsonl\n",
      "-rw-r--r--  1 root root  4.2M Aug 20 01:07 gen-word-2215-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2220-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2225-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2230-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2235-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2240-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2245-count.jsonl\n",
      "-rw-r--r--  1 root root  234K Aug 20 01:07 gen-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2250-count.jsonl\n",
      "-rw-r--r--  1 root root  4.3M Aug 20 01:07 gen-word-2255-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2260-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2265-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2270-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2275-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2280-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2285-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2290-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2295-count.jsonl\n",
      "-rw-r--r--  1 root root  237K Aug 20 01:07 gen-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2300-count.jsonl\n",
      "-rw-r--r--  1 root root  4.4M Aug 20 01:07 gen-word-2305-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2310-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2315-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2320-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2325-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2330-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2335-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2340-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2345-count.jsonl\n",
      "-rw-r--r--  1 root root  237K Aug 20 01:07 gen-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2350-count.jsonl\n",
      "-rw-r--r--  1 root root  4.5M Aug 20 01:07 gen-word-2355-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2360-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2365-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2370-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2375-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2380-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2385-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2390-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2395-count.jsonl\n",
      "-rw-r--r--  1 root root  244K Aug 20 01:07 gen-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  4.6M Aug 20 01:07 gen-word-2400-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2405-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2410-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2415-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2420-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2425-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2430-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2435-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2440-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2445-count.jsonl\n",
      "-rw-r--r--  1 root root  248K Aug 20 01:07 gen-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2450-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2455-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2460-count.jsonl\n",
      "-rw-r--r--  1 root root  4.7M Aug 20 01:07 gen-word-2465-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2470-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2475-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2480-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2485-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2490-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2495-count.jsonl\n",
      "-rw-r--r--  1 root root   34K Aug 20 01:07 gen-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root  253K Aug 20 01:07 gen-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2500-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2505-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2510-count.jsonl\n",
      "-rw-r--r--  1 root root  4.8M Aug 20 01:07 gen-word-2515-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2520-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2525-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2530-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2535-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2540-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2545-count.jsonl\n",
      "-rw-r--r--  1 root root  252K Aug 20 01:07 gen-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2550-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2555-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2560-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2565-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2570-count.jsonl\n",
      "-rw-r--r--  1 root root  4.9M Aug 20 01:07 gen-word-2575-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2580-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2585-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2590-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2595-count.jsonl\n",
      "-rw-r--r--  1 root root  265K Aug 20 01:07 gen-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2600-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2605-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2610-count.jsonl\n",
      "-rw-r--r--  1 root root  5.0M Aug 20 01:07 gen-word-2615-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2620-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2625-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2630-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2635-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2640-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2645-count.jsonl\n",
      "-rw-r--r--  1 root root  264K Aug 20 01:07 gen-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2650-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2655-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2660-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2665-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2670-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2675-count.jsonl\n",
      "-rw-r--r--  1 root root  5.1M Aug 20 01:07 gen-word-2680-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2685-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2690-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2695-count.jsonl\n",
      "-rw-r--r--  1 root root  275K Aug 20 01:07 gen-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2700-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2705-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2710-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2715-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2720-count.jsonl\n",
      "-rw-r--r--  1 root root  5.2M Aug 20 01:07 gen-word-2725-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2730-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2735-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2740-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2745-count.jsonl\n",
      "-rw-r--r--  1 root root  275K Aug 20 01:07 gen-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2750-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2755-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2760-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2765-count.jsonl\n",
      "-rw-r--r--  1 root root  5.3M Aug 20 01:07 gen-word-2770-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2775-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2780-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2785-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2790-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2795-count.jsonl\n",
      "-rw-r--r--  1 root root  284K Aug 20 01:07 gen-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2800-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2805-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2810-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2815-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2820-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2825-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2830-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2835-count.jsonl\n",
      "-rw-r--r--  1 root root  5.4M Aug 20 01:07 gen-word-2840-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2845-count.jsonl\n",
      "-rw-r--r--  1 root root  289K Aug 20 01:07 gen-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2850-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2855-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2860-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2865-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2870-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2875-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2880-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2885-count.jsonl\n",
      "-rw-r--r--  1 root root  5.5M Aug 20 01:07 gen-word-2890-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2895-count.jsonl\n",
      "-rw-r--r--  1 root root  286K Aug 20 01:07 gen-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2900-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2905-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2910-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2915-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2920-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2925-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2930-count.jsonl\n",
      "-rw-r--r--  1 root root  5.6M Aug 20 01:07 gen-word-2935-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2940-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2945-count.jsonl\n",
      "-rw-r--r--  1 root root  300K Aug 20 01:07 gen-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2950-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2955-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2960-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2965-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2970-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2975-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2980-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2985-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2990-count.jsonl\n",
      "-rw-r--r--  1 root root  5.7M Aug 20 01:07 gen-word-2995-count.jsonl\n",
      "-rw-r--r--  1 root root   39K Aug 20 01:07 gen-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root  299K Aug 20 01:07 gen-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3000-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3005-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3010-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3015-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3020-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3025-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3030-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3035-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3040-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3045-count.jsonl\n",
      "-rw-r--r--  1 root root  300K Aug 20 01:07 gen-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  5.8M Aug 20 01:07 gen-word-3050-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3055-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3060-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3065-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3070-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3075-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3080-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3085-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3090-count.jsonl\n",
      "-rw-r--r--  1 root root  5.9M Aug 20 01:07 gen-word-3095-count.jsonl\n",
      "-rw-r--r--  1 root root  304K Aug 20 01:07 gen-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3100-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3105-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3110-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3115-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3120-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3125-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3130-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3135-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3140-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3145-count.jsonl\n",
      "-rw-r--r--  1 root root  317K Aug 20 01:07 gen-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  6.0M Aug 20 01:07 gen-word-3150-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3155-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3160-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3165-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3170-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3175-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3180-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3185-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3190-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3195-count.jsonl\n",
      "-rw-r--r--  1 root root  319K Aug 20 01:07 gen-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  6.1M Aug 20 01:07 gen-word-3200-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3205-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3210-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3215-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3220-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3225-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3230-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3235-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3240-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3245-count.jsonl\n",
      "-rw-r--r--  1 root root  324K Aug 20 01:07 gen-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3250-count.jsonl\n",
      "-rw-r--r--  1 root root  6.2M Aug 20 01:07 gen-word-3255-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3260-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3265-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3270-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3275-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3280-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3285-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3290-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3295-count.jsonl\n",
      "-rw-r--r--  1 root root  332K Aug 20 01:07 gen-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  6.3M Aug 20 01:07 gen-word-3300-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3305-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3310-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3315-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3320-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3325-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3330-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3335-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3340-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3345-count.jsonl\n",
      "-rw-r--r--  1 root root  338K Aug 20 01:07 gen-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3350-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3355-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3360-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3365-count.jsonl\n",
      "-rw-r--r--  1 root root  6.4M Aug 20 01:07 gen-word-3370-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3375-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3380-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3385-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3390-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3395-count.jsonl\n",
      "-rw-r--r--  1 root root  344K Aug 20 01:07 gen-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3400-count.jsonl\n",
      "-rw-r--r--  1 root root  6.5M Aug 20 01:07 gen-word-3405-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3410-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3415-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3420-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3425-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3430-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3435-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3440-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3445-count.jsonl\n",
      "-rw-r--r--  1 root root  344K Aug 20 01:07 gen-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3450-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3455-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3460-count.jsonl\n",
      "-rw-r--r--  1 root root  6.6M Aug 20 01:07 gen-word-3465-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3470-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3475-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3480-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3485-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3490-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3495-count.jsonl\n",
      "-rw-r--r--  1 root root   43K Aug 20 01:07 gen-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root  354K Aug 20 01:07 gen-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3500-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3505-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3510-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3515-count.jsonl\n",
      "-rw-r--r--  1 root root  6.7M Aug 20 01:07 gen-word-3520-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3525-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3530-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3535-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3540-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3545-count.jsonl\n",
      "-rw-r--r--  1 root root  357K Aug 20 01:07 gen-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3550-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3555-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3560-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3565-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3570-count.jsonl\n",
      "-rw-r--r--  1 root root  6.8M Aug 20 01:07 gen-word-3575-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3580-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3585-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3590-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3595-count.jsonl\n",
      "-rw-r--r--  1 root root  359K Aug 20 01:07 gen-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3600-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3605-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3610-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3615-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3620-count.jsonl\n",
      "-rw-r--r--  1 root root  6.9M Aug 20 01:07 gen-word-3625-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3630-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3635-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3640-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3645-count.jsonl\n",
      "-rw-r--r--  1 root root  363K Aug 20 01:07 gen-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3650-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3655-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3660-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3665-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3670-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3675-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3680-count.jsonl\n",
      "-rw-r--r--  1 root root  7.0M Aug 20 01:07 gen-word-3685-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3690-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3695-count.jsonl\n",
      "-rw-r--r--  1 root root  365K Aug 20 01:07 gen-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3700-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3705-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3710-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3715-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3720-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3725-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3730-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3735-count.jsonl\n",
      "-rw-r--r--  1 root root  7.1M Aug 20 01:07 gen-word-3740-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3745-count.jsonl\n",
      "-rw-r--r--  1 root root  377K Aug 20 01:07 gen-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3750-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3755-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3760-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3765-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3770-count.jsonl\n",
      "-rw-r--r--  1 root root  7.2M Aug 20 01:07 gen-word-3775-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3780-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3785-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3790-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3795-count.jsonl\n",
      "-rw-r--r--  1 root root  379K Aug 20 01:07 gen-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3800-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3805-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3810-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3815-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3820-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3825-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3830-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3835-count.jsonl\n",
      "-rw-r--r--  1 root root  7.3M Aug 20 01:07 gen-word-3840-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3845-count.jsonl\n",
      "-rw-r--r--  1 root root  387K Aug 20 01:07 gen-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3850-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3855-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3860-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3865-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3870-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3875-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3880-count.jsonl\n",
      "-rw-r--r--  1 root root  7.4M Aug 20 01:07 gen-word-3885-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3890-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3895-count.jsonl\n",
      "-rw-r--r--  1 root root  391K Aug 20 01:07 gen-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3900-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3905-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3910-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3915-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3920-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3925-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3930-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3935-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3940-count.jsonl\n",
      "-rw-r--r--  1 root root  7.5M Aug 20 01:07 gen-word-3945-count.jsonl\n",
      "-rw-r--r--  1 root root  395K Aug 20 01:07 gen-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3950-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3955-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3960-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3965-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3970-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3975-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3980-count.jsonl\n",
      "-rw-r--r--  1 root root  7.7M Aug 20 01:07 gen-word-3985-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3990-count.jsonl\n",
      "-rw-r--r--  1 root root  7.6M Aug 20 01:07 gen-word-3995-count.jsonl\n",
      "-rw-r--r--  1 root root   49K Aug 20 01:07 gen-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root  401K Aug 20 01:07 gen-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  7.7M Aug 20 01:07 gen-word-4000-count.jsonl\n",
      "-rw-r--r--  1 root root  404K Aug 20 01:07 gen-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root  410K Aug 20 01:07 gen-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root  415K Aug 20 01:07 gen-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root  421K Aug 20 01:07 gen-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root  417K Aug 20 01:07 gen-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root  427K Aug 20 01:07 gen-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root  435K Aug 20 01:07 gen-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root  437K Aug 20 01:07 gen-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root  436K Aug 20 01:07 gen-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root   53K Aug 20 01:07 gen-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root  447K Aug 20 01:07 gen-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root  450K Aug 20 01:07 gen-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root  455K Aug 20 01:07 gen-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root  461K Aug 20 01:07 gen-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root  471K Aug 20 01:07 gen-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root  472K Aug 20 01:07 gen-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root  475K Aug 20 01:07 gen-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root  486K Aug 20 01:07 gen-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root  486K Aug 20 01:07 gen-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root  483K Aug 20 01:07 gen-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root   15K Aug 20 01:07 gen-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root   59K Aug 20 01:07 gen-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root  495K Aug 20 01:07 gen-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  999K Aug 20 01:07 gen-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root 1022K Aug 20 01:07 gen-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root 1016K Aug 20 01:07 gen-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root 1022K Aug 20 01:07 gen-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root   64K Aug 20 01:07 gen-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  1.1M Aug 20 01:07 gen-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root   68K Aug 20 01:07 gen-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  1.2M Aug 20 01:07 gen-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root   74K Aug 20 01:07 gen-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  1.3M Aug 20 01:07 gen-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root   77K Aug 20 01:07 gen-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  1.4M Aug 20 01:07 gen-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root   85K Aug 20 01:07 gen-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  1.5M Aug 20 01:07 gen-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root   87K Aug 20 01:07 gen-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  1.6M Aug 20 01:07 gen-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root   92K Aug 20 01:07 gen-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  1.7M Aug 20 01:07 gen-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root   96K Aug 20 01:07 gen-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  1.8M Aug 20 01:07 gen-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root  101K Aug 20 01:07 gen-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  1.9M Aug 20 01:07 gen-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  2.0M Aug 20 01:07 gen-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root   51K Aug 20 01:07 shuffle-word-10-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-100-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1005-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1015-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1025-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1035-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1045-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-105-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-1055-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1065-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1075-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1085-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1095-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-110-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1105-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1115-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1125-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1135-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1145-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 20 01:07 shuffle-word-115-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1155-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1165-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1175-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1185-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1195-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-120-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1205-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1215-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1225-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1235-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1245-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-125-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1255-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1265-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1275-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1285-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1295-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-130-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1305-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1315-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1325-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1335-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-1345-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-135-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1355-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1365-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1375-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1385-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1395-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-140-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1405-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1415-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1425-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1435-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1445-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-145-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1455-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1465-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1475-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1485-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1495-count.jsonl\n",
      "-rw-r--r--  1 root root   46K Aug 20 01:07 shuffle-word-15-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-150-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1505-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-1515-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1525-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1535-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1545-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-155-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1555-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1565-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1575-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1585-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1595-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-160-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1605-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1615-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1625-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1635-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1645-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-165-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1655-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1665-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1675-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1685-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1695-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-170-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1705-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1715-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1725-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1735-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1745-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-175-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1755-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1765-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1775-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1785-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1795-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-180-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1805-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1815-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1825-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1835-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1845-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-185-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1855-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1865-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1875-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1885-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1895-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-190-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1905-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1915-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1925-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1935-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1945-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-195-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1955-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1965-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1975-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-1985-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-1995-count.jsonl\n",
      "-rw-r--r--  1 root root   39K Aug 20 01:07 shuffle-word-20-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-200-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2005-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2010-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2015-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2020-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2025-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2030-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2035-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2040-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2045-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-205-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2050-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2055-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2060-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2065-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2070-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2075-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2080-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2085-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2090-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2095-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-210-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2105-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2110-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2115-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2120-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2125-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2130-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2135-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2140-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2145-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-215-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2150-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2155-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2160-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2165-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2170-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2175-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2180-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2185-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2190-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2195-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-220-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2205-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2210-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2215-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2220-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2225-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2230-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2235-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2240-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2245-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-225-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2250-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2255-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2260-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2265-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2270-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2275-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2280-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2285-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2290-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2295-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-230-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2305-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2310-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2315-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2320-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2325-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2330-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2335-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2340-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2345-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-235-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2350-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2355-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2360-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2365-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2370-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2375-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2380-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2385-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-2390-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2395-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-240-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2405-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2410-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2415-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2420-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 20 01:07 shuffle-word-2425-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2430-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2435-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2440-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2445-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-245-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2450-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 20 01:07 shuffle-word-2455-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-2460-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2465-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-2470-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2475-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2480-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-2485-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2490-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2495-count.jsonl\n",
      "-rw-r--r--  1 root root   33K Aug 20 01:07 shuffle-word-25-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-250-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 20 01:07 shuffle-word-2505-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 20 01:07 shuffle-word-2510-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 20 01:07 shuffle-word-2515-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 20 01:07 shuffle-word-2520-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2525-count.jsonl\n",
      "-rw-r--r--  1 root root  513K Aug 20 01:07 shuffle-word-2530-count.jsonl\n",
      "-rw-r--r--  1 root root  514K Aug 20 01:07 shuffle-word-2535-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2540-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 20 01:07 shuffle-word-2545-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-255-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2550-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 20 01:07 shuffle-word-2555-count.jsonl\n",
      "-rw-r--r--  1 root root  513K Aug 20 01:07 shuffle-word-2560-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2565-count.jsonl\n",
      "-rw-r--r--  1 root root  518K Aug 20 01:07 shuffle-word-2570-count.jsonl\n",
      "-rw-r--r--  1 root root  513K Aug 20 01:07 shuffle-word-2575-count.jsonl\n",
      "-rw-r--r--  1 root root  517K Aug 20 01:07 shuffle-word-2580-count.jsonl\n",
      "-rw-r--r--  1 root root  515K Aug 20 01:07 shuffle-word-2585-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2590-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-2595-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-260-count.jsonl\n",
      "-rw-r--r--  1 root root  516K Aug 20 01:07 shuffle-word-2600-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2605-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2610-count.jsonl\n",
      "-rw-r--r--  1 root root  514K Aug 20 01:07 shuffle-word-2615-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2620-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2625-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2630-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2635-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2640-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2645-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-265-count.jsonl\n",
      "-rw-r--r--  1 root root  512K Aug 20 01:07 shuffle-word-2650-count.jsonl\n",
      "-rw-r--r--  1 root root  514K Aug 20 01:07 shuffle-word-2655-count.jsonl\n",
      "-rw-r--r--  1 root root  512K Aug 20 01:07 shuffle-word-2660-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2665-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2670-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2675-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2680-count.jsonl\n",
      "-rw-r--r--  1 root root  512K Aug 20 01:07 shuffle-word-2685-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2690-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2695-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-270-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2700-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2705-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2710-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2715-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2720-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2725-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2730-count.jsonl\n",
      "-rw-r--r--  1 root root  510K Aug 20 01:07 shuffle-word-2735-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2740-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2745-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-275-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2750-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2755-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2760-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2765-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2770-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2775-count.jsonl\n",
      "-rw-r--r--  1 root root  511K Aug 20 01:07 shuffle-word-2780-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2785-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2790-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2795-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-280-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2800-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2805-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2810-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2815-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2820-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2825-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2830-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2835-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2840-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2845-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-285-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2850-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2855-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2860-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2865-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2870-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2875-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2880-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2885-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2890-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2895-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-290-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2900-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2905-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2910-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2915-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2920-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2925-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2930-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2935-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2940-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2945-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-295-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2950-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2955-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2960-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2965-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-2970-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2975-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2980-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2985-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2990-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-2995-count.jsonl\n",
      "-rw-r--r--  1 root root   36K Aug 20 01:07 shuffle-word-30-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-300-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3000-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3005-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3010-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3015-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3020-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3025-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3030-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3035-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3040-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3045-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-305-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3050-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3055-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3060-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3065-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3070-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3075-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3080-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3085-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3090-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3095-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-310-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3100-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3105-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3110-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3115-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3120-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3125-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3130-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3135-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3140-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3145-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-315-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3150-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3155-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3160-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3165-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3170-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3175-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3180-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3185-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3190-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3195-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-320-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3200-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3205-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3210-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3215-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3220-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3225-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3230-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3235-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3240-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3245-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-325-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3250-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3255-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3260-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3265-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3270-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3275-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3280-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3285-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3290-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3295-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-330-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3300-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3305-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3310-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3315-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3320-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3325-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3330-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3335-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3340-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3345-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-335-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3350-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3355-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3360-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3365-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3370-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3375-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3380-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3385-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3390-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3395-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-340-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3400-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3405-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3410-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3415-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3420-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3425-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3430-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3435-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3440-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3445-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-345-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3450-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3455-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3460-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3465-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3470-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3475-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3480-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3485-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3490-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3495-count.jsonl\n",
      "-rw-r--r--  1 root root   32K Aug 20 01:07 shuffle-word-35-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-350-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3500-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3505-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3510-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3515-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3520-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3525-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3530-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3535-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3540-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3545-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-355-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3550-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3555-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3560-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3565-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3570-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3575-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3580-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3585-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3590-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3595-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-360-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3600-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3605-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3610-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3615-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3620-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3625-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3630-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3635-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3640-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3645-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-365-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3650-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3655-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3660-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3665-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3670-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3675-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3680-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3685-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3690-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3695-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-370-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3700-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3705-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3710-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3715-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3720-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3725-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3730-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3735-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3740-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3745-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-375-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3750-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3755-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3760-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3765-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3770-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3775-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3780-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3785-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3790-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3795-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-380-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3800-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3805-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3810-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3815-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3820-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3825-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3830-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3835-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3840-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3845-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-385-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3850-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3855-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3860-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3865-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3870-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3875-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3880-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3885-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3890-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3895-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-390-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3900-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3905-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3910-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3915-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3920-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3925-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3930-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3935-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3940-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3945-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-395-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3950-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3955-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3960-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3965-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3970-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3975-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3980-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3985-count.jsonl\n",
      "-rw-r--r--  1 root root  508K Aug 20 01:07 shuffle-word-3990-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-3995-count.jsonl\n",
      "-rw-r--r--  1 root root   31K Aug 20 01:07 shuffle-word-40-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-400-count.jsonl\n",
      "-rw-r--r--  1 root root  509K Aug 20 01:07 shuffle-word-4000-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-405-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-410-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-415-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-420-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-425-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-430-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-435-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-440-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-445-count.jsonl\n",
      "-rw-r--r--  1 root root   32K Aug 20 01:07 shuffle-word-45-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-450-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-455-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-460-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-465-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-470-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-475-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-480-count.jsonl\n",
      "-rw-r--r--  1 root root   26K Aug 20 01:07 shuffle-word-485-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-490-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-495-count.jsonl\n",
      "-rw-r--r--  1 root root   86K Aug 20 01:07 shuffle-word-5-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-50-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-500-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-505-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-510-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-515-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-520-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-525-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-530-count.jsonl\n",
      "-rw-r--r--  1 root root  530K Aug 20 01:07 shuffle-word-535-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-540-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-545-count.jsonl\n",
      "-rw-r--r--  1 root root   27K Aug 20 01:07 shuffle-word-55-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-550-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 20 01:07 shuffle-word-555-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-560-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-565-count.jsonl\n",
      "-rw-r--r--  1 root root  529K Aug 20 01:07 shuffle-word-570-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-575-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-580-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 20 01:07 shuffle-word-585-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-590-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-595-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-60-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-600-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-605-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-610-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-615-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-620-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-625-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-630-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-635-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-640-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-645-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-65-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-650-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-655-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-660-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-665-count.jsonl\n",
      "-rw-r--r--  1 root root  528K Aug 20 01:07 shuffle-word-670-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-675-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-680-count.jsonl\n",
      "-rw-r--r--  1 root root  520K Aug 20 01:07 shuffle-word-685-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-690-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-695-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 20 01:07 shuffle-word-70-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-700-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-705-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-710-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-715-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-720-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-725-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-730-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-735-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-740-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-745-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 20 01:07 shuffle-word-75-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-750-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-755-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-760-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-765-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-770-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-775-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-780-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-785-count.jsonl\n",
      "-rw-r--r--  1 root root  526K Aug 20 01:07 shuffle-word-790-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-795-count.jsonl\n",
      "-rw-r--r--  1 root root   29K Aug 20 01:07 shuffle-word-80-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-800-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-805-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-810-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-815-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-820-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-825-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-830-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-835-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-840-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-845-count.jsonl\n",
      "-rw-r--r--  1 root root   30K Aug 20 01:07 shuffle-word-85-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-850-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-855-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-860-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-865-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-870-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-875-count.jsonl\n",
      "-rw-r--r--  1 root root  525K Aug 20 01:07 shuffle-word-880-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-885-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-890-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-895-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-90-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-900-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-905-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-910-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-915-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-920-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-925-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-930-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-935-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-940-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-945-count.jsonl\n",
      "-rw-r--r--  1 root root   28K Aug 20 01:07 shuffle-word-95-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-950-count.jsonl\n",
      "-rw-r--r--  1 root root  524K Aug 20 01:07 shuffle-word-955-count.jsonl\n",
      "-rw-r--r--  1 root root  527K Aug 20 01:07 shuffle-word-960-count.jsonl\n",
      "-rw-r--r--  1 root root  523K Aug 20 01:07 shuffle-word-965-count.jsonl\n",
      "-rw-r--r--  1 root root  519K Aug 20 01:07 shuffle-word-970-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-975-count.jsonl\n",
      "-rw-r--r--  1 root root  521K Aug 20 01:07 shuffle-word-980-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-985-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-990-count.jsonl\n",
      "-rw-r--r--  1 root root  522K Aug 20 01:07 shuffle-word-995-count.jsonl\n",
      "-rw-r--r--  1 root root   13K Aug 20 01:07 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 100 &\n",
    "for i in {5..500..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 2100 words dataset\n",
    "# \n",
    "for i in {505..4000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 200 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-4k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=4096', '--model.ctx_len=4096', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-4k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=4096', '--model.ctx_len=4096', '--model.bptt_learning_range=1', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-1k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3240243665\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3240243665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230820_010757-1q4pcun7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/1q4pcun7\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1601/1601 [00:00<00:00, 8650.49it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d815de74e0f7adef/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.78it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 3240243665\n",
      "[rank: 3] Global seed set to 3240243665\n",
      "[rank: 5] Global seed set to 3240243665\n",
      "[rank: 7] Global seed set to 3240243665\n",
      "[rank: 1] Global seed set to 3240243665\n",
      "[rank: 6] Global seed set to 3240243665\n",
      "[rank: 4] Global seed set to 3240243665\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d815de74e0f7adef/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s]\n",
      "[rank: 3] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 01:08:46,405] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 01:08:48,444] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 01:08:48,757] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 01:08:48,781] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 01:08:48,838] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 01:08:49,010] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3240243665\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 01:08:49,053] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 3240243665                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 01:10:46,769] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06902956962585449 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10107088088989258 seconds\n",
      "Time to load fused_adam op: 0.10120606422424316 seconds\n",
      "Time to load fused_adam op: 0.10140848159790039 seconds\n",
      "Time to load fused_adam op: 0.10136175155639648 seconds\n",
      "Time to load fused_adam op: 0.10134744644165039 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10165977478027344 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10172891616821289 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06900215148925781 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10214781761169434 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10251832008361816 seconds\n",
      "Time to load utils op: 0.10239791870117188 seconds\n",
      "Time to load utils op: 0.10227155685424805 seconds\n",
      "Time to load utils op: 0.10234475135803223 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10231781005859375 seconds\n",
      "Time to load utils op: 0.10251379013061523 seconds\n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006475448608398438 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.000606536865234375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00060272216796875 seconds\n",
      "Time to load utils op: 0.0007064342498779297 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0006535053253173828 seconds\n",
      "Time to load utils op: 0.0007452964782714844 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006880760192871094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008525848388671875 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/15433 [12:03<3:40:35,  1.11it/s, v_num=cun7, train/loss=5.8/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 15433/15433 [3:51:46<00:00,  1.11it/s, v_num=cun7, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|â–ˆâ–                 | 1/16 [00:00<00:06,  2.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|â–ˆâ–ˆâ–                | 2/16 [00:00<00:06,  2.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|â–ˆâ–ˆâ–ˆâ–Œ               | 3/16 [00:01<00:05,  2.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 4/16 [00:01<00:04,  2.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 5/16 [00:01<00:03,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 6/16 [00:02<00:03,  2.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 7/16 [00:02<00:03,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 8/16 [00:02<00:02,  2.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 9/16 [00:03<00:02,  2.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 10/16 [00:03<00:02,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/16 [00:03<00:01,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 12/16 [00:04<00:01,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 13/16 [00:04<00:01,  2.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/16 [00:04<00:00,  2.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 15/16 [00:05<00:00,  2.94it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 15433/15433 [3:52:00<00:00,  1.11it/s, v_num=cun7, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 15433/15433 [3:52:00<00:00,  1.11it/s, v_num=cun7, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 15433/15433 [3:52:16<00:00,  1.11it/s, v_num=cun7, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–‚â–†â–â–ƒâ–ƒâ–‡â–…â–ˆâ–â–ƒâ–†â–‡â–ˆâ–„â–â–ƒâ–„â–„â–ƒâ–ƒâ–ˆâ–‡â–†â–†â–…â–„â–‡â–ƒâ–ˆâ–‡â–…â–‡â–â–ˆâ–„â–‚â–‡â–ƒâ–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–…â–ˆâ–â–‚â–‡â–†â–†â–‡â–‡â–„â–†â–…â–‚â–„â–…â–â–‚â–„â–â–‚â–‚â–‡â–…â–ƒâ–„â–â–„â–„â–â–ƒâ–â–â–â–‚â–â–„â–ƒâ–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 2862\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 1.65625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 1.18013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-4k (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/1q4pcun7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230820_010757-1q4pcun7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-4k (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-4k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=4096 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-1k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-4k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 20 05:04 ../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-4k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 98.88888888888889% similarity, with 89 matched token, and 1 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 99.33333333333333% similarity, with 149 matched token, and 1 token mismatch\n",
      "## Model validation for 160 tokens : 98.75% similarity, with 158 matched token, and 2 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.94736842105263% similarity, with 188 matched token, and 2 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 98.63636363636363% similarity, with 217 matched token, and 3 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 99.16666666666667% similarity, with 238 matched token, and 2 token mismatch\n",
      "## Model validation for 250 tokens : 99.2% similarity, with 248 matched token, and 2 token mismatch\n",
      "## Model validation for 260 tokens : 99.23076923076923% similarity, with 258 matched token, and 2 token mismatch\n",
      "## Model validation for 270 tokens : 98.51851851851852% similarity, with 266 matched token, and 4 token mismatch\n",
      "## Model validation for 280 tokens : 98.21428571428571% similarity, with 275 matched token, and 5 token mismatch\n",
      "## Model validation for 290 tokens : 98.62068965517241% similarity, with 286 matched token, and 4 token mismatch\n",
      "## Model validation for 300 tokens : 98.0% similarity, with 294 matched token, and 6 token mismatch\n",
      "## Model validation for 325 tokens : 97.53846153846155% similarity, with 317 matched token, and 8 token mismatch\n",
      "## Model validation for 350 tokens : 97.14285714285714% similarity, with 340 matched token, and 10 token mismatch\n",
      "## Model validation for 375 tokens : 95.73333333333333% similarity, with 359 matched token, and 16 token mismatch\n",
      "## Model validation for 400 tokens : 95.5% similarity, with 382 matched token, and 18 token mismatch\n",
      "## Model validation for 425 tokens : 94.58823529411765% similarity, with 402 matched token, and 23 token mismatch\n",
      "## Model validation for 450 tokens : 94.66666666666667% similarity, with 426 matched token, and 24 token mismatch\n",
      "## Model validation for 475 tokens : 93.89473684210526% similarity, with 446 matched token, and 29 token mismatch\n",
      "## Model validation for 500 tokens : 92.0% similarity, with 460 matched token, and 40 token mismatch\n",
      "## Model validation for 525 tokens : 91.42857142857143% similarity, with 480 matched token, and 45 token mismatch\n",
      "## Model validation for 550 tokens : 91.27272727272727% similarity, with 502 matched token, and 48 token mismatch\n",
      "## Model validation for 575 tokens : 90.95652173913044% similarity, with 523 matched token, and 52 token mismatch\n",
      "## Model validation for 600 tokens : 90.5% similarity, with 543 matched token, and 57 token mismatch\n",
      "## Model validation for 625 tokens : 90.4% similarity, with 565 matched token, and 60 token mismatch\n",
      "## Model validation for 650 tokens : 88.46153846153845% similarity, with 575 matched token, and 75 token mismatch\n",
      "## Model validation for 675 tokens : 88.29629629629629% similarity, with 596 matched token, and 79 token mismatch\n",
      "## Model validation for 700 tokens : 88.0% similarity, with 616 matched token, and 84 token mismatch\n",
      "## Model validation for 750 tokens : 86.0% similarity, with 645 matched token, and 105 token mismatch\n",
      "## Model validation for 800 tokens : 84.25% similarity, with 674 matched token, and 126 token mismatch\n",
      "## Model validation for 850 tokens : 84.47058823529412% similarity, with 718 matched token, and 132 token mismatch\n",
      "## Model validation for 900 tokens : 81.22222222222221% similarity, with 731 matched token, and 169 token mismatch\n",
      "## Model validation for 950 tokens : 80.52631578947368% similarity, with 765 matched token, and 185 token mismatch\n",
      "## Model validation for 1000 tokens : 77.8% similarity, with 778 matched token, and 222 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 6 : Ramping up the ctx size (8192), memory training\n",
    "\n",
    "- Tune 6: Large ctx size (8192), Scaling up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 50 samples - at ../dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 134 samples (1 token repeat) - 20 max words - at ../dataset/shuffle-word-20-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 50 samples - at ../dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 50 samples - at ../dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 50 samples - at ../dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 50 samples - at ../dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 50 samples - at ../dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 50 samples - at ../dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 50 samples - at ../dataset/gen-word-20-count.jsonl\n",
      "Generated a single JSONL file with 34 samples (1 token repeat) - 75 max words - at ../dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 46 samples (1 token repeat) - 55 max words - at ../dataset/shuffle-word-55-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 50 samples - at ../dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 50 samples - at ../dataset/gen-word-5-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (1 token repeat) - 35 max words - at ../dataset/shuffle-word-35-count.jsonl\n",
      "Generated a single JSONL file with 175 samples (1 token repeat) - 15 max words - at ../dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 50 samples - at ../dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 53 samples (1 token repeat) - 50 max words - at ../dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 59 samples (1 token repeat) - 45 max words - at ../dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 50 samples - at ../dataset/gen-word-60-count.jsonl\n",
      "Generated a single JSONL file with 45 samples (1 token repeat) - 60 max words - at ../dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 66 samples (1 token repeat) - 40 max words - at ../dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (1 token repeat) - 65 max words - at ../dataset/shuffle-word-65-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 50 samples - at ../dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 50 samples - at ../dataset/gen-word-70-count.jsonl\n",
      "Generated a single JSONL file with 27 samples (1 token repeat) - 95 max words - at ../dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 39 samples (1 token repeat) - 70 max words - at ../dataset/shuffle-word-70-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 50 samples - at ../dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 50 samples - at ../dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 264 samples (1 token repeat) - 10 max words - at ../dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 87 samples (1 token repeat) - 30 max words - at ../dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 50 samples - at ../dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 50 samples - at ../dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 50 samples - at ../dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 50 samples - at ../dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 541 samples (1 token repeat) - 5 max words - at ../dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 108 samples (1 token repeat) - 25 max words - at ../dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 50 samples - at ../dataset/gen-word-120-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 170 max words - at ../dataset/shuffle-word-170-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 50 samples - at ../dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 32 samples (1 token repeat) - 80 max words - at ../dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 50 samples - at ../dataset/gen-word-100-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (1 token repeat) - 90 max words - at ../dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 28 samples (1 token repeat) - 100 max words - at ../dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 325 max words - at ../dataset/shuffle-word-325-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 50 samples - at ../dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 50 samples - at ../dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 50 samples - at ../dataset/gen-word-175-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 215 max words - at ../dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 19 samples (1 token repeat) - 115 max words - at ../dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 380 max words - at ../dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 265 max words - at ../dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 330 max words - at ../dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 135 max words - at ../dataset/shuffle-word-135-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 50 samples - at ../dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 50 samples - at ../dataset/gen-word-195-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 120 max words - at ../dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 450 max words - at ../dataset/shuffle-word-450-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 50 samples - at ../dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 50 samples - at ../dataset/gen-word-235-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 475 max words - at ../dataset/shuffle-word-475-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 360 max words - at ../dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 110 max words - at ../dataset/shuffle-word-110-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 50 samples - at ../dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 13 samples (1 token repeat) - 190 max words - at ../dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 160 max words - at ../dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (1 token repeat) - 105 max words - at ../dataset/shuffle-word-105-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 50 samples - at ../dataset/gen-word-255-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 185 max words - at ../dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 485 max words - at ../dataset/shuffle-word-485-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 370 max words - at ../dataset/shuffle-word-370-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 50 samples - at ../dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 50 samples - at ../dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 50 samples - at ../dataset/gen-word-140-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 200 max words - at ../dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 11 samples (1 token repeat) - 205 max words - at ../dataset/shuffle-word-205-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 50 samples - at ../dataset/gen-word-160-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 145 max words - at ../dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 17 samples (1 token repeat) - 130 max words - at ../dataset/shuffle-word-130-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 50 samples - at ../dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 50 samples - at ../dataset/gen-word-220-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 465 max words - at ../dataset/shuffle-word-465-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 50 samples - at ../dataset/gen-word-400-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 275 max words - at ../dataset/shuffle-word-275-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 50 samples - at ../dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 50 samples - at ../dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 50 samples - at ../dataset/gen-word-330-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 365 max words - at ../dataset/shuffle-word-365-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 50 samples - at ../dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 50 samples - at ../dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 50 samples - at ../dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 50 samples - at ../dataset/gen-word-260-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 335 max words - at ../dataset/shuffle-word-335-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 50 samples - at ../dataset/gen-word-480-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 490 max words - at ../dataset/shuffle-word-490-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 50 samples - at ../dataset/gen-word-300-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 230 max words - at ../dataset/shuffle-word-230-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 50 samples - at ../dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 50 samples - at ../dataset/gen-word-315-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 390 max words - at ../dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 140 max words - at ../dataset/shuffle-word-140-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 50 samples - at ../dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 50 samples - at ../dataset/gen-word-370-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 405 max words - at ../dataset/shuffle-word-405-count.jsonl\n",
      "Generated JSONL file with - 475 max words, 50 samples - at ../dataset/gen-word-475-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 175 max words - at ../dataset/shuffle-word-175-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 50 samples - at ../dataset/gen-word-430-count.jsonl\n",
      "Generated a single JSONL file with 18 samples (1 token repeat) - 125 max words - at ../dataset/shuffle-word-125-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 50 samples - at ../dataset/gen-word-295-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 210 max words - at ../dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 445 max words - at ../dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 195 max words - at ../dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 14 samples (1 token repeat) - 180 max words - at ../dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 430 max words - at ../dataset/shuffle-word-430-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 50 samples - at ../dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 50 samples - at ../dataset/gen-word-390-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 400 max words - at ../dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 440 max words - at ../dataset/shuffle-word-440-count.jsonl\n",
      "Generated JSONL file with - 495 max words, 50 samples - at ../dataset/gen-word-495-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 250 max words - at ../dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 415 max words - at ../dataset/shuffle-word-415-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 50 samples - at ../dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 50 samples - at ../dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 50 samples - at ../dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 50 samples - at ../dataset/gen-word-445-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 280 max words - at ../dataset/shuffle-word-280-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 50 samples - at ../dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 465 max words, 50 samples - at ../dataset/gen-word-465-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 455 max words - at ../dataset/shuffle-word-455-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 50 samples - at ../dataset/gen-word-405-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 410 max words - at ../dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 270 max words - at ../dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 260 max words - at ../dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 255 max words - at ../dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 320 max words - at ../dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 295 max words - at ../dataset/shuffle-word-295-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 245 max words - at ../dataset/shuffle-word-245-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 480 max words - at ../dataset/shuffle-word-480-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 425 max words - at ../dataset/shuffle-word-425-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 50 samples - at ../dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 50 samples - at ../dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 50 samples - at ../dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 50 samples - at ../dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 50 samples - at ../dataset/gen-word-375-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 350 max words - at ../dataset/shuffle-word-350-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 50 samples - at ../dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 50 samples - at ../dataset/gen-word-440-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 50 samples - at ../dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 50 samples - at ../dataset/gen-word-380-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 300 max words - at ../dataset/shuffle-word-300-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 50 samples - at ../dataset/gen-word-250-count.jsonl\n",
      "Generated a single JSONL file with 9 samples (1 token repeat) - 290 max words - at ../dataset/shuffle-word-290-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 50 samples - at ../dataset/gen-word-320-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 460 max words - at ../dataset/shuffle-word-460-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 50 samples - at ../dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 50 samples - at ../dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 50 samples - at ../dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 50 samples - at ../dataset/gen-word-280-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 155 max words - at ../dataset/shuffle-word-155-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 315 max words - at ../dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 50 samples - at ../dataset/gen-word-130-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 495 max words - at ../dataset/shuffle-word-495-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 50 samples - at ../dataset/gen-word-190-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 500 max words - at ../dataset/shuffle-word-500-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 560 max words - at ../dataset/shuffle-word-560-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 545 max words - at ../dataset/shuffle-word-545-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 395 max words - at ../dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 310 max words - at ../dataset/shuffle-word-310-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 50 samples - at ../dataset/gen-word-165-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 235 max words - at ../dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 470 max words - at ../dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 15 samples (1 token repeat) - 165 max words - at ../dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 50 samples - at ../dataset/gen-word-510-count.jsonl\n",
      "Generated a single JSONL file with 29 samples (1 token repeat) - 85 max words - at ../dataset/shuffle-word-85-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 50 samples - at ../dataset/gen-word-270-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 530 max words - at ../dataset/shuffle-word-530-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 50 samples - at ../dataset/gen-word-490-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 225 max words - at ../dataset/shuffle-word-225-count.jsonl\n",
      "Generated JSONL file with - 535 max words, 50 samples - at ../dataset/gen-word-535-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 635 max words - at ../dataset/shuffle-word-635-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 580 max words - at ../dataset/shuffle-word-580-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 50 samples - at ../dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 635 max words, 50 samples - at ../dataset/gen-word-635-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 670 max words - at ../dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 505 max words - at ../dataset/shuffle-word-505-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 50 samples - at ../dataset/gen-word-450-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 510 max words - at ../dataset/shuffle-word-510-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 745 max words - at ../dataset/shuffle-word-745-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 375 max words - at ../dataset/shuffle-word-375-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 50 samples - at ../dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 50 samples - at ../dataset/gen-word-470-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 50 samples - at ../dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 50 samples - at ../dataset/gen-word-185-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 345 max words - at ../dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 435 max words - at ../dataset/shuffle-word-435-count.jsonl\n",
      "Generated JSONL file with - 455 max words, 50 samples - at ../dataset/gen-word-455-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 285 max words - at ../dataset/shuffle-word-285-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 50 samples - at ../dataset/gen-word-240-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 385 max words - at ../dataset/shuffle-word-385-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 50 samples - at ../dataset/gen-word-245-count.jsonl\n",
      "Generated a single JSONL file with 16 samples (1 token repeat) - 150 max words - at ../dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 685 max words - at ../dataset/shuffle-word-685-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 355 max words - at ../dataset/shuffle-word-355-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 575 max words - at ../dataset/shuffle-word-575-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 640 max words - at ../dataset/shuffle-word-640-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 50 samples - at ../dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 6 samples (1 token repeat) - 420 max words - at ../dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 7 samples (1 token repeat) - 340 max words - at ../dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 690 max words - at ../dataset/shuffle-word-690-count.jsonl\n",
      "Generated JSONL file with - 505 max words, 50 samples - at ../dataset/gen-word-505-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 550 max words - at ../dataset/shuffle-word-550-count.jsonl\n",
      "Generated a single JSONL file with 8 samples (1 token repeat) - 305 max words - at ../dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 710 max words - at ../dataset/shuffle-word-710-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 50 samples - at ../dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 50 samples - at ../dataset/gen-word-530-count.jsonl\n",
      "Generated JSONL file with - 515 max words, 50 samples - at ../dataset/gen-word-515-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 50 samples - at ../dataset/gen-word-105-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 525 max words - at ../dataset/shuffle-word-525-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 540 max words - at ../dataset/shuffle-word-540-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 50 samples - at ../dataset/gen-word-350-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 565 max words - at ../dataset/shuffle-word-565-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 240 max words - at ../dataset/shuffle-word-240-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 695 max words - at ../dataset/shuffle-word-695-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 50 samples - at ../dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 50 samples - at ../dataset/gen-word-560-count.jsonl\n",
      "Generated JSONL file with - 485 max words, 50 samples - at ../dataset/gen-word-485-count.jsonl\n",
      "Generated JSONL file with - 585 max words, 50 samples - at ../dataset/gen-word-585-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 605 max words - at ../dataset/shuffle-word-605-count.jsonl\n",
      "Generated JSONL file with - 625 max words, 50 samples - at ../dataset/gen-word-625-count.jsonl\n",
      "Generated JSONL file with - 735 max words, 50 samples - at ../dataset/gen-word-735-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 50 samples - at ../dataset/gen-word-660-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 50 samples - at ../dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 700 max words - at ../dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 10 samples (1 token repeat) - 220 max words - at ../dataset/shuffle-word-220-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 50 samples - at ../dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 555 max words, 50 samples - at ../dataset/gen-word-555-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 50 samples - at ../dataset/gen-word-600-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 630 max words - at ../dataset/shuffle-word-630-count.jsonl\n",
      "Generated JSONL file with - 715 max words, 50 samples - at ../dataset/gen-word-715-count.jsonl\n",
      "Generated JSONL file with - 575 max words, 50 samples - at ../dataset/gen-word-575-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 50 samples - at ../dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 525 max words, 50 samples - at ../dataset/gen-word-525-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 50 samples - at ../dataset/gen-word-520-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 50 samples - at ../dataset/gen-word-620-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 655 max words - at ../dataset/shuffle-word-655-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 875 max words - at ../dataset/shuffle-word-875-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 665 max words - at ../dataset/shuffle-word-665-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 610 max words - at ../dataset/shuffle-word-610-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 680 max words - at ../dataset/shuffle-word-680-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 785 max words - at ../dataset/shuffle-word-785-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 905 max words - at ../dataset/shuffle-word-905-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 820 max words - at ../dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 585 max words - at ../dataset/shuffle-word-585-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 880 max words - at ../dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 515 max words - at ../dataset/shuffle-word-515-count.jsonl\n",
      "Generated JSONL file with - 685 max words, 50 samples - at ../dataset/gen-word-685-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 50 samples - at ../dataset/gen-word-540-count.jsonl\n",
      "Generated JSONL file with - 695 max words, 50 samples - at ../dataset/gen-word-695-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 775 max words - at ../dataset/shuffle-word-775-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 535 max words - at ../dataset/shuffle-word-535-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 600 max words - at ../dataset/shuffle-word-600-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 960 max words - at ../dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 570 max words - at ../dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 970 max words - at ../dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 885 max words - at ../dataset/shuffle-word-885-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 990 max words - at ../dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 750 max words - at ../dataset/shuffle-word-750-count.jsonl\n",
      "Generated JSONL file with - 665 max words, 50 samples - at ../dataset/gen-word-665-count.jsonl\n",
      "Generated JSONL file with - 675 max words, 50 samples - at ../dataset/gen-word-675-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4700 max words - at ../dataset/shuffle-word-4700-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 935 max words - at ../dataset/shuffle-word-935-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 790 max words - at ../dataset/shuffle-word-790-count.jsonl\n",
      "Generated JSONL file with - 615 max words, 50 samples - at ../dataset/gen-word-615-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 730 max words - at ../dataset/shuffle-word-730-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 50 samples - at ../dataset/gen-word-850-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 835 max words - at ../dataset/shuffle-word-835-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 950 max words - at ../dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 920 max words - at ../dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 590 max words - at ../dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 890 max words - at ../dataset/shuffle-word-890-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 50 samples - at ../dataset/gen-word-710-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2300 max words - at ../dataset/shuffle-word-2300-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 50 samples - at ../dataset/gen-word-820-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 50 samples - at ../dataset/gen-word-640-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 50 samples - at ../dataset/gen-word-670-count.jsonl\n",
      "Generated JSONL file with - 605 max words, 50 samples - at ../dataset/gen-word-605-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1600 max words - at ../dataset/shuffle-word-1600-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 50 samples - at ../dataset/gen-word-920-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 770 max words - at ../dataset/shuffle-word-770-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 50 samples - at ../dataset/gen-word-740-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3300 max words - at ../dataset/shuffle-word-3300-count.jsonl\n",
      "Generated JSONL file with - 915 max words, 50 samples - at ../dataset/gen-word-915-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 50 samples - at ../dataset/gen-word-790-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 50 samples - at ../dataset/gen-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 595 max words - at ../dataset/shuffle-word-595-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3100 max words - at ../dataset/shuffle-word-3100-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 50 samples - at ../dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 765 max words, 50 samples - at ../dataset/gen-word-765-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2900 max words - at ../dataset/shuffle-word-2900-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 615 max words - at ../dataset/shuffle-word-615-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 995 max words - at ../dataset/shuffle-word-995-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 900 max words - at ../dataset/shuffle-word-900-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 925 max words - at ../dataset/shuffle-word-925-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 740 max words - at ../dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 735 max words - at ../dataset/shuffle-word-735-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 955 max words - at ../dataset/shuffle-word-955-count.jsonl\n",
      "Generated JSONL file with - 655 max words, 50 samples - at ../dataset/gen-word-655-count.jsonl\n",
      "Generated JSONL file with - 705 max words, 50 samples - at ../dataset/gen-word-705-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 815 max words - at ../dataset/shuffle-word-815-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 930 max words - at ../dataset/shuffle-word-930-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 765 max words - at ../dataset/shuffle-word-765-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 985 max words - at ../dataset/shuffle-word-985-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 895 max words - at ../dataset/shuffle-word-895-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 50 samples - at ../dataset/gen-word-950-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 845 max words - at ../dataset/shuffle-word-845-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 870 max words - at ../dataset/shuffle-word-870-count.jsonl\n",
      "Generated JSONL file with - 995 max words, 50 samples - at ../dataset/gen-word-995-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4800 max words - at ../dataset/shuffle-word-4800-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 625 max words - at ../dataset/shuffle-word-625-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 825 max words - at ../dataset/shuffle-word-825-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6300 max words - at ../dataset/shuffle-word-6300-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 860 max words - at ../dataset/shuffle-word-860-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 980 max words - at ../dataset/shuffle-word-980-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 910 max words - at ../dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6900 max words - at ../dataset/shuffle-word-6900-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 850 max words - at ../dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 865 max words - at ../dataset/shuffle-word-865-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 555 max words - at ../dataset/shuffle-word-555-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 805 max words - at ../dataset/shuffle-word-805-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5900 max words - at ../dataset/shuffle-word-5900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5800 max words - at ../dataset/shuffle-word-5800-count.jsonl\n",
      "Generated JSONL file with - 545 max words, 50 samples - at ../dataset/gen-word-545-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 780 max words - at ../dataset/shuffle-word-780-count.jsonl\n",
      "Generated JSONL file with - 945 max words, 50 samples - at ../dataset/gen-word-945-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 830 max words - at ../dataset/shuffle-word-830-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 975 max words - at ../dataset/shuffle-word-975-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 795 max words - at ../dataset/shuffle-word-795-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4500 max words - at ../dataset/shuffle-word-4500-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 855 max words - at ../dataset/shuffle-word-855-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 800 max words - at ../dataset/shuffle-word-800-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 50 samples - at ../dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5500 max words - at ../dataset/shuffle-word-5500-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 50 samples - at ../dataset/gen-word-760-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 675 max words - at ../dataset/shuffle-word-675-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 8000 max words - at ../dataset/shuffle-word-8000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6200 max words - at ../dataset/shuffle-word-6200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4300 max words - at ../dataset/shuffle-word-4300-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1900 max words - at ../dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5600 max words - at ../dataset/shuffle-word-5600-count.jsonl\n",
      "Generated JSONL file with - 815 max words, 50 samples - at ../dataset/gen-word-815-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1700 max words - at ../dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 755 max words - at ../dataset/shuffle-word-755-count.jsonl\n",
      "Generated JSONL file with - 875 max words, 50 samples - at ../dataset/gen-word-875-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6400 max words - at ../dataset/shuffle-word-6400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5000 max words - at ../dataset/shuffle-word-5000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5200 max words - at ../dataset/shuffle-word-5200-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7300 max words - at ../dataset/shuffle-word-7300-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 725 max words - at ../dataset/shuffle-word-725-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3400 max words - at ../dataset/shuffle-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6100 max words - at ../dataset/shuffle-word-6100-count.jsonl\n",
      "Generated JSONL file with - 645 max words, 50 samples - at ../dataset/gen-word-645-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 660 max words - at ../dataset/shuffle-word-660-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1500 max words - at ../dataset/shuffle-word-1500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5100 max words - at ../dataset/shuffle-word-5100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7600 max words - at ../dataset/shuffle-word-7600-count.jsonl\n",
      "Generated JSONL file with - 865 max words, 50 samples - at ../dataset/gen-word-865-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4600 max words - at ../dataset/shuffle-word-4600-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 720 max words - at ../dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 5 samples (1 token repeat) - 520 max words - at ../dataset/shuffle-word-520-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 50 samples - at ../dataset/gen-word-890-count.jsonl\n",
      "Generated JSONL file with - 955 max words, 50 samples - at ../dataset/gen-word-955-count.jsonl\n",
      "Generated JSONL file with - 785 max words, 50 samples - at ../dataset/gen-word-785-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 50 samples - at ../dataset/gen-word-830-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6000 max words - at ../dataset/shuffle-word-6000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7700 max words - at ../dataset/shuffle-word-7700-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7500 max words - at ../dataset/shuffle-word-7500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4900 max words - at ../dataset/shuffle-word-4900-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 2800 max words - at ../dataset/shuffle-word-2800-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4100 max words - at ../dataset/shuffle-word-4100-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6500 max words - at ../dataset/shuffle-word-6500-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5300 max words - at ../dataset/shuffle-word-5300-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 760 max words - at ../dataset/shuffle-word-760-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 650 max words - at ../dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 945 max words - at ../dataset/shuffle-word-945-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3600 max words - at ../dataset/shuffle-word-3600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7200 max words - at ../dataset/shuffle-word-7200-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 620 max words - at ../dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2400 max words - at ../dataset/shuffle-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3200 max words - at ../dataset/shuffle-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 645 max words - at ../dataset/shuffle-word-645-count.jsonl\n",
      "Generated JSONL file with - 755 max words, 50 samples - at ../dataset/gen-word-755-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7100 max words - at ../dataset/shuffle-word-7100-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 840 max words - at ../dataset/shuffle-word-840-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 50 samples - at ../dataset/gen-word-700-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 50 samples - at ../dataset/gen-word-860-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2200 max words - at ../dataset/shuffle-word-2200-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 50 samples - at ../dataset/gen-word-840-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 50 samples - at ../dataset/gen-word-910-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 810 max words - at ../dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7800 max words - at ../dataset/shuffle-word-7800-count.jsonl\n",
      "Generated a single JSONL file with 31 samples (20 token repeat) - 2600 max words - at ../dataset/shuffle-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3700 max words - at ../dataset/shuffle-word-3700-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5400 max words - at ../dataset/shuffle-word-5400-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3500 max words - at ../dataset/shuffle-word-3500-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 50 samples - at ../dataset/gen-word-570-count.jsonl\n",
      "Generated JSONL file with - 565 max words, 50 samples - at ../dataset/gen-word-565-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4000 max words - at ../dataset/shuffle-word-4000-count.jsonl\n",
      "Generated JSONL file with - 725 max words, 50 samples - at ../dataset/gen-word-725-count.jsonl\n",
      "Generated JSONL file with - 805 max words, 50 samples - at ../dataset/gen-word-805-count.jsonl\n",
      "Generated JSONL file with - 905 max words, 50 samples - at ../dataset/gen-word-905-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 50 samples - at ../dataset/gen-word-870-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 50 samples - at ../dataset/gen-word-580-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6700 max words - at ../dataset/shuffle-word-6700-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 1000 max words - at ../dataset/shuffle-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3900 max words - at ../dataset/shuffle-word-3900-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 50 samples - at ../dataset/gen-word-770-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 940 max words - at ../dataset/shuffle-word-940-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 50 samples - at ../dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 50 samples - at ../dataset/gen-word-940-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1100 max words - at ../dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2100 max words - at ../dataset/shuffle-word-2100-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 965 max words - at ../dataset/shuffle-word-965-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7000 max words - at ../dataset/shuffle-word-7000-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4200 max words - at ../dataset/shuffle-word-4200-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 2000 max words - at ../dataset/shuffle-word-2000-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 50 samples - at ../dataset/gen-word-630-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 715 max words - at ../dataset/shuffle-word-715-count.jsonl\n",
      "Generated JSONL file with - 935 max words, 50 samples - at ../dataset/gen-word-935-count.jsonl\n",
      "Generated JSONL file with - 975 max words, 50 samples - at ../dataset/gen-word-975-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 4400 max words - at ../dataset/shuffle-word-4400-count.jsonl\n",
      "Generated JSONL file with - 825 max words, 50 samples - at ../dataset/gen-word-825-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 50 samples - at ../dataset/gen-word-880-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 50 samples - at ../dataset/gen-word-780-count.jsonl\n",
      "Generated JSONL file with - 965 max words, 50 samples - at ../dataset/gen-word-965-count.jsonl\n",
      "Generated a single JSONL file with 4 samples (1 token repeat) - 705 max words - at ../dataset/shuffle-word-705-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3800 max words - at ../dataset/shuffle-word-3800-count.jsonl\n",
      "Generated JSONL file with - 795 max words, 50 samples - at ../dataset/gen-word-795-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7900 max words - at ../dataset/shuffle-word-7900-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 50 samples - at ../dataset/gen-word-720-count.jsonl\n",
      "Generated JSONL file with - 855 max words, 50 samples - at ../dataset/gen-word-855-count.jsonl\n",
      "Generated a single JSONL file with 50 samples (20 token repeat) - 1300 max words - at ../dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 7400 max words - at ../dataset/shuffle-word-7400-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 50 samples - at ../dataset/gen-word-960-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 50 samples - at ../dataset/gen-word-690-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6800 max words - at ../dataset/shuffle-word-6800-count.jsonl\n",
      "Generated a single JSONL file with 3 samples (1 token repeat) - 915 max words - at ../dataset/shuffle-word-915-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 50 samples - at ../dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 595 max words, 50 samples - at ../dataset/gen-word-595-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 50 samples - at ../dataset/gen-word-590-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 5700 max words - at ../dataset/shuffle-word-5700-count.jsonl\n",
      "Generated JSONL file with - 835 max words, 50 samples - at ../dataset/gen-word-835-count.jsonl\n",
      "Generated JSONL file with - 845 max words, 50 samples - at ../dataset/gen-word-845-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 6600 max words - at ../dataset/shuffle-word-6600-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 50 samples - at ../dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 50 samples - at ../dataset/gen-word-810-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 50 samples - at ../dataset/gen-word-930-count.jsonl\n",
      "Generated JSONL file with - 775 max words, 50 samples - at ../dataset/gen-word-775-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 50 samples - at ../dataset/gen-word-680-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 50 samples - at ../dataset/gen-word-900-count.jsonl\n",
      "Generated a single JSONL file with 24 samples (20 token repeat) - 2700 max words - at ../dataset/shuffle-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 60 samples (20 token repeat) - 1200 max words - at ../dataset/shuffle-word-1200-count.jsonl\n",
      "Generated JSONL file with - 895 max words, 50 samples - at ../dataset/gen-word-895-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 50 samples - at ../dataset/gen-word-650-count.jsonl\n",
      "Generated a single JSONL file with 38 samples (20 token repeat) - 2500 max words - at ../dataset/shuffle-word-2500-count.jsonl\n",
      "Generated JSONL file with - 745 max words, 50 samples - at ../dataset/gen-word-745-count.jsonl\n",
      "Generated JSONL file with - 885 max words, 50 samples - at ../dataset/gen-word-885-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 50 samples - at ../dataset/gen-word-980-count.jsonl\n",
      "Generated JSONL file with - 925 max words, 50 samples - at ../dataset/gen-word-925-count.jsonl\n",
      "Generated a single JSONL file with 40 samples (20 token repeat) - 1800 max words - at ../dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 41 samples (20 token repeat) - 1400 max words - at ../dataset/shuffle-word-1400-count.jsonl\n",
      "Generated JSONL file with - 985 max words, 50 samples - at ../dataset/gen-word-985-count.jsonl\n",
      "Generated a single JSONL file with 20 samples (20 token repeat) - 3000 max words - at ../dataset/shuffle-word-3000-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 2000 samples - at ../dataset/gen-word-1100-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 2000 samples - at ../dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 2000 samples - at ../dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 2000 samples - at ../dataset/gen-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 2000 samples - at ../dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 2000 samples - at ../dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 2000 samples - at ../dataset/gen-word-2000-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 2000 samples - at ../dataset/gen-word-2100-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 2000 samples - at ../dataset/gen-word-1300-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 2000 samples - at ../dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 2000 samples - at ../dataset/gen-word-2400-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 2000 samples - at ../dataset/gen-word-1900-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 2000 samples - at ../dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 2000 samples - at ../dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 2600 max words, 2000 samples - at ../dataset/gen-word-2600-count.jsonl\n",
      "Generated JSONL file with - 2700 max words, 2000 samples - at ../dataset/gen-word-2700-count.jsonl\n",
      "Generated JSONL file with - 3000 max words, 2000 samples - at ../dataset/gen-word-3000-count.jsonl\n",
      "Generated JSONL file with - 2900 max words, 2000 samples - at ../dataset/gen-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2800 max words, 2000 samples - at ../dataset/gen-word-2800-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 2000 samples - at ../dataset/gen-word-2300-count.jsonl\n",
      "Generated JSONL file with - 3500 max words, 2000 samples - at ../dataset/gen-word-3500-count.jsonl\n",
      "Generated JSONL file with - 3400 max words, 2000 samples - at ../dataset/gen-word-3400-count.jsonl\n",
      "Generated JSONL file with - 3300 max words, 2000 samples - at ../dataset/gen-word-3300-count.jsonl\n",
      "Generated JSONL file with - 4100 max words, 2000 samples - at ../dataset/gen-word-4100-count.jsonl\n",
      "Generated JSONL file with - 3700 max words, 2000 samples - at ../dataset/gen-word-3700-count.jsonl\n",
      "Generated JSONL file with - 3600 max words, 2000 samples - at ../dataset/gen-word-3600-count.jsonl\n",
      "Generated JSONL file with - 3100 max words, 2000 samples - at ../dataset/gen-word-3100-count.jsonl\n",
      "Generated JSONL file with - 3800 max words, 2000 samples - at ../dataset/gen-word-3800-count.jsonl\n",
      "Generated JSONL file with - 4400 max words, 2000 samples - at ../dataset/gen-word-4400-count.jsonl\n",
      "Generated JSONL file with - 3900 max words, 2000 samples - at ../dataset/gen-word-3900-count.jsonl\n",
      "Generated JSONL file with - 4200 max words, 2000 samples - at ../dataset/gen-word-4200-count.jsonl\n",
      "Generated JSONL file with - 4000 max words, 2000 samples - at ../dataset/gen-word-4000-count.jsonl\n",
      "Generated JSONL file with - 4300 max words, 2000 samples - at ../dataset/gen-word-4300-count.jsonl\n",
      "Generated JSONL file with - 4800 max words, 2000 samples - at ../dataset/gen-word-4800-count.jsonl\n",
      "Generated JSONL file with - 3200 max words, 2000 samples - at ../dataset/gen-word-3200-count.jsonl\n",
      "Generated JSONL file with - 5100 max words, 2000 samples - at ../dataset/gen-word-5100-count.jsonl\n",
      "Generated JSONL file with - 4500 max words, 2000 samples - at ../dataset/gen-word-4500-count.jsonl\n",
      "Generated JSONL file with - 4700 max words, 2000 samples - at ../dataset/gen-word-4700-count.jsonl\n",
      "Generated JSONL file with - 5200 max words, 2000 samples - at ../dataset/gen-word-5200-count.jsonl\n",
      "Generated JSONL file with - 5000 max words, 2000 samples - at ../dataset/gen-word-5000-count.jsonl\n",
      "Generated JSONL file with - 4600 max words, 2000 samples - at ../dataset/gen-word-4600-count.jsonl\n",
      "Generated JSONL file with - 4900 max words, 2000 samples - at ../dataset/gen-word-4900-count.jsonl\n",
      "Generated JSONL file with - 5600 max words, 2000 samples - at ../dataset/gen-word-5600-count.jsonl\n",
      "Generated JSONL file with - 5300 max words, 2000 samples - at ../dataset/gen-word-5300-count.jsonl\n",
      "Generated JSONL file with - 5500 max words, 2000 samples - at ../dataset/gen-word-5500-count.jsonl\n",
      "Generated JSONL file with - 5400 max words, 2000 samples - at ../dataset/gen-word-5400-count.jsonl\n",
      "Generated JSONL file with - 5700 max words, 2000 samples - at ../dataset/gen-word-5700-count.jsonl\n",
      "Generated JSONL file with - 5900 max words, 2000 samples - at ../dataset/gen-word-5900-count.jsonl\n",
      "Generated JSONL file with - 5800 max words, 2000 samples - at ../dataset/gen-word-5800-count.jsonl\n",
      "Generated JSONL file with - 6100 max words, 2000 samples - at ../dataset/gen-word-6100-count.jsonl\n",
      "Generated JSONL file with - 6300 max words, 2000 samples - at ../dataset/gen-word-6300-count.jsonl\n",
      "Generated JSONL file with - 6000 max words, 2000 samples - at ../dataset/gen-word-6000-count.jsonl\n",
      "Generated JSONL file with - 6400 max words, 2000 samples - at ../dataset/gen-word-6400-count.jsonl\n",
      "Generated JSONL file with - 6500 max words, 2000 samples - at ../dataset/gen-word-6500-count.jsonl\n",
      "Generated JSONL file with - 6600 max words, 2000 samples - at ../dataset/gen-word-6600-count.jsonl\n",
      "Generated JSONL file with - 7400 max words, 2000 samples - at ../dataset/gen-word-7400-count.jsonl\n",
      "Generated JSONL file with - 7300 max words, 2000 samples - at ../dataset/gen-word-7300-count.jsonl\n",
      "Generated JSONL file with - 6800 max words, 2000 samples - at ../dataset/gen-word-6800-count.jsonl\n",
      "Generated JSONL file with - 6900 max words, 2000 samples - at ../dataset/gen-word-6900-count.jsonl\n",
      "Generated JSONL file with - 7100 max words, 2000 samples - at ../dataset/gen-word-7100-count.jsonl\n",
      "Generated JSONL file with - 6200 max words, 2000 samples - at ../dataset/gen-word-6200-count.jsonl\n",
      "Generated JSONL file with - 7200 max words, 2000 samples - at ../dataset/gen-word-7200-count.jsonl\n",
      "Generated JSONL file with - 7000 max words, 2000 samples - at ../dataset/gen-word-7000-count.jsonl\n",
      "Generated JSONL file with - 7600 max words, 2000 samples - at ../dataset/gen-word-7600-count.jsonl\n",
      "Generated JSONL file with - 6700 max words, 2000 samples - at ../dataset/gen-word-6700-count.jsonl\n",
      "Generated JSONL file with - 7500 max words, 2000 samples - at ../dataset/gen-word-7500-count.jsonl\n",
      "Generated JSONL file with - 7800 max words, 2000 samples - at ../dataset/gen-word-7800-count.jsonl\n",
      "Generated JSONL file with - 7900 max words, 2000 samples - at ../dataset/gen-word-7900-count.jsonl\n",
      "Generated JSONL file with - 8000 max words, 2000 samples - at ../dataset/gen-word-8000-count.jsonl\n",
      "Generated JSONL file with - 7700 max words, 2000 samples - at ../dataset/gen-word-7700-count.jsonl\n",
      "## Done ##\n",
      "total 6.1G\n",
      "-rw-r--r-- 1 root root  10K Aug 20 05:05 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  54K Aug 20 05:05 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 484K Aug 20 05:05 gen-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  57K Aug 20 05:05 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  60K Aug 20 05:05 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  22M Aug 20 05:05 gen-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  63K Aug 20 05:05 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  65K Aug 20 05:05 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  23M Aug 20 05:05 gen-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  66K Aug 20 05:05 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  71K Aug 20 05:05 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  25M Aug 20 05:05 gen-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  72K Aug 20 05:05 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  73K Aug 20 05:05 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  27M Aug 20 05:05 gen-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  73K Aug 20 05:05 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  13K Aug 20 05:05 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  78K Aug 20 05:05 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  29M Aug 20 05:05 gen-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  81K Aug 20 05:05 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  83K Aug 20 05:05 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  31M Aug 20 05:05 gen-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  88K Aug 20 05:05 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  87K Aug 20 05:05 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  33M Aug 20 05:05 gen-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  92K Aug 20 05:05 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  97K Aug 20 05:05 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  35M Aug 20 05:05 gen-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  96K Aug 20 05:05 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  95K Aug 20 05:05 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  37M Aug 20 05:05 gen-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  99K Aug 20 05:05 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  15K Aug 20 05:05 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 102K Aug 20 05:05 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  39M Aug 20 05:05 gen-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root 102K Aug 20 05:05 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 110K Aug 20 05:05 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  41M Aug 20 05:05 gen-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root 110K Aug 20 05:05 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 113K Aug 20 05:05 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  42M Aug 20 05:05 gen-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root 115K Aug 20 05:05 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 121K Aug 20 05:05 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  44M Aug 20 05:05 gen-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root 120K Aug 20 05:05 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 119K Aug 20 05:05 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  46M Aug 20 05:05 gen-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root 123K Aug 20 05:05 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  18K Aug 20 05:05 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 128K Aug 20 05:05 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  48M Aug 20 05:05 gen-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root 125K Aug 20 05:05 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 131K Aug 20 05:05 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  50M Aug 20 05:05 gen-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root 136K Aug 20 05:05 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 140K Aug 20 05:05 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  52M Aug 20 05:05 gen-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root 137K Aug 20 05:05 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 142K Aug 20 05:05 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  54M Aug 20 05:05 gen-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root 144K Aug 20 05:05 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 146K Aug 20 05:05 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  56M Aug 20 05:05 gen-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root 150K Aug 20 05:05 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  20K Aug 20 05:05 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 154K Aug 20 05:05 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  58M Aug 20 05:05 gen-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root 156K Aug 20 05:05 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 155K Aug 20 05:05 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  60M Aug 20 05:05 gen-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root 156K Aug 20 05:05 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 164K Aug 20 05:05 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  61M Aug 20 05:05 gen-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root 161K Aug 20 05:05 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 162K Aug 20 05:05 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  63M Aug 20 05:05 gen-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root 167K Aug 20 05:05 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 174K Aug 20 05:05 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  65M Aug 20 05:05 gen-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root 177K Aug 20 05:05 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  23K Aug 20 05:05 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 176K Aug 20 05:05 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  67M Aug 20 05:05 gen-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root 173K Aug 20 05:05 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 182K Aug 20 05:05 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  69M Aug 20 05:05 gen-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root 181K Aug 20 05:05 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 186K Aug 20 05:05 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  71M Aug 20 05:05 gen-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root 186K Aug 20 05:05 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 192K Aug 20 05:05 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  73M Aug 20 05:05 gen-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root 193K Aug 20 05:05 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 196K Aug 20 05:05 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  75M Aug 20 05:05 gen-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root 198K Aug 20 05:05 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 206K Aug 20 05:05 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  77M Aug 20 05:05 gen-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root 204K Aug 20 05:05 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 207K Aug 20 05:05 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  79M Aug 20 05:05 gen-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root 203K Aug 20 05:05 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 213K Aug 20 05:05 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  80M Aug 20 05:05 gen-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root 208K Aug 20 05:05 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 217K Aug 20 05:05 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  82M Aug 20 05:05 gen-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root 216K Aug 20 05:05 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 218K Aug 20 05:05 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  84M Aug 20 05:05 gen-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root 220K Aug 20 05:05 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 223K Aug 20 05:05 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  86M Aug 20 05:05 gen-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root 229K Aug 20 05:05 gen-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root 231K Aug 20 05:05 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  88M Aug 20 05:05 gen-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root 232K Aug 20 05:05 gen-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root 238K Aug 20 05:05 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  90M Aug 20 05:05 gen-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root 232K Aug 20 05:05 gen-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root 239K Aug 20 05:05 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  92M Aug 20 05:05 gen-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root 240K Aug 20 05:05 gen-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root 245K Aug 20 05:05 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  94M Aug 20 05:05 gen-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root 242K Aug 20 05:05 gen-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.9K Aug 20 05:05 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 20 05:05 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 249K Aug 20 05:05 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  96M Aug 20 05:05 gen-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root 257K Aug 20 05:05 gen-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root 250K Aug 20 05:05 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  97M Aug 20 05:05 gen-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root 251K Aug 20 05:05 gen-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root 257K Aug 20 05:05 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root 100M Aug 20 05:05 gen-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root 264K Aug 20 05:05 gen-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root 268K Aug 20 05:05 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 101M Aug 20 05:05 gen-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root 267K Aug 20 05:05 gen-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root 268K Aug 20 05:05 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root 103M Aug 20 05:05 gen-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root 271K Aug 20 05:05 gen-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  33K Aug 20 05:05 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 272K Aug 20 05:05 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 105M Aug 20 05:05 gen-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root 274K Aug 20 05:05 gen-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root 277K Aug 20 05:05 gen-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 107M Aug 20 05:05 gen-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root 277K Aug 20 05:05 gen-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root 279K Aug 20 05:05 gen-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 109M Aug 20 05:05 gen-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root 288K Aug 20 05:05 gen-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root 290K Aug 20 05:05 gen-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 111M Aug 20 05:05 gen-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root 287K Aug 20 05:05 gen-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root 295K Aug 20 05:05 gen-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root 113M Aug 20 05:05 gen-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root 295K Aug 20 05:05 gen-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 20 05:05 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 301K Aug 20 05:05 gen-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 115M Aug 20 05:05 gen-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root 301K Aug 20 05:05 gen-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root 307K Aug 20 05:05 gen-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 117M Aug 20 05:05 gen-word-6100-count.jsonl\n",
      "-rw-r--r-- 1 root root 306K Aug 20 05:05 gen-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root 305K Aug 20 05:05 gen-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 118M Aug 20 05:05 gen-word-6200-count.jsonl\n",
      "-rw-r--r-- 1 root root 308K Aug 20 05:05 gen-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root 315K Aug 20 05:05 gen-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 120M Aug 20 05:05 gen-word-6300-count.jsonl\n",
      "-rw-r--r-- 1 root root 314K Aug 20 05:05 gen-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root 317K Aug 20 05:05 gen-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root 122M Aug 20 05:05 gen-word-6400-count.jsonl\n",
      "-rw-r--r-- 1 root root 312K Aug 20 05:05 gen-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root  35K Aug 20 05:05 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 324K Aug 20 05:05 gen-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 124M Aug 20 05:05 gen-word-6500-count.jsonl\n",
      "-rw-r--r-- 1 root root 327K Aug 20 05:05 gen-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root 325K Aug 20 05:05 gen-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 126M Aug 20 05:05 gen-word-6600-count.jsonl\n",
      "-rw-r--r-- 1 root root 329K Aug 20 05:05 gen-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root 332K Aug 20 05:05 gen-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 128M Aug 20 05:05 gen-word-6700-count.jsonl\n",
      "-rw-r--r-- 1 root root 334K Aug 20 05:05 gen-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root 336K Aug 20 05:05 gen-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 130M Aug 20 05:05 gen-word-6800-count.jsonl\n",
      "-rw-r--r-- 1 root root 340K Aug 20 05:05 gen-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root 332K Aug 20 05:05 gen-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root 132M Aug 20 05:05 gen-word-6900-count.jsonl\n",
      "-rw-r--r-- 1 root root 344K Aug 20 05:05 gen-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root  39K Aug 20 05:05 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 340K Aug 20 05:05 gen-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 134M Aug 20 05:05 gen-word-7000-count.jsonl\n",
      "-rw-r--r-- 1 root root 350K Aug 20 05:05 gen-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root 346K Aug 20 05:05 gen-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 136M Aug 20 05:05 gen-word-7100-count.jsonl\n",
      "-rw-r--r-- 1 root root 345K Aug 20 05:05 gen-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root 356K Aug 20 05:05 gen-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 138M Aug 20 05:05 gen-word-7200-count.jsonl\n",
      "-rw-r--r-- 1 root root 356K Aug 20 05:05 gen-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root 366K Aug 20 05:05 gen-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 139M Aug 20 05:05 gen-word-7300-count.jsonl\n",
      "-rw-r--r-- 1 root root 364K Aug 20 05:05 gen-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root 362K Aug 20 05:05 gen-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root 141M Aug 20 05:05 gen-word-7400-count.jsonl\n",
      "-rw-r--r-- 1 root root 368K Aug 20 05:05 gen-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root  41K Aug 20 05:05 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 365K Aug 20 05:05 gen-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 143M Aug 20 05:05 gen-word-7500-count.jsonl\n",
      "-rw-r--r-- 1 root root 375K Aug 20 05:05 gen-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root 383K Aug 20 05:05 gen-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 145M Aug 20 05:05 gen-word-7600-count.jsonl\n",
      "-rw-r--r-- 1 root root 380K Aug 20 05:05 gen-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root 384K Aug 20 05:05 gen-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 147M Aug 20 05:05 gen-word-7700-count.jsonl\n",
      "-rw-r--r-- 1 root root 387K Aug 20 05:05 gen-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root 379K Aug 20 05:05 gen-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 149M Aug 20 05:05 gen-word-7800-count.jsonl\n",
      "-rw-r--r-- 1 root root 386K Aug 20 05:05 gen-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root 391K Aug 20 05:05 gen-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root 151M Aug 20 05:05 gen-word-7900-count.jsonl\n",
      "-rw-r--r-- 1 root root 395K Aug 20 05:05 gen-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root  43K Aug 20 05:05 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 389K Aug 20 05:05 gen-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 153M Aug 20 05:05 gen-word-8000-count.jsonl\n",
      "-rw-r--r-- 1 root root 393K Aug 20 05:05 gen-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root 395K Aug 20 05:05 gen-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root 404K Aug 20 05:05 gen-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root 402K Aug 20 05:05 gen-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root 412K Aug 20 05:05 gen-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root 411K Aug 20 05:05 gen-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root 411K Aug 20 05:05 gen-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root 409K Aug 20 05:05 gen-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root 425K Aug 20 05:05 gen-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root  47K Aug 20 05:05 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 420K Aug 20 05:05 gen-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root 418K Aug 20 05:05 gen-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root 421K Aug 20 05:05 gen-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root 432K Aug 20 05:05 gen-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root 429K Aug 20 05:05 gen-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root 431K Aug 20 05:05 gen-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root 421K Aug 20 05:05 gen-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root 439K Aug 20 05:05 gen-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root 435K Aug 20 05:05 gen-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root 442K Aug 20 05:05 gen-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root  50K Aug 20 05:05 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 450K Aug 20 05:05 gen-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root 444K Aug 20 05:05 gen-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root 445K Aug 20 05:05 gen-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root 444K Aug 20 05:05 gen-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root 451K Aug 20 05:05 gen-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root 449K Aug 20 05:05 gen-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root 454K Aug 20 05:05 gen-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root 460K Aug 20 05:05 gen-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root 466K Aug 20 05:05 gen-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root 459K Aug 20 05:05 gen-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root  50K Aug 20 05:05 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 468K Aug 20 05:05 gen-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root 461K Aug 20 05:05 gen-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root 470K Aug 20 05:05 gen-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root 478K Aug 20 05:05 gen-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root 480K Aug 20 05:05 gen-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root 473K Aug 20 05:05 gen-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root 484K Aug 20 05:05 gen-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root 482K Aug 20 05:05 gen-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root 490K Aug 20 05:05 gen-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root 487K Aug 20 05:05 gen-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root  49K Aug 20 05:05 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 524K Aug 20 05:05 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 522K Aug 20 05:05 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 525K Aug 20 05:05 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 20 05:05 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root  44K Aug 20 05:05 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 522K Aug 20 05:05 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 519K Aug 20 05:05 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 518K Aug 20 05:05 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 522K Aug 20 05:05 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 20 05:05 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root  41K Aug 20 05:05 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 520K Aug 20 05:05 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 521K Aug 20 05:05 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 520K Aug 20 05:05 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 519K Aug 20 05:05 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 518K Aug 20 05:05 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root  39K Aug 20 05:05 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 518K Aug 20 05:05 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 516K Aug 20 05:05 shuffle-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 512K Aug 20 05:05 shuffle-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root  33K Aug 20 05:05 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 20 05:05 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 20 05:05 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-455-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-465-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-475-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-485-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-495-count.jsonl\n",
      "-rw-r--r-- 1 root root  87K Aug 20 05:05 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 20 05:05 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-505-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-515-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-525-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-535-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-545-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-555-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-565-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-575-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-585-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-595-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-605-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6100-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-615-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-6200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-625-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-635-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6400-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-645-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6500-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-655-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-6600-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-665-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-6700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-675-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-6800-count.jsonl\n",
      "-rw-r--r-- 1 root root  28K Aug 20 05:05 shuffle-word-685-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-6900-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-695-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-705-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7100-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-715-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7200-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-725-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7300-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-735-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7400-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-745-count.jsonl\n",
      "-rw-r--r-- 1 root root  32K Aug 20 05:05 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7500-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-755-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-7600-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-765-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-7700-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-775-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root 508K Aug 20 05:05 shuffle-word-7800-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-785-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-7900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-795-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root 509K Aug 20 05:05 shuffle-word-8000-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-805-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-815-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-825-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-835-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-845-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-855-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-865-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-875-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-885-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-895-count.jsonl\n",
      "-rw-r--r-- 1 root root  30K Aug 20 05:05 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-905-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-915-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-925-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-935-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-945-count.jsonl\n",
      "-rw-r--r-- 1 root root  29K Aug 20 05:05 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-955-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-965-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-975-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  27K Aug 20 05:05 shuffle-word-985-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  26K Aug 20 05:05 shuffle-word-995-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1K Aug 20 05:05 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ../dataset\n",
    "rm -rf ../dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/word-2-count.jsonl 2 50 &\n",
    "for i in {5..1000..5} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 50 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 1 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 4200 words dataset\n",
    "# \n",
    "for i in {1100..8000..100} \n",
    "do\n",
    "    python ../memory_script/gen_limited_prompt_completion_jsonl.py ../dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ../memory_script/shuffle_limited_prompt_completion_jsonl.py ../dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -lh ../dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1122072072\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1122072072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230820_050544-yxfm5yul\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/yxfm5yul\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [00:00<00:00, 52744.44it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 155.82it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 1122072072\n",
      "[rank: 6] Global seed set to 1122072072\n",
      "[rank: 4] Global seed set to 1122072072\n",
      "[rank: 5] Global seed set to 1122072072\n",
      "[rank: 2] Global seed set to 1122072072\n",
      "[rank: 3] Global seed set to 1122072072\n",
      "[rank: 1] Global seed set to 1122072072\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.01it/s]\n",
      "[rank: 1] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 05:06:34,304] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 05:06:35,676] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 05:06:36,011] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 05:06:36,050] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   0%|                      | 0/154888 [00:00<?, ? examples/s][rank: 2] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 05:06:36,594] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Map (num_proc=64):   0%|           | 101/154888 [00:00<10:05, 255.75 examples/s][rank: 4] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 05:06:36,651] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1122072072\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 05:06:36,683] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 1122072072                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 05:09:44,690] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07014107704162598 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10139107704162598 seconds\n",
      "Time to load fused_adam op: 0.1010735034942627 seconds\n",
      "Time to load fused_adam op: 0.10145974159240723 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10138249397277832 seconds\n",
      "Time to load fused_adam op: 0.10162806510925293 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10174393653869629 seconds\n",
      "Time to load fused_adam op: 0.10182356834411621 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07132101058959961 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10239148139953613 seconds\n",
      "Time to load utils op: 0.10240459442138672 seconds\n",
      "Time to load utils op: 0.1024932861328125 seconds\n",
      "Time to load utils op: 0.10291028022766113 seconds\n",
      "Time to load utils op: 0.10199737548828125 seconds\n",
      "Time to load utils op: 0.10246539115905762 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10285735130310059 seconds\n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006670951843261719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006661415100097656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007398128509521484 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0013005733489990234 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0012068748474121094 seconds\n",
      "Time to load utils op: 0.001367330551147461 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008127689361572266 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00086212158203125 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   6%| | 800/12316 [22:51<5:29:07,  1.71s/it, v_num=5yul, train/loss=6.4/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:53:07<00:00,  1.72s/it, v_num=5yul, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 1/13 [00:00<00:09,  1.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 2/13 [00:01<00:06,  1.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 3/13 [00:01<00:04,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4/13 [00:02<00:05,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 5/13 [00:02<00:04,  1.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 6/13 [00:03<00:03,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 7/13 [00:03<00:03,  1.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 8/13 [00:03<00:02,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 9/13 [00:04<00:02,  1.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 10/13 [00:05<00:01,  1.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11/13 [00:05<00:01,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/13 [00:06<00:00,  1.89it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:53:26<00:00,  1.72s/it, v_num=5yul, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:53:26<00:00,  1.72s/it, v_num=5yul, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:53:41<00:00,  1.72s/it, v_num=5yul, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–ƒâ–‡â–‡â–‚â–…â–‡â–ƒâ–ƒâ–‚â–ˆâ–„â–ˆâ–„â–†â–‚â–„â–„â–ƒâ–…â–‡â–ˆâ–ˆâ–‡â–‡â–â–„â–â–„â–„â–…â–†â–‚â–ƒâ–†â–ƒâ–…â–†â–„â–ˆâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–…â–ˆâ–‡â–‚â–…â–†â–…â–ˆâ–‚â–ƒâ–ƒâ–†â–„â–„â–…â–†â–â–‡â–‡â–†â–†â–â–„â–â–†â–â–…â–ƒâ–‚â–…â–…â–…â–‚â–„â–†â–‚â–‚â–„â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1786\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.47461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.64388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-8k (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/yxfm5yul\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230820_050544-yxfm5yul/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-8k (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=8192 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=2 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-8k.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 20 17:13 ../model/v5-L6-D4096-E0_1-mem-ctx-8k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k/last.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-8k.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-8k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 99.33333333333333% similarity, with 149 matched token, and 1 token mismatch\n",
      "## Model validation for 160 tokens : 99.375% similarity, with 159 matched token, and 1 token mismatch\n",
      "## Model validation for 170 tokens : 98.82352941176471% similarity, with 168 matched token, and 2 token mismatch\n",
      "## Model validation for 180 tokens : 98.88888888888889% similarity, with 178 matched token, and 2 token mismatch\n",
      "## Model validation for 190 tokens : 98.94736842105263% similarity, with 188 matched token, and 2 token mismatch\n",
      "## Model validation for 200 tokens : 99.0% similarity, with 198 matched token, and 2 token mismatch\n",
      "## Model validation for 210 tokens : 99.04761904761905% similarity, with 208 matched token, and 2 token mismatch\n",
      "## Model validation for 220 tokens : 99.0909090909091% similarity, with 218 matched token, and 2 token mismatch\n",
      "## Model validation for 230 tokens : 99.1304347826087% similarity, with 228 matched token, and 2 token mismatch\n",
      "## Model validation for 240 tokens : 99.58333333333333% similarity, with 239 matched token, and 1 token mismatch\n",
      "## Model validation for 250 tokens : 99.2% similarity, with 248 matched token, and 2 token mismatch\n",
      "## Model validation for 260 tokens : 99.23076923076923% similarity, with 258 matched token, and 2 token mismatch\n",
      "## Model validation for 270 tokens : 98.51851851851852% similarity, with 266 matched token, and 4 token mismatch\n",
      "## Model validation for 280 tokens : 98.57142857142858% similarity, with 276 matched token, and 4 token mismatch\n",
      "## Model validation for 290 tokens : 98.62068965517241% similarity, with 286 matched token, and 4 token mismatch\n",
      "## Model validation for 300 tokens : 98.0% similarity, with 294 matched token, and 6 token mismatch\n",
      "## Model validation for 325 tokens : 98.46153846153847% similarity, with 320 matched token, and 5 token mismatch\n",
      "## Model validation for 350 tokens : 97.71428571428571% similarity, with 342 matched token, and 8 token mismatch\n",
      "## Model validation for 375 tokens : 96.53333333333333% similarity, with 362 matched token, and 13 token mismatch\n",
      "## Model validation for 400 tokens : 96.75% similarity, with 387 matched token, and 13 token mismatch\n",
      "## Model validation for 425 tokens : 96.47058823529412% similarity, with 410 matched token, and 15 token mismatch\n",
      "## Model validation for 450 tokens : 96.0% similarity, with 432 matched token, and 18 token mismatch\n",
      "## Model validation for 475 tokens : 95.36842105263158% similarity, with 453 matched token, and 22 token mismatch\n",
      "## Model validation for 500 tokens : 93.8% similarity, with 469 matched token, and 31 token mismatch\n",
      "## Model validation for 525 tokens : 93.9047619047619% similarity, with 493 matched token, and 32 token mismatch\n",
      "## Model validation for 550 tokens : 93.81818181818183% similarity, with 516 matched token, and 34 token mismatch\n",
      "## Model validation for 575 tokens : 92.8695652173913% similarity, with 534 matched token, and 41 token mismatch\n",
      "## Model validation for 600 tokens : 92.66666666666666% similarity, with 556 matched token, and 44 token mismatch\n",
      "## Model validation for 625 tokens : 92.0% similarity, with 575 matched token, and 50 token mismatch\n",
      "## Model validation for 650 tokens : 90.46153846153845% similarity, with 588 matched token, and 62 token mismatch\n",
      "## Model validation for 675 tokens : 90.96296296296296% similarity, with 614 matched token, and 61 token mismatch\n",
      "## Model validation for 700 tokens : 91.0% similarity, with 637 matched token, and 63 token mismatch\n",
      "## Model validation for 750 tokens : 89.2% similarity, with 669 matched token, and 81 token mismatch\n",
      "## Model validation for 800 tokens : 88.0% similarity, with 704 matched token, and 96 token mismatch\n",
      "## Model validation for 850 tokens : 87.52941176470588% similarity, with 744 matched token, and 106 token mismatch\n",
      "## Model validation for 900 tokens : 87.66666666666667% similarity, with 789 matched token, and 111 token mismatch\n",
      "## Model validation for 950 tokens : 85.78947368421052% similarity, with 815 matched token, and 135 token mismatch\n",
      "## Model validation for 1000 tokens : 83.6% similarity, with 836 matched token, and 164 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 1000 tokens : 83.6% similarity, with 836 matched token, and 164 token mismatch\n",
      "## Model validation for 1050 tokens : 81.71428571428572% similarity, with 858 matched token, and 192 token mismatch\n",
      "## Model validation for 1100 tokens : 80.0909090909091% similarity, with 881 matched token, and 219 token mismatch\n",
      "## Model validation for 1150 tokens : 78.34782608695652% similarity, with 901 matched token, and 249 token mismatch\n",
      "## Model validation for 1200 tokens : 77.08333333333334% similarity, with 925 matched token, and 275 token mismatch\n",
      "## Model validation for 1250 tokens : 74.56% similarity, with 932 matched token, and 318 token mismatch\n",
      "## Model validation for 1300 tokens : 73.07692307692307% similarity, with 950 matched token, and 350 token mismatch\n",
      "## Model validation for 1350 tokens : 72.5925925925926% similarity, with 980 matched token, and 370 token mismatch\n",
      "## Model validation for 1400 tokens : 70.71428571428572% similarity, with 990 matched token, and 410 token mismatch\n",
      "## Model validation for 1450 tokens : 68.13793103448276% similarity, with 988 matched token, and 462 token mismatch\n",
      "## Model validation for 1500 tokens : 66.0% similarity, with 990 matched token, and 510 token mismatch\n",
      "## Model validation for 1550 tokens : 65.48387096774194% similarity, with 1015 matched token, and 535 token mismatch\n",
      "## Model validation for 1600 tokens : 63.375% similarity, with 1014 matched token, and 586 token mismatch\n",
      "## Model validation for 1650 tokens : 62.24242424242424% similarity, with 1027 matched token, and 623 token mismatch\n",
      "## Model validation for 1700 tokens : 60.411764705882355% similarity, with 1027 matched token, and 673 token mismatch\n",
      "## Model validation for 1750 tokens : 59.14285714285714% similarity, with 1035 matched token, and 715 token mismatch\n",
      "## Model validation for 1800 tokens : 56.388888888888886% similarity, with 1015 matched token, and 785 token mismatch\n",
      "## Model validation for 1850 tokens : 55.945945945945944% similarity, with 1035 matched token, and 815 token mismatch\n",
      "## Model validation for 1900 tokens : 54.94736842105263% similarity, with 1044 matched token, and 856 token mismatch\n",
      "## Model validation for 1950 tokens : 53.230769230769226% similarity, with 1038 matched token, and 912 token mismatch\n",
      "## Model validation for 2000 tokens : 52.75% similarity, with 1055 matched token, and 945 token mismatch\n",
      "## Model validation for 2050 tokens : 50.926829268292686% similarity, with 1044 matched token, and 1006 token mismatch\n",
      "## Model validation for 2100 tokens : 49.57142857142857% similarity, with 1041 matched token, and 1059 token mismatch\n",
      "## Model validation for 2150 tokens : 47.72093023255814% similarity, with 1026 matched token, and 1124 token mismatch\n",
      "## Model validation for 2200 tokens : 46.409090909090914% similarity, with 1021 matched token, and 1179 token mismatch\n",
      "## Model validation for 2250 tokens : 45.33333333333333% similarity, with 1020 matched token, and 1230 token mismatch\n",
      "## Model validation for 2300 tokens : 43.21739130434782% similarity, with 994 matched token, and 1306 token mismatch\n",
      "## Model validation for 2350 tokens : 41.65957446808511% similarity, with 979 matched token, and 1371 token mismatch\n",
      "## Model validation for 2400 tokens : 40.91666666666667% similarity, with 982 matched token, and 1418 token mismatch\n",
      "## Model validation for 2450 tokens : 39.02040816326531% similarity, with 956 matched token, and 1494 token mismatch\n",
      "## Model validation for 2500 tokens : 37.92% similarity, with 948 matched token, and 1552 token mismatch\n",
      "## Model validation for 2550 tokens : 36.470588235294116% similarity, with 930 matched token, and 1620 token mismatch\n",
      "## Model validation for 2600 tokens : 35.46153846153846% similarity, with 922 matched token, and 1678 token mismatch\n",
      "## Model validation for 2650 tokens : 34.56603773584906% similarity, with 916 matched token, and 1734 token mismatch\n",
      "## Model validation for 2700 tokens : 33.88888888888889% similarity, with 915 matched token, and 1785 token mismatch\n",
      "## Model validation for 2750 tokens : 32.654545454545456% similarity, with 898 matched token, and 1852 token mismatch\n",
      "## Model validation for 2800 tokens : 31.607142857142854% similarity, with 885 matched token, and 1915 token mismatch\n",
      "## Model validation for 2850 tokens : 31.05263157894737% similarity, with 885 matched token, and 1965 token mismatch\n",
      "## Model validation for 2900 tokens : 30.10344827586207% similarity, with 873 matched token, and 2027 token mismatch\n",
      "## Model validation for 2950 tokens : 29.355932203389827% similarity, with 866 matched token, and 2084 token mismatch\n",
      "## Model validation for 3000 tokens : 28.666666666666668% similarity, with 860 matched token, and 2140 token mismatch\n",
      "## Model validation for 3050 tokens : 27.77049180327869% similarity, with 847 matched token, and 2203 token mismatch\n",
      "## Model validation for 3100 tokens : 27.387096774193548% similarity, with 849 matched token, and 2251 token mismatch\n",
      "## Model validation for 3150 tokens : 26.88888888888889% similarity, with 847 matched token, and 2303 token mismatch\n",
      "## Model validation for 3200 tokens : 26.34375% similarity, with 843 matched token, and 2357 token mismatch\n",
      "## Model validation for 3250 tokens : 25.784615384615385% similarity, with 838 matched token, and 2412 token mismatch\n",
      "## Model validation for 3300 tokens : 25.181818181818183% similarity, with 831 matched token, and 2469 token mismatch\n",
      "## Model validation for 3350 tokens : 24.44776119402985% similarity, with 819 matched token, and 2531 token mismatch\n",
      "## Model validation for 3400 tokens : 24.058823529411764% similarity, with 818 matched token, and 2582 token mismatch\n",
      "## Model validation for 3450 tokens : 23.391304347826086% similarity, with 807 matched token, and 2643 token mismatch\n",
      "## Model validation for 3500 tokens : 23.0% similarity, with 805 matched token, and 2695 token mismatch\n",
      "## Model validation for 3550 tokens : 22.7887323943662% similarity, with 809 matched token, and 2741 token mismatch\n",
      "## Model validation for 3600 tokens : 22.47222222222222% similarity, with 809 matched token, and 2791 token mismatch\n",
      "## Model validation for 3650 tokens : 22.054794520547947% similarity, with 805 matched token, and 2845 token mismatch\n",
      "## Model validation for 3700 tokens : 21.783783783783782% similarity, with 806 matched token, and 2894 token mismatch\n",
      "## Model validation for 3750 tokens : 21.52% similarity, with 807 matched token, and 2943 token mismatch\n",
      "## Model validation for 3800 tokens : 21.342105263157897% similarity, with 811 matched token, and 2989 token mismatch\n",
      "## Model validation for 3850 tokens : 21.038961038961038% similarity, with 810 matched token, and 3040 token mismatch\n",
      "## Model validation for 3900 tokens : 20.692307692307693% similarity, with 807 matched token, and 3093 token mismatch\n",
      "## Model validation for 3950 tokens : 20.32911392405063% similarity, with 803 matched token, and 3147 token mismatch\n",
      "## Model validation for 4000 tokens : 20.05% similarity, with 802 matched token, and 3198 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k.pth\" \"none\" 1000 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side track - positional loss expeirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-8k (pos_loss_bias=0.01, train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.position_loss_bias=0.01', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/v5-slim-memory/v5base-mem-template.yaml', '--trainer.logger.init_args.name=v5-L6-D4096-E0.1 - Mem-Tune ctx-8k (pos_loss_bias=0.01, train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/', '--model.lr_init=3e-4', '--model.lr_final=1e-4', '--data.max_token_size=8192', '--model.position_loss_bias=0.01', '--model.ctx_len=4096', '--model.bptt_learning_range=2', '--model.load_model=../model/v5-L6-D4096-E0_1-mem-ctx-4k.pth'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2950559155\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2950559155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230820_110844-zqk4gs7r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-8k (pos_loss_bias=0.01, train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/zqk4gs7r\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 541/541 [00:00<00:00, 101336.12it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 44.45it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-04ac46231e824880_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-343bf3b731d1b31a_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a719fa7c82fc1524.arrow and /root/.cache/huggingface/datasets/json/default-bf2f441e71a48dab/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-764c575791720fb6.arrow\n",
      "Saving the dataset (2/8 shards):  27%|â–Ž| 26631/98521 [00:01<00:03, 18532.42 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (2/8 shards):  31%|â–Ž| 30631/98521 [00:01<00:03, 20667.54 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (3/8 shards):  38%|â–| 36946/98521 [00:01<00:02, 22541.19 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (3/8 shards):  40%|â–| 38946/98521 [00:01<00:02, 20192.98 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (3/8 shards):  44%|â–| 42946/98521 [00:02<00:02, 22012.59 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (4/8 shards):  56%|â–Œ| 55261/98521 [00:02<00:01, 22168.27 exam[rank: 7] Global seed set to 2950559155\n",
      "[rank: 6] Global seed set to 2950559155\n",
      "[rank: 3] Global seed set to 2950559155\n",
      "Saving the dataset (4/8 shards):  60%|â–Œ| 59261/98521 [00:02<00:01, 23230.60 exam[rank: 2] Global seed set to 2950559155\n",
      "[rank: 5] Global seed set to 2950559155\n",
      "[rank: 1] Global seed set to 2950559155\n",
      "[rank: 4] Global seed set to 2950559155\n",
      "[rank: 0] Global seed set to 2950559155                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 11:09:10,298] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 11:09:33,207] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 11:09:34,249] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 11:09:36,006] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 11:09:36,146] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 11:09:36,175] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 11:09:36,223] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2950559155\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 11:09:36,248] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /root/rwkv-x-playground/checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0699014663696289 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10125279426574707 seconds\n",
      "Time to load fused_adam op: 0.10137295722961426 seconds\n",
      "Time to load fused_adam op: 0.1013636589050293 seconds\n",
      "Time to load fused_adam op: 0.10127139091491699 seconds\n",
      "Time to load fused_adam op: 0.10142660140991211 seconds\n",
      "Time to load fused_adam op: 0.10172867774963379 seconds\n",
      "Time to load fused_adam op: 0.10379719734191895 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06940150260925293 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1020667552947998 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10216569900512695 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10216498374938965 seconds\n",
      "Time to load utils op: 0.10230803489685059 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1025087833404541 seconds\n",
      "Time to load utils op: 0.10299444198608398 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10273528099060059 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(215097344, False), (96, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0005958080291748047 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006494522094726562 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005865097045898438 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006580352783203125 seconds\n",
      "Time to load utils op: 0.0013861656188964844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006861686706542969 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006771087646484375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008707046508789062 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 205 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 205 M \n",
      "--------------------------------------\n",
      "1.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,883.118 Total estimated model params size (MB)\n",
      "Epoch 0:   6%| | 800/12316 [23:17<5:35:23,  1.75s/it, v_num=gs7r, train/loss=0.0/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:54:12<00:00,  1.73s/it, v_num=gs7r, train/loss=5\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 1/13 [00:01<00:21,  1.75s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 2/13 [00:02<00:12,  1.17s/it]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 3/13 [00:02<00:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4/13 [00:03<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 5/13 [00:04<00:06,  1.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 6/13 [00:04<00:05,  1.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 7/13 [00:05<00:04,  1.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 8/13 [00:05<00:03,  1.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 9/13 [00:06<00:02,  1.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 10/13 [00:06<00:02,  1.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11/13 [00:07<00:01,  1.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/13 [00:07<00:00,  1.52it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:54:31<00:00,  1.73s/it, v_num=gs7r, train/loss=5\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:54:31<00:00,  1.73s/it, v_num=gs7r, train/loss=5\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆ| 12316/12316 [5:54:49<00:00,  1.73s/it, v_num=gs7r, train/loss=5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len â–…â–ˆâ–‡â–†â–‡â–…â–â–†â–„â–…â–ƒâ–†â–…â–ƒâ–ƒâ–‚â–ƒâ–†â–ˆâ–„â–†â–ƒâ–ˆâ–†â–†â–…â–…â–â–„â–‡â–†â–ƒâ–†â–ƒâ–„â–†â–ƒâ–…â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss â–ˆâ–‡â–‡â–‡â–‡â–‡â–‚â–â–â–‡â–†â–„â–ƒâ–‚â–ƒâ–‡â–‡â–â–…â–…â–‚â–â–†â–‚â–†â–‚â–â–„â–„â–‚â–†â–ƒâ–‚â–…â–â–…â–…â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1786\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 1.09375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.95061\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mv5-L6-D4096-E0.1 - Mem-Tune ctx-8k (pos_loss_bias=0.01, train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/zqk4gs7r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230820_110844-zqk4gs7r/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-mem-template.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Tune ctx-8k (pos_loss_bias=0.01, train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k-p0_01/\" \\\n",
    "        --model.lr_init=3e-4 \\\n",
    "        --model.lr_final=1e-4 \\\n",
    "        --data.max_token_size=8192 \\\n",
    "        --model.position_loss_bias=0.01 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=2 \\\n",
    "        --model.load_model=\"../model/{FILENAME_PREFIX}-mem-ctx-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/v5-L6-D4096-E0_1-mem-ctx-8k/last-v1.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 126 params 1720779520 elements\n",
      "Saving bf16 state dict to ../model/v5-L6-D4096-E0_1-mem-ctx-8k-p0_01.pth\n",
      "-rw-r--r-- 1 root root 3.3G Aug 20 17:14 ../model/v5-L6-D4096-E0_1-mem-ctx-8k-p0_01.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/{FILENAME_PREFIX}-mem-ctx-8k/last-v1.ckpt\" \\\n",
    "        \"../model/{FILENAME_PREFIX}-mem-ctx-8k-p0_01.pth\" \"bf16\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-mem-ctx-8k-p0_01.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 98.66666666666667% similarity, with 74 matched token, and 1 token mismatch\n",
      "## Model validation for 80 tokens : 98.75% similarity, with 79 matched token, and 1 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 98.88888888888889% similarity, with 89 matched token, and 1 token mismatch\n",
      "## Model validation for 95 tokens : 98.94736842105263% similarity, with 94 matched token, and 1 token mismatch\n",
      "## Model validation for 100 tokens : 98.0% similarity, with 98 matched token, and 2 token mismatch\n",
      "## Model validation for 105 tokens : 98.09523809523809% similarity, with 103 matched token, and 2 token mismatch\n",
      "## Model validation for 110 tokens : 98.18181818181819% similarity, with 108 matched token, and 2 token mismatch\n",
      "## Model validation for 115 tokens : 97.3913043478261% similarity, with 112 matched token, and 3 token mismatch\n",
      "## Model validation for 120 tokens : 97.5% similarity, with 117 matched token, and 3 token mismatch\n",
      "## Model validation for 125 tokens : 97.6% similarity, with 122 matched token, and 3 token mismatch\n",
      "## Model validation for 130 tokens : 97.6923076923077% similarity, with 127 matched token, and 3 token mismatch\n",
      "## Model validation for 135 tokens : 97.77777777777777% similarity, with 132 matched token, and 3 token mismatch\n",
      "## Model validation for 140 tokens : 97.85714285714285% similarity, with 137 matched token, and 3 token mismatch\n",
      "## Model validation for 145 tokens : 97.93103448275862% similarity, with 142 matched token, and 3 token mismatch\n",
      "## Model validation for 150 tokens : 97.33333333333334% similarity, with 146 matched token, and 4 token mismatch\n",
      "## Model validation for 160 tokens : 97.5% similarity, with 156 matched token, and 4 token mismatch\n",
      "## Model validation for 170 tokens : 97.6470588235294% similarity, with 166 matched token, and 4 token mismatch\n",
      "## Model validation for 180 tokens : 97.77777777777777% similarity, with 176 matched token, and 4 token mismatch\n",
      "## Model validation for 190 tokens : 97.36842105263158% similarity, with 185 matched token, and 5 token mismatch\n",
      "## Model validation for 200 tokens : 97.5% similarity, with 195 matched token, and 5 token mismatch\n",
      "## Model validation for 210 tokens : 96.66666666666667% similarity, with 203 matched token, and 7 token mismatch\n",
      "## Model validation for 220 tokens : 97.72727272727273% similarity, with 215 matched token, and 5 token mismatch\n",
      "## Model validation for 230 tokens : 97.3913043478261% similarity, with 224 matched token, and 6 token mismatch\n",
      "## Model validation for 240 tokens : 97.08333333333333% similarity, with 233 matched token, and 7 token mismatch\n",
      "## Model validation for 250 tokens : 96.8% similarity, with 242 matched token, and 8 token mismatch\n",
      "## Model validation for 260 tokens : 96.92307692307692% similarity, with 252 matched token, and 8 token mismatch\n",
      "## Model validation for 270 tokens : 95.92592592592592% similarity, with 259 matched token, and 11 token mismatch\n",
      "## Model validation for 280 tokens : 94.64285714285714% similarity, with 265 matched token, and 15 token mismatch\n",
      "## Model validation for 290 tokens : 93.79310344827586% similarity, with 272 matched token, and 18 token mismatch\n",
      "## Model validation for 300 tokens : 94.33333333333334% similarity, with 283 matched token, and 17 token mismatch\n",
      "## Model validation for 325 tokens : 93.53846153846153% similarity, with 304 matched token, and 21 token mismatch\n",
      "## Model validation for 350 tokens : 93.42857142857143% similarity, with 327 matched token, and 23 token mismatch\n",
      "## Model validation for 375 tokens : 90.4% similarity, with 339 matched token, and 36 token mismatch\n",
      "## Model validation for 400 tokens : 89.25% similarity, with 357 matched token, and 43 token mismatch\n",
      "## Model validation for 425 tokens : 87.52941176470588% similarity, with 372 matched token, and 53 token mismatch\n",
      "## Model validation for 450 tokens : 86.8888888888889% similarity, with 391 matched token, and 59 token mismatch\n",
      "## Model validation for 475 tokens : 86.73684210526315% similarity, with 412 matched token, and 63 token mismatch\n",
      "## Model validation for 500 tokens : 85.0% similarity, with 425 matched token, and 75 token mismatch\n",
      "## Model validation for 525 tokens : 84.19047619047619% similarity, with 442 matched token, and 83 token mismatch\n",
      "## Model validation for 550 tokens : 82.9090909090909% similarity, with 456 matched token, and 94 token mismatch\n",
      "## Model validation for 575 tokens : 83.47826086956522% similarity, with 480 matched token, and 95 token mismatch\n",
      "## Model validation for 600 tokens : 82.33333333333334% similarity, with 494 matched token, and 106 token mismatch\n",
      "## Model validation for 625 tokens : 82.08% similarity, with 513 matched token, and 112 token mismatch\n",
      "## Model validation for 650 tokens : 79.84615384615384% similarity, with 519 matched token, and 131 token mismatch\n",
      "## Model validation for 675 tokens : 79.7037037037037% similarity, with 538 matched token, and 137 token mismatch\n",
      "## Model validation for 700 tokens : 78.71428571428571% similarity, with 551 matched token, and 149 token mismatch\n",
      "## Model validation for 750 tokens : 77.33333333333333% similarity, with 580 matched token, and 170 token mismatch\n",
      "## Model validation for 800 tokens : 75.375% similarity, with 603 matched token, and 197 token mismatch\n",
      "## Model validation for 850 tokens : 76.0% similarity, with 646 matched token, and 204 token mismatch\n",
      "## Model validation for 900 tokens : 74.44444444444444% similarity, with 670 matched token, and 230 token mismatch\n",
      "## Model validation for 950 tokens : 73.26315789473684% similarity, with 696 matched token, and 254 token mismatch\n",
      "## Model validation for 1000 tokens : 71.7% similarity, with 717 matched token, and 283 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k-p0_01.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 1000 tokens : 71.7% similarity, with 717 matched token, and 283 token mismatch\n",
      "## Model validation for 1050 tokens : 70.19047619047619% similarity, with 737 matched token, and 313 token mismatch\n",
      "## Model validation for 1100 tokens : 68.45454545454545% similarity, with 753 matched token, and 347 token mismatch\n",
      "## Model validation for 1150 tokens : 65.39130434782608% similarity, with 752 matched token, and 398 token mismatch\n",
      "## Model validation for 1200 tokens : 64.41666666666667% similarity, with 773 matched token, and 427 token mismatch\n",
      "## Model validation for 1250 tokens : 63.519999999999996% similarity, with 794 matched token, and 456 token mismatch\n",
      "## Model validation for 1300 tokens : 62.69230769230769% similarity, with 815 matched token, and 485 token mismatch\n",
      "## Model validation for 1350 tokens : 60.51851851851852% similarity, with 817 matched token, and 533 token mismatch\n",
      "## Model validation for 1400 tokens : 61.5% similarity, with 861 matched token, and 539 token mismatch\n",
      "## Model validation for 1450 tokens : 60.689655172413794% similarity, with 880 matched token, and 570 token mismatch\n",
      "## Model validation for 1500 tokens : 59.53333333333334% similarity, with 893 matched token, and 607 token mismatch\n",
      "## Model validation for 1550 tokens : 57.54838709677419% similarity, with 892 matched token, and 658 token mismatch\n",
      "## Model validation for 1600 tokens : 56.43749999999999% similarity, with 903 matched token, and 697 token mismatch\n",
      "## Model validation for 1650 tokens : 55.333333333333336% similarity, with 913 matched token, and 737 token mismatch\n",
      "## Model validation for 1700 tokens : 54.64705882352941% similarity, with 929 matched token, and 771 token mismatch\n",
      "## Model validation for 1750 tokens : 53.94285714285715% similarity, with 944 matched token, and 806 token mismatch\n",
      "## Model validation for 1800 tokens : 51.83333333333333% similarity, with 933 matched token, and 867 token mismatch\n",
      "## Model validation for 1850 tokens : 50.21621621621621% similarity, with 929 matched token, and 921 token mismatch\n",
      "## Model validation for 1900 tokens : 49.473684210526315% similarity, with 940 matched token, and 960 token mismatch\n",
      "## Model validation for 1950 tokens : 48.15384615384615% similarity, with 939 matched token, and 1011 token mismatch\n",
      "## Model validation for 2000 tokens : 46.0% similarity, with 920 matched token, and 1080 token mismatch\n",
      "## Model validation for 2050 tokens : 44.487804878048784% similarity, with 912 matched token, and 1138 token mismatch\n",
      "## Model validation for 2100 tokens : 43.476190476190474% similarity, with 913 matched token, and 1187 token mismatch\n",
      "## Model validation for 2150 tokens : 41.25581395348837% similarity, with 887 matched token, and 1263 token mismatch\n",
      "## Model validation for 2200 tokens : 40.04545454545455% similarity, with 881 matched token, and 1319 token mismatch\n",
      "## Model validation for 2250 tokens : 40.31111111111111% similarity, with 907 matched token, and 1343 token mismatch\n",
      "## Model validation for 2300 tokens : 39.0% similarity, with 897 matched token, and 1403 token mismatch\n",
      "## Model validation for 2350 tokens : 38.0% similarity, with 893 matched token, and 1457 token mismatch\n",
      "## Model validation for 2400 tokens : 37.708333333333336% similarity, with 905 matched token, and 1495 token mismatch\n",
      "## Model validation for 2450 tokens : 36.3265306122449% similarity, with 890 matched token, and 1560 token mismatch\n",
      "## Model validation for 2500 tokens : 34.72% similarity, with 868 matched token, and 1632 token mismatch\n",
      "## Model validation for 2550 tokens : 33.76470588235294% similarity, with 861 matched token, and 1689 token mismatch\n",
      "## Model validation for 2600 tokens : 32.34615384615385% similarity, with 841 matched token, and 1759 token mismatch\n",
      "## Model validation for 2650 tokens : 31.09433962264151% similarity, with 824 matched token, and 1826 token mismatch\n",
      "## Model validation for 2700 tokens : 30.296296296296298% similarity, with 818 matched token, and 1882 token mismatch\n",
      "## Model validation for 2750 tokens : 29.272727272727273% similarity, with 805 matched token, and 1945 token mismatch\n",
      "## Model validation for 2800 tokens : 28.607142857142858% similarity, with 801 matched token, and 1999 token mismatch\n",
      "## Model validation for 2850 tokens : 28.175438596491226% similarity, with 803 matched token, and 2047 token mismatch\n",
      "## Model validation for 2900 tokens : 27.448275862068964% similarity, with 796 matched token, and 2104 token mismatch\n",
      "## Model validation for 2950 tokens : 26.576271186440675% similarity, with 784 matched token, and 2166 token mismatch\n",
      "## Model validation for 3000 tokens : 26.066666666666666% similarity, with 782 matched token, and 2218 token mismatch\n",
      "## Model validation for 3050 tokens : 25.049180327868854% similarity, with 764 matched token, and 2286 token mismatch\n",
      "## Model validation for 3100 tokens : 24.387096774193548% similarity, with 756 matched token, and 2344 token mismatch\n",
      "## Model validation for 3150 tokens : 23.49206349206349% similarity, with 740 matched token, and 2410 token mismatch\n",
      "## Model validation for 3200 tokens : 22.96875% similarity, with 735 matched token, and 2465 token mismatch\n",
      "## Model validation for 3250 tokens : 22.76923076923077% similarity, with 740 matched token, and 2510 token mismatch\n",
      "## Model validation for 3300 tokens : 21.90909090909091% similarity, with 723 matched token, and 2577 token mismatch\n",
      "## Model validation for 3350 tokens : 21.52238805970149% similarity, with 721 matched token, and 2629 token mismatch\n",
      "## Model validation for 3400 tokens : 20.823529411764703% similarity, with 708 matched token, and 2692 token mismatch\n",
      "## Model validation for 3450 tokens : 20.434782608695652% similarity, with 705 matched token, and 2745 token mismatch\n",
      "## Model validation for 3500 tokens : 19.942857142857143% similarity, with 698 matched token, and 2802 token mismatch\n",
      "## Model validation for 3550 tokens : 19.661971830985916% similarity, with 698 matched token, and 2852 token mismatch\n",
      "## Model validation for 3600 tokens : 19.25% similarity, with 693 matched token, and 2907 token mismatch\n",
      "## Model validation for 3650 tokens : 18.904109589041095% similarity, with 690 matched token, and 2960 token mismatch\n",
      "## Model validation for 3700 tokens : 18.45945945945946% similarity, with 683 matched token, and 3017 token mismatch\n",
      "## Model validation for 3750 tokens : 18.106666666666666% similarity, with 679 matched token, and 3071 token mismatch\n",
      "## Model validation for 3800 tokens : 18.0% similarity, with 684 matched token, and 3116 token mismatch\n",
      "## Model validation for 3850 tokens : 17.584415584415584% similarity, with 677 matched token, and 3173 token mismatch\n",
      "## Model validation for 3900 tokens : 17.307692307692307% similarity, with 675 matched token, and 3225 token mismatch\n",
      "## Model validation for 3950 tokens : 17.11392405063291% similarity, with 676 matched token, and 3274 token mismatch\n",
      "## Model validation for 4000 tokens : 16.575% similarity, with 663 matched token, and 3337 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "!export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "        python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-mem-ctx-8k-p0_01.pth\" \"none\" 1000 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

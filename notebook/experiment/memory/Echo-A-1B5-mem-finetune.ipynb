{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory Finetune)\n",
    "This continues off from `Echo-A-1B5-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init required dirs\n",
    "!mkdir -p ../../../model/\n",
    "!mkdir -p ../../../datapath/\n",
    "!mkdir -p ../../../checkpoint/\n",
    "\n",
    "# Download the Stage2.pth file\n",
    "!rm -rf ../../../model/Echo-A-1B5-Stage2.pth\n",
    "!cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Stage2.pth\n",
    "!ls -alh ../../../model/Echo-A-1B5-Stage2.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 20000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 20000 samples - at ./dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 20000 samples - at ./dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 10000 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 20000 samples - at ./dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/word-80-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-2-count.jsonl  2  20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-5-count.jsonl  5  20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-10-count.jsonl 10 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-20-count.jsonl 20 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-40-count.jsonl 40 20000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-80-count.jsonl 80 20000 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-100-count.jsonl 100 10000 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2_offload\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 : Simple Memory finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/picocreator/.cache/huggingface/datasets/json/default-6bf53b52af0b7239/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10034.22it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 322.86it/s]\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/json/default-6bf53b52af0b7239/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 58.81it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-mem-finetune/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-11 20:58:08,627] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1487066888\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1487066888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230711_205810-aq3rvmce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEcho-A-1B5 - Mem-Finetune-1 (bs=256, train-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/aq3rvmce\u001b[0m\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       128\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 128\n",
      "   - effective_batch_size:    128\n",
      "\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset json (/home/picocreator/.cache/huggingface/datasets/json/default-6bf53b52af0b7239/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 356.90it/s]\n",
      "[rank: 0] Global seed set to 1487066888                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-11 20:59:18,835] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-11 20:59:18,835] [WARNING] [deepspeed.py:638:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 2.000e-04 (0.0002)\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.336179733276367 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 4122/123500 [28:43<13:51:50,  2.39it/s, v_num=vmce, train/loss="
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generating rwkv_zero_to_fp32.py file\n",
      "# Running rwkv_zero_to_fp32.py file, exporting the RWKV model\n",
      "[2023-07-11 13:57:22,867] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint './checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.gradients, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.9.5\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to rwkv_model.pth\n",
      "# Exported RWKV model\n",
      "# RWKV fp32 model is located at ../checkpoint/Echo-A-1B5-mem-finetune/last.ckpt/rwkv_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Echo-A-1B5-mem-finetune/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 picocreator picocreator 5.7G Jul 11 13:58 ../model/Echo-A-1B5-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets move and save this model\n",
    "!cd \"{TRAINER_DIR}\" && cp ../checkpoint/Echo-A-1B5-mem-finetune/last.ckpt/rwkv_model.pth ../model/Echo-A-1B5-Tune1.pth\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-A-1B5-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /home/picocreator/rwkv-proj/picocreator-memory-experiment/model/Echo-A-1B5-Tune1.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cuda [float32, float32], store 25 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   2048  2048 \n",
      "blocks.0.att.output.weight        f32   cuda:0   2048  2048 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   2048       \n",
      "blocks.0.att.value.weight         f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   8192  2048 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln1.weight               f32   cuda:0   2048       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln2.weight               f32   cuda:0   2048       \n",
      "................................................................................................................................................................................................................................................\n",
      "blocks.23.att.key.weight          f32   cuda:0   2048  2048 \n",
      "blocks.23.att.output.weight       f32   cuda:0   2048  2048 \n",
      "blocks.23.att.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.att.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_v          f32   cuda:0   2048       \n",
      "blocks.23.att.value.weight        f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.key.weight          f32   cuda:0   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.ffn.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.ffn.value.weight        f32   cuda:0   8192  2048 \n",
      "blocks.23.ln1.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln1.weight              f32   cuda:0   2048       \n",
      "blocks.23.ln2.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln2.weight              f32   cuda:0   2048       \n",
      "................................................................................................................\n",
      "emb.weight                        f32      cpu  50277  2048 \n",
      "head.weight                       f32   cuda:0   2048 50277 \n",
      "ln_out.bias                       f32   cuda:0   2048       \n",
      "ln_out.weight                     f32   cuda:0   2048       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_decay          f32   cuda:0   2048       \n",
      ".......\n",
      "blocks.0.att.time_first           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_first          f32   cuda:0   2048       \n",
      ".......###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 40.0% similarity, with 2 matched token, and 3 token mismatch\n",
      "Model validation at 10 tokens : 20.0% similarity, with 2 matched token, and 8 token mismatch\n",
      "Model validation at 15 tokens : 33.33333333333333% similarity, with 5 matched token, and 10 token mismatch\n",
      "Model validation at 20 tokens : 20.0% similarity, with 4 matched token, and 16 token mismatch\n",
      "Model validation at 25 tokens : 16.0% similarity, with 4 matched token, and 21 token mismatch\n",
      "Model validation at 30 tokens : 13.333333333333334% similarity, with 4 matched token, and 26 token mismatch\n",
      "Model validation at 35 tokens : 8.571428571428571% similarity, with 3 matched token, and 32 token mismatch\n",
      "Model validation at 40 tokens : 5.0% similarity, with 2 matched token, and 38 token mismatch\n",
      "Model validation at 45 tokens : 4.444444444444445% similarity, with 2 matched token, and 43 token mismatch\n",
      "Model validation at 50 tokens : 6.0% similarity, with 3 matched token, and 47 token mismatch\n",
      "Model validation at 55 tokens : 5.454545454545454% similarity, with 3 matched token, and 52 token mismatch\n",
      "Model validation at 60 tokens : 5.0% similarity, with 3 matched token, and 57 token mismatch\n",
      "Model validation at 65 tokens : 4.615384615384616% similarity, with 3 matched token, and 62 token mismatch\n",
      "Model validation at 70 tokens : 4.285714285714286% similarity, with 3 matched token, and 67 token mismatch\n",
      "Model validation at 75 tokens : 4.0% similarity, with 3 matched token, and 72 token mismatch\n",
      "Model validation at 80 tokens : 3.75% similarity, with 3 matched token, and 77 token mismatch\n",
      "Model validation at 85 tokens : 4.705882352941177% similarity, with 4 matched token, and 81 token mismatch\n",
      "Model validation at 90 tokens : 5.555555555555555% similarity, with 5 matched token, and 85 token mismatch\n",
      "Model validation at 95 tokens : 5.263157894736842% similarity, with 5 matched token, and 90 token mismatch\n",
      "Model validation at 100 tokens : 5.0% similarity, with 5 matched token, and 95 token mismatch\n",
      "Model validation at 110 tokens : 4.545454545454546% similarity, with 5 matched token, and 105 token mismatch\n",
      "Model validation at 120 tokens : 4.166666666666666% similarity, with 5 matched token, and 115 token mismatch\n",
      "Model validation at 130 tokens : 3.8461538461538463% similarity, with 5 matched token, and 125 token mismatch\n",
      "Model validation at 140 tokens : 3.571428571428571% similarity, with 5 matched token, and 135 token mismatch\n",
      "Model validation at 150 tokens : 5.333333333333334% similarity, with 8 matched token, and 142 token mismatch\n",
      "Model validation at 175 tokens : 5.142857142857142% similarity, with 9 matched token, and 166 token mismatch\n",
      "Model validation at 200 tokens : 5.5% similarity, with 11 matched token, and 189 token mismatch\n",
      "Model validation at 225 tokens : 4.444444444444445% similarity, with 10 matched token, and 215 token mismatch\n",
      "Model validation at 250 tokens : 4.3999999999999995% similarity, with 11 matched token, and 239 token mismatch\n",
      "Model validation at 275 tokens : 3.6363636363636362% similarity, with 10 matched token, and 265 token mismatch\n",
      "Model validation at 300 tokens : 3.6666666666666665% similarity, with 11 matched token, and 289 token mismatch\n",
      "Model validation at 325 tokens : 3.3846153846153846% similarity, with 11 matched token, and 314 token mismatch\n",
      "Model validation at 350 tokens : 3.1428571428571432% similarity, with 11 matched token, and 339 token mismatch\n",
      "Model validation at 375 tokens : 3.4666666666666663% similarity, with 13 matched token, and 362 token mismatch\n",
      "Model validation at 400 tokens : 3.25% similarity, with 13 matched token, and 387 token mismatch\n",
      "Model validation at 425 tokens : 3.058823529411765% similarity, with 13 matched token, and 412 token mismatch\n",
      "Model validation at 450 tokens : 2.888888888888889% similarity, with 13 matched token, and 437 token mismatch\n",
      "Model validation at 475 tokens : 2.736842105263158% similarity, with 13 matched token, and 462 token mismatch\n",
      "Model validation at 500 tokens : 3.0% similarity, with 15 matched token, and 485 token mismatch\n",
      "Model validation at 550 tokens : 2.909090909090909% similarity, with 16 matched token, and 534 token mismatch\n",
      "Model validation at 600 tokens : 2.666666666666667% similarity, with 16 matched token, and 584 token mismatch\n",
      "Model validation at 650 tokens : 2.6153846153846154% similarity, with 17 matched token, and 633 token mismatch\n",
      "Model validation at 700 tokens : 2.4285714285714284% similarity, with 17 matched token, and 683 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-A-1B5-Tune1.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

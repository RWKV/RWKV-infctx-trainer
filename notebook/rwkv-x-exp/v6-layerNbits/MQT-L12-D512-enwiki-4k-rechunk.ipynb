{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L12 Channel Mix & Time Mix Quantized Training\n",
    "\n",
    "Starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits\n",
      "TRAINER_DIR: /home/recursal/RWKV-infctx-trainer/RWKV-v6-QT\n",
      "PROJECT_DIR: /home/recursal/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"MQT-\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v6-QT/\"))\n",
    "\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=12\n",
    "D_SIZE=512\n",
    "\n",
    "# Deepspeed and batch size\n",
    "DEEPSPEED_STAGE=\"deepspeed_stage_2\"\n",
    "BATCH_SIZE=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:37<00:00, 26659.45 examples/s]\n",
      "Filter (num_proc=160): 100%|â–ˆ| 1000000/1000000 [00:06<00:00, 149382.73 examples/\n",
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 472276/472276 [00:24<00:00, 19407.40 examples/s]\n",
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124119/124119 [00:20<00:00, 5977.08 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|â–ˆ| 124119/124119 [00:03<00:00, 39297.29 ex\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 100/100 [00:00<00:00, 4449.34 examples/\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_100k-world-4k-rechunk.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-22 05:29:04,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-v6base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Output model exists, skipping init_model\n",
      "-rw-rw-r-- 1 recursal recursal 170M Apr 22 02:46 /home/recursal/RWKV-infctx-trainer/RWKV-v6/../model/L6-D512-world-v6base-init.pth\n"
     ]
    }
   ],
   "source": [
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=12\n",
    "D_SIZE=512\n",
    "\n",
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer {L_SIZE} --n_embd {D_SIZE} \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\"\n",
    "!ls -lh \"{TRAINER_DIR}/../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L12-D512 : Multiple quantization perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-28 05:33:37,910] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_RKV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_RKV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_RKV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_RKV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-28 05:33:43,220] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,223] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,244] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,249] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,252] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,267] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:33:43,441] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (RKV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 4] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240428_053402-9ej6im9i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-Qnf4-TM_RKV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/9ej6im9i\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06028437614440918 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06313943862915039 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07134056091308594 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2019362449645996 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2019340991973877 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30193614959716797 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.3014566898345947 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20233964920043945 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 31.2 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "98.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "98.3 M    Total params\n",
      "393.093   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:59<17:00,  0.21it/s, v_num=im9i, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:59<00:00,  0.21it/s, v_num=im9i, train/loss=4.880]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:59<00:00,  0.21it/s, v_num=im9i, train/loss=4.880]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.042 MB of 0.042 MB uploaded (0.003 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–…â–ˆâ–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 27.37406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-Qnf4-TM_RKV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/9ej6im9i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240428_053402-9ej6im9i/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-28 05:59:20,387] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-28 05:59:26,176] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,190] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,232] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,246] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,347] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 05:59:26,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240428_055944-c9lagae1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/c9lagae1\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06426787376403809 seconds\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07622432708740234 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06455755233764648 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2017519474029541 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20209741592407227 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20200443267822266 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20253515243530273 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2023022174835205 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 34.3 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "101 M     Trainable params\n",
      "0         Non-trainable params\n",
      "101 M     Total params\n",
      "405.676   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:53<16:48,  0.21it/s, v_num=gae1, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:53<00:00,  0.21it/s, v_num=gae1, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:53<00:00,  0.21it/s, v_num=gae1, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 27.49133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/c9lagae1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240428_055944-c9lagae1/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-28 06:24:53,437] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_KV-CM_0/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_KV-CM_0 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-Qnf4-TM_KV-CM_0/', '--trainer.logger.init_args.name=MQT-L12-D512-Qnf4-TM_KV-CM_0 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-28 06:24:59,064] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,074] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,086] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,101] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,104] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,246] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-28 06:24:59,275] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (0)\n",
      "====================================================================\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240428_062517-mumnu26b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-Qnf4-TM_KV-CM_0 (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/mumnu26b\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.061156272888183594 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06708765029907227 seconds\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10209155082702637 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10270047187805176 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10170698165893555 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10185790061950684 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20209646224975586 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20184659957885742 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 37.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "104 M     Trainable params\n",
      "0         Non-trainable params\n",
      "104 M     Total params\n",
      "418.259   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [08:04<17:11,  0.21it/s, v_num=u26b, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [25:18<00:00,  0.21it/s, v_num=u26b, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [25:18<00:00,  0.21it/s, v_num=u26b, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–‡â–ˆâ–†â–†â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 27.02485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-Qnf4-TM_KV-CM_0 (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/mumnu26b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240428_062517-mumnu26b/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"0\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L12-D512 : Multiple quantization perf + Layer repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-29 18:17:06,406] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-29 18:17:11,931] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:11,933] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:11,935] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:11,944] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:12,180] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:12,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:17:12,228] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240429_181731-m1ol8dn1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/m1ol8dn1\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06472373008728027 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06379103660583496 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Time to load fused_adam op: 0.10167336463928223 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06721830368041992 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20185184478759766 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20206427574157715 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20227360725402832 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20223617553710938 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 23.3 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "90.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.4 M    Total params\n",
      "361.636   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:33<16:05,  0.22it/s, v_num=8dn1, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:37<00:00,  0.22it/s, v_num=8dn1, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:37<00:00,  0.22it/s, v_num=8dn1, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.018 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–ƒâ–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 28.94757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-CLR-2_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/m1ol8dn1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240429_181731-m1ol8dn1/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_CMIX_REUSE_VARS=\"KV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-29 18:41:26,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-29 18:41:31,999] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,020] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,072] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,144] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,204] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,217] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 18:41:32,243] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (R)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240429_184150-8g1r4uwu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/8g1r4uwu\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06481218338012695 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10156941413879395 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10181879997253418 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10198402404785156 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10233473777770996 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10198760032653809 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06815528869628906 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20223522186279297 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 34.3 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "101 M     Trainable params\n",
      "0         Non-trainable params\n",
      "101 M     Total params\n",
      "405.676   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:52<16:46,  0.21it/s, v_num=4uwu, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:44<00:00,  0.21it/s, v_num=4uwu, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [24:44<00:00,  0.21it/s, v_num=4uwu, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 27.64894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-CLR-2_R-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/8g1r4uwu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240429_184150-8g1r4uwu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_CMIX_REUSE_VARS=\"R\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-29 19:06:53,179] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-29 19:06:59,149] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,157] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,257] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,356] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,408] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,426] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:06:59,533] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240429_190718-r8b38g6q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/r8b38g6q\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "Loading extension module fused_adam...\n",
      "\n",
      "Time to load fused_adam op: 0.06438422203063965 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07707095146179199 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10176920890808105 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20180010795593262 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2019808292388916 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2020580768585205 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2031238079071045 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20193123817443848 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 23.3 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "90.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.4 M    Total params\n",
      "361.636   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:40<16:20,  0.22it/s, v_num=8g6q, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:59<00:00,  0.22it/s, v_num=8g6q, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:59<00:00,  0.22it/s, v_num=8g6q, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–†â–†â–†â–†â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 28.51217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-CLR-2_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/r8b38g6q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240429_190718-r8b38g6q/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-29 19:31:34,807] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-29 19:31:40,520] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,520] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,560] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,589] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,777] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 19:31:40,828] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 4] Seed set to 3941088705\n",
      "[rank: 2] Seed set to 3941088705\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240429_193159-pyibnvcb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/pyibnvcb\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06344270706176758 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0683290958404541 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20205068588256836 seconds\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06973481178283691 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30206990242004395 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30162835121154785 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30225706100463867 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.302457332611084 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 17.8 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "84.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "84.9 M    Total params\n",
      "339.616   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:32<16:02,  0.22it/s, v_num=nvcb, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:41<00:00,  0.22it/s, v_num=nvcb, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:41<00:00,  0.22it/s, v_num=nvcb, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–†â–‡â–‡â–†â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 28.8748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/pyibnvcb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240429_193159-pyibnvcb/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-29 21:18:47,603] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-4_KV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-4_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-CLR-4_KV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-CLR-4_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-29 21:18:52,914] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:52,933] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:52,937] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:52,941] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:52,991] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:53,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-29 21:18:53,214] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (KV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240429_211912-0ojknbfu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-CLR-4_KV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/0ojknbfu\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.062483787536621094 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0660398006439209 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10218429565429688 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10160231590270996 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10204482078552246 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20205259323120117 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20231056213378906 seconds\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.20262956619262695 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 17.8 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "84.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "84.9 M    Total params\n",
      "339.616   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:33<16:05,  0.22it/s, v_num=nbfu, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:  43%|â–Š | 134/313 [10:07<13:31,  0.22it/s, v_num=nbfu, train/loss=5.470]"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"KV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"R\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"KV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"R\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"1\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C and R mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 07:12:57,976] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_R-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_R-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_R-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_R-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 07:13:03,403] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,413] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,425] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,478] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,565] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 07:13:03,639] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (R)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_071322-dpsztt5n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-2_R-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/dpsztt5n\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.061532020568847656 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06486845016479492 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10175204277038574 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2020418643951416 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10204100608825684 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20154881477355957 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20218539237976074 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06817150115966797 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 16.2 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "83.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "83.3 M    Total params\n",
      "333.324   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:   4%|   | 13/313 [00:59<22:52,  0.22it/s, v_num=tt5n, train/loss=8.190]"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"R\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"KV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 12:13:50,602] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 12:13:55,940] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:55,941] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:55,954] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:56,035] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:56,122] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:56,190] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:13:56,234] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVO)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_121414-ghgd6j4y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/ghgd6j4y\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06510233879089355 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06952643394470215 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06696557998657227 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20217466354370117 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...Loading extension module fused_adam...\n",
      "\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30181407928466797 seconds\n",
      "Time to load fused_adam op: 0.20215463638305664 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.3017857074737549 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.3025040626525879 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 14.6 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "81.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.8 M    Total params\n",
      "327.033   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:26<15:51,  0.22it/s, v_num=6j4y, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:23<00:00,  0.22it/s, v_num=6j4y, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:23<00:00,  0.22it/s, v_num=6j4y, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–ƒâ–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–„â–„â–„â–„â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.24643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-2_RKVO-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/ghgd6j4y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_121414-ghgd6j4y/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKVO\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 12:37:54,943] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 12:38:00,959] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:00,963] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:00,984] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:00,984] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:01,159] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:01,303] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 12:38:01,328] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_123819-l6z834hb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/l6z834hb\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.062652587890625 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10162663459777832 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1015324592590332 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10167312622070312 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1020967960357666 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10196328163146973 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10160565376281738 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07304739952087402 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 14.6 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "81.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.8 M    Total params\n",
      "327.033   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:25<15:49,  0.22it/s, v_num=34hb, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:16<00:00,  0.22it/s, v_num=34hb, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:16<00:00,  0.22it/s, v_num=34hb, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–„â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.39767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-2_RKVG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/l6z834hb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_123819-l6z834hb/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKVG\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 13:01:51,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 13:01:57,819] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:57,821] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:57,822] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:57,931] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:57,960] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:58,072] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:01:58,211] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKVOG)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_130216-tfkkp4t0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/tfkkp4t0\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06253647804260254 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10176396369934082 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1016089916229248 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10193252563476562 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10178351402282715 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10176801681518555 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10196566581726074 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10160613059997559 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 13.1 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "80.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.2 M    Total params\n",
      "320.741   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:25<15:49,  0.22it/s, v_num=p4t0, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:16<00:00,  0.22it/s, v_num=p4t0, train/loss=4.840]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:16<00:00,  0.22it/s, v_num=p4t0, train/loss=4.840]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.018 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–…â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.38438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-2_RKVOG-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/tfkkp4t0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_130216-tfkkp4t0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKVOG\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 13:25:49,110] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 13:25:55,342] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,361] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,363] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,365] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,549] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:25:55,577] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 2 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_132614-b0vel7vc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/b0vel7vc\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.062213897705078125 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06677651405334473 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0731039047241211 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2017991542816162 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20200157165527344 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20198512077331543 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20237135887145996 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.3021690845489502 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 14.4 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "81.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.5 M    Total params\n",
      "325.984   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:31<16:02,  0.22it/s, v_num=l7vc, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:31<00:00,  0.22it/s, v_num=l7vc, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:31<00:00,  0.22it/s, v_num=l7vc, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–‡â–„â–‚â–‚â–â–‚â–„â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.07601\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-2_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/b0vel7vc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_132614-b0vel7vc/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"2\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 13:50:02,136] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 13:50:07,694] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:07,698] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:07,715] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:07,981] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:08,032] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:08,034] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 13:50:08,055] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 3] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_135026-koxyd3bm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/koxyd3bm\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05974411964416504 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07185578346252441 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10170984268188477 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20209431648254395 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20208168029785156 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20218968391418457 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.20243144035339355 seconds\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.201951265335083 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 15.4 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "82.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "82.5 M    Total params\n",
      "330.179   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:30<15:58,  0.22it/s, v_num=d3bm, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:30<00:00,  0.22it/s, v_num=d3bm, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:30<00:00,  0.22it/s, v_num=d3bm, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–…â–‡â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–†â–†â–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.09933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-4_RKV-CLR-4_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/koxyd3bm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_135026-koxyd3bm/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 14:30:13,816] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 14:30:19,438] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,443] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,443] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,466] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,663] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,698] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:30:19,715] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 4 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 6] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_143038-oem9i9mj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/oem9i9mj\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06555628776550293 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10163021087646484 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10142660140991211 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06580567359924316 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06746578216552734 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20193123817443848 seconds\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.30199742317199707 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30181312561035156 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 13.6 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "80.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.7 M    Total params\n",
      "322.839   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:28<15:56,  0.22it/s, v_num=i9mj, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:23<00:00,  0.22it/s, v_num=i9mj, train/loss=4.810]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:23<00:00,  0.22it/s, v_num=i9mj, train/loss=4.810]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.25213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-4_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/oem9i9mj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_143038-oem9i9mj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"4\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 14:54:19,030] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 14:54:25,220] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,452] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,500] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,503] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,545] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 14:54:25,547] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 6 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 1] Seed set to 3941088705\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_145444-l4ot46o8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/l4ot46o8\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Time to load fused_adam op: 0.062163591384887695 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07254528999328613 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1017913818359375 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10174298286437988 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.1016836166381836 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1027224063873291 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20185494422912598 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20197772979736328 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 13.3 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "80.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.4 M    Total params\n",
      "321.790   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:27<15:53,  0.22it/s, v_num=46o8, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:22<00:00,  0.22it/s, v_num=46o8, train/loss=4.780]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:22<00:00,  0.22it/s, v_num=46o8, train/loss=4.780]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–…â–…â–…â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.25936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-6_RKV-CLR-6_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/l4ot46o8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_145444-l4ot46o8/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"6\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-30 16:51:24,401] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L12-D512-world-v6base-init.pth', '--model.lr_init=3e-4', '--model.lr_final=3e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R/', '--trainer.logger.init_args.name=MQT-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-30 16:51:29,952] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:29,986] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:29,996] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:29,999] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:30,023] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:30,220] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-30 16:51:30,223] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] CMIX reuse multiplier : 12 (RKV)\n",
      "[RWKV] TMIX Quantize type    : nf4 (KV)\n",
      "[RWKV] CMIX Quantize type    : nf4 (R)\n",
      "====================================================================\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240430_165149-zekduzfp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMQT-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/zekduzfp\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0634768009185791 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06786346435546875 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1014409065246582 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07356977462768555 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06784915924072266 seconds\n",
      "Time to load fused_adam op: 0.2016887664794922 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.30190587043762207 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.3021092414855957 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 11.2 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "78.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "78.4 M    Total params\n",
      "313.401   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [07:23<15:43,  0.23it/s, v_num=uzfp, train/loss=5.690]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:13<00:00,  0.22it/s, v_num=uzfp, train/loss=4.780]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [23:13<00:00,  0.22it/s, v_num=uzfp, train/loss=4.780]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.032 MB of 0.032 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 29.45795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 4.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 4.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mMQT-L12-D512-TLR-12_RKV-CLR-12_RKV-Qnf4-TM_KV-CM_R (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/zekduzfp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240430_165149-zekduzfp/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"3e-4\"\n",
    "LR_FINAL=\"3e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"nf4\"\n",
    "\n",
    "# mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "RWKV_TMIX_QVARS=\"KV\"\n",
    "\n",
    "RWKV_CMIX_REUSE_MULTIPLIER=\"12\"\n",
    "RWKV_CMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "RWKV_TMIX_REUSE_MULTIPLIER=\"12\"\n",
    "RWKV_TMIX_REUSE_VARS=\"RKV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_QVARS=\"{RWKV_TMIX_QVARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_MULTIPLIER=\"{RWKV_TMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_MULTIPLIER=\"{RWKV_CMIX_REUSE_MULTIPLIER}\" && \\\n",
    "    export RWKV_CMIX_REUSE_VARS=\"{RWKV_CMIX_REUSE_VARS}\" && \\\n",
    "    export RWKV_TMIX_REUSE_VARS=\"{RWKV_TMIX_REUSE_VARS}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-TLR-{RWKV_TMIX_REUSE_MULTIPLIER}_{RWKV_TMIX_REUSE_VARS}-CLR-{RWKV_CMIX_REUSE_MULTIPLIER}_{RWKV_CMIX_REUSE_VARS}-Q{RWKV_TIMX_QTYPE}-TM_{RWKV_TMIX_QVARS}-CM_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Mix Quantized Training\n",
    "\n",
    "Starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits\n",
      "TRAINER_DIR: /home/recursal/RWKV-infctx-trainer/RWKV-v6-QT\n",
      "PROJECT_DIR: /home/recursal/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"CQT-\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v6-QT/\"))\n",
    "\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=6\n",
    "D_SIZE=512\n",
    "\n",
    "# Deepspeed and batch size\n",
    "DEEPSPEED_STAGE=\"deepspeed_stage_2\"\n",
    "BATCH_SIZE=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=160): 100%|███| 1000000/1000000 [00:37<00:00, 26659.45 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1000000/1000000 [00:06<00:00, 149382.73 examples/\n",
      "Map (num_proc=160): 100%|█████| 472276/472276 [00:24<00:00, 19407.40 examples/s]\n",
      "Map (num_proc=160): 100%|██████| 124119/124119 [00:20<00:00, 5977.08 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|█| 124119/124119 [00:03<00:00, 39297.29 ex\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 4449.34 examples/\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_100k-world-4k-rechunk.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-22 05:29:04,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-v6base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Output model exists, skipping init_model\n",
      "-rw-rw-r-- 1 recursal recursal 170M Apr 22 02:46 /home/recursal/RWKV-infctx-trainer/RWKV-v6/../model/L6-D512-world-v6base-init.pth\n"
     ]
    }
   ],
   "source": [
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=6\n",
    "D_SIZE=512\n",
    "\n",
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer {L_SIZE} --n_embd {D_SIZE} \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\"\n",
    "!ls -lh \"{TRAINER_DIR}/../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"R\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"4bit\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"K\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"K\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"K\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"4bit\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run - V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"V\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"V\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"V\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"4bit\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run - RK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"RK\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"RK\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"RK\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"4bit\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run - KV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"KV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"KV\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel mix quantized vars\n",
    "RWKV_CMIX_QVARS=\"KV\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"4bit\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QVARS=\"{RWKV_CMIX_QVARS}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}_{RWKV_CMIX_QVARS} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

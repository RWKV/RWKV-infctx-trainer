{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Training\n",
    "\n",
    "Starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits\n",
      "TRAINER_DIR: /home/recursal/RWKV-infctx-trainer/RWKV-v6-QT\n",
      "PROJECT_DIR: /home/recursal/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"QT-\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v6-QT/\"))\n",
    "\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=6\n",
    "D_SIZE=512\n",
    "\n",
    "# Deepspeed and batch size\n",
    "DEEPSPEED_STAGE=\"deepspeed_stage_2\"\n",
    "BATCH_SIZE=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:37<00:00, 26659.45 examples/s]\n",
      "Filter (num_proc=160): 100%|â–ˆ| 1000000/1000000 [00:06<00:00, 149382.73 examples/\n",
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 472276/472276 [00:24<00:00, 19407.40 examples/s]\n",
      "Map (num_proc=160): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124119/124119 [00:20<00:00, 5977.08 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|â–ˆ| 124119/124119 [00:03<00:00, 39297.29 ex\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 100/100 [00:00<00:00, 4449.34 examples/\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_100k-world-4k-rechunk.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-22 05:29:04,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-v6base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Output model exists, skipping init_model\n",
      "-rw-rw-r-- 1 recursal recursal 170M Apr 22 02:46 /home/recursal/RWKV-infctx-trainer/RWKV-v6/../model/L6-D512-world-v6base-init.pth\n"
     ]
    }
   ],
   "source": [
    "# Baseline layer count, and embedding size\n",
    "L_SIZE=6\n",
    "D_SIZE=512\n",
    "\n",
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer {L_SIZE} --n_embd {D_SIZE} \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\"\n",
    "!ls -lh \"{TRAINER_DIR}/../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6-D512 , CMix Quantized run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-26 02:23:30,410] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L6-D512-world-v6base-init.pth', '--model.lr_init=4e-3', '--model.lr_final=4e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L6-D512-CM_fp4/', '--trainer.logger.init_args.name=QT-L6-D512-CM_fp4 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L6-D512-world-v6base-init.pth', '--model.lr_init=4e-3', '--model.lr_final=4e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L6-D512-CM_fp4/', '--trainer.logger.init_args.name=QT-L6-D512-CM_fp4 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-26 02:23:35,916] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:35,934] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:35,960] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:36,072] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:36,091] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:36,118] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:23:36,172] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-native' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : fp4\n",
      "====================================================================\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240426_022347-fsnt979g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mQT-L6-D512-CM_fp4 (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/fsnt979g\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-03 (0.004)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06551885604858398 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10184812545776367 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10283851623535156 seconds\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10291147232055664 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10250258445739746 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10238218307495117 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10276126861572266 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10288000106811523 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 9.3 M \n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "76.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "76.4 M    Total params\n",
      "305.603   Total estimated model params size (MB)\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "Epoch 0:  32%|â–‹ | 100/313 [05:05<10:51,  0.33it/s, v_num=979g, train/loss=5.750]/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [15:58<00:00,  0.33it/s, v_num=979g, train/loss=5.120]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆ| 313/313 [15:58<00:00,  0.33it/s, v_num=979g, train/loss=5.120]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.042 MB of 0.042 MB uploaded (0.003 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 â–â–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss â–ˆâ–‡â–†â–…â–…â–„â–„â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss â–ˆâ–‡â–†â–…â–…â–„â–„â–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss â–ˆâ–…â–ƒâ–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 42.81453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 41025.536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 2496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 5.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 5.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 32768.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 5.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mQT-L6-D512-CM_fp4 (Rechunk 4k, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/fsnt979g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NjU0ODg2Mw==/version_details/v2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240426_022347-fsnt979g/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"fp4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: no process found\n",
      "[2024-04-26 02:40:29,895] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L6-D512-world-v6base-init.pth', '--model.lr_init=4e-3', '--model.lr_final=4e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L6-D512-CM_nf4/', '--trainer.logger.init_args.name=QT-L6-D512-CM_nf4 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/rwkv-x-exp/v6-layerNbits/config/enwiki_100k-world-32k-rechunk.yaml', '--model.load_model=../model/L6-D512-world-v6base-init.pth', '--model.lr_init=4e-3', '--model.lr_final=4e-4', '--trainer.callbacks.init_args.dirpath=../checkpoint/v6-enwiki-100k-L6-D512-CM_nf4/', '--trainer.logger.init_args.name=QT-L6-D512-CM_nf4 (Rechunk 4k, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.microbatch_size=4', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-04-26 02:40:35,940] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:35,969] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:35,974] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:36,046] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:36,164] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:36,180] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-26 02:40:36,201] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "====================================================================\n",
      "[RWKV] TMIX reuse multiplier : 1\n",
      "[RWKV] CMIX reuse multiplier : 1\n",
      "[RWKV] TMIX Quantize type    : \n",
      "[RWKV] CMIX Quantize type    : nf4\n",
      "====================================================================\n",
      "[rank: 4] Seed set to 3941088705\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[rank: 1] Seed set to 3941088705\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Loading dataset from data_path:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      ">> Dataset load finished:  ../datapath/enwiki_100k-world-32k-rechunk/\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240426_024047-cy5jzkzo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mQT-L6-D512-CM_nf4 (Rechunk 4k, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-v6x-layerNbits/runs/cy5jzkzo\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "    cli_main()\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "    self.init_deepspeed()\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "    self._initialize_deepspeed_train(self.model)\n",
      "    LightningCLI(\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "    ) = self._init_optimizers()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "    optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "self._run_subcommand(self.subcommand)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "    cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)\n",
      "    ^^^^^^^^^^^^^\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "    setattr(self._actual_script_module, attr, value)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "    self._modules[attr] = value\n",
      "    ~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "    self._c.setattr(k, v)\n",
      "    call._call_and_handle_interrupt(\n",
      "RuntimeError: Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x630ca20)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x630ca20)'\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "    self.init_deepspeed()\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "    self._initialize_deepspeed_train(self.model)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "    ) = self._init_optimizers()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "    optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "    cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)\n",
      "    ^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "    setattr(self._actual_script_module, attr, value)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "Traceback (most recent call last):\n",
      "    self._modules[attr] = value\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "    ~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "    self._c.setattr(k, v)\n",
      "    cli_main()RuntimeError\n",
      ": Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x5ee3b70)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x5ee3b70)'\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "    cli_main() \n",
      "          ^^^^^^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "^^Traceback (most recent call last):\n",
      "^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "^^^^^^^^^^^^^^^^^^^^^    ^LightningCLI(^\n",
      "^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "^Traceback (most recent call last):\n",
      "^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "^^^^    ^cli_main()^\n",
      "^^^^^^^^      File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "^self._run_subcommand(self.subcommand)^\n",
      "^^^^^^^^^^^    ^cli_main()^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "^^^^^    ^LightningCLI(\n",
      "\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "Traceback (most recent call last):\n",
      "    fn(**fn_kwargs)  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "        \n",
      "return function(*args, **kwargs)LightningCLI(    \n",
      "\n",
      "self._run_subcommand(self.subcommand)  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "\n",
      "   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "           cli_main()   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "\n",
      "  ^^^^^^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "^^    ^    self._run_subcommand(self.subcommand)^\n",
      "call._call_and_handle_interrupt(^\n",
      "^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "^^^          File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "^LightningCLI(fn(**fn_kwargs)^\n",
      "\n",
      "^    ^return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "^\n",
      "^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "           ^^^^    ^fn(**fn_kwargs)^\n",
      "    ^self._run_subcommand(self.subcommand)^\n",
      "^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "^    ^call._call_and_handle_interrupt(^\n",
      "    ^self._run(model, ckpt_path=ckpt_path)^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "^^^^^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "^    ^return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)^\n",
      "^    ^call._call_and_handle_interrupt(^\n",
      "^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "^     ^ fn(**fn_kwargs)^ \n",
      "^ ^     ^ return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)^ \n",
      "^       File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "^self.strategy.setup(self) ^\n",
      " ^  ^^^   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "^^ ^^ ^^ ^^ ^^ ^^     ^^call._call_and_handle_interrupt(  ^^\n",
      "     ^^self.init_deepspeed()^\n",
      "^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "^^^^^^    ^^^return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)^^^\n",
      "^^^^^^    ^^^self._initialize_deepspeed_train(self.model)^^ ^\n",
      "^^ ^^^ ^^^ ^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "^ ^^^ ^ ^^^ ^^^ ^^^ ^^^ \n",
      "^^^    ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "^^^^^^) = self._init_optimizers()^^^\n",
      "^^^^^^^^    ^^^ return function(*args, **kwargs)^^^ \n",
      "^^^ ^^^ ^ ^^ ^ ^^ ^ ^^ ^ ^^ ^ ^^^^ ^^^^ ^^^^ ^^^^ ^^^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    ^optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)^^^\n",
      "^\n",
      "^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "^ ^^ ^^ ^^ ^^ ^^^ ^ ^        \n",
      " ^self._run(model, ckpt_path=ckpt_path)return function(*args, **kwargs)\n",
      " ^\n",
      "   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "^ ^  ^    File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "^  ^  \n",
      "        return function(*args, **kwargs)   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      " \n",
      "        ^ ^  ^  ^      ^      return function(*args, **kwargs)^self.strategy.setup(self)  \n",
      "^\n",
      "  ^    ^   ^   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "  ^^ ^^^ ^^^ ^^^ ^^^ ^    ^self.init_deepspeed()^ ^^^\n",
      " ^^^ ^^^ ^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "^^    ^^^^self._initialize_deepspeed_train(self.model)^^^\n",
      "^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "^^^^^^^^^^^^    ^^\n",
      "self._run(model, ckpt_path=ckpt_path)^^\n",
      "^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "^^^    ^^) = self._init_optimizers()^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "^\n",
      "^^ ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      " ^ ^ ^ ^ ^ ^     ^self._run(model, ckpt_path=ckpt_path)^^^\n",
      "^^^^^^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "^        ^^self._run(model, ckpt_path=ckpt_path)self.strategy.setup(self)^^\n",
      "\n",
      "^^^\n",
      "^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "    self.strategy.setup(self)\n",
      "    self.init_deepspeed() \n",
      "   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "     optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)   File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "\n",
      "      self.strategy.setup(self) \n",
      "               File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "  self.init_deepspeed()  \n",
      "         self._initialize_deepspeed_train(self.model)  \n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "^ ^ ^     ^self.init_deepspeed()  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      " ^ ^\n",
      "      ^ ^ ^      File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      " ^ ^self._initialize_deepspeed_train(self.model) \n",
      " ^ ^ ^ ^      File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      " ^) = self._init_optimizers() ^\n",
      "^^    ^^self._initialize_deepspeed_train(self.model)^^\n",
      "^^^^ ^ ^^ ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "^ ^^ ^^     ^^ ) = self._init_optimizers()^^ ^\n",
      "^^^^^^^    ^^^) = self._init_optimizers()^^^^ \n",
      "^^^ ^^^ ^^^  ^^^^  ^^^  ^^^  ^^^  ^^^^ ^^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^\n",
      "^optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)^^^\n",
      "^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "\n",
      "^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      " ^^ ^^ \n",
      "^ ^ ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      " ^ ^ ^ ^ ^ ^ ^     ^optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)         ^\n",
      "optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module) optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)^\n",
      "^ \n",
      "^  ^  \n",
      "       File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "                          output = fn(*args, **kwargs)    \n",
      "                             ^    ^    ^    ^     ^^  ^ ^^  Traceback (most recent call last):\n",
      " ^^   ^^   ^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 296, in <module>\n",
      "^  ^ ^^ ^ ^^ ^  ^^^  ^^^  ^^^  ^^^  ^^^ ^^^^^^^^^^^^^^    ^^^^^cli_main()^\n",
      "^^^^^^^^^^^^^^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/lightning_trainer.py\", line 271, in cli_main\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^    ^LightningCLI(  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "^^^^\n",
      "^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "^^^^^^^^^^^^^^^^^^^^^^^    ^^^^cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)^\n",
      "^^^^^^^    ^^^ ^ self._run_subcommand(self.subcommand)^^^^ \n",
      "^^^^ ^^^^^^^^^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^^^fn(**fn_kwargs)^\n",
      "^^^^^^^\n",
      "^^^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "^^\n",
      "^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^    ^setattr(self._actual_script_module, attr, value)^        ^^\n",
      "^call._call_and_handle_interrupt(optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)^^\n",
      "\n",
      "\n",
      "^^\n",
      "^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      " ^ ^ ^ ^     ^ return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)^     \n",
      "^optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model) ^\n",
      " ^ ^      ^optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)      ^\n",
      "self._modules[attr] = value    \n",
      "\n",
      "          File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    ^    ^      ^    ^^ ^     ~^ ^output = fn(*args, **kwargs) ~^ ^\n",
      " ~^ ^ ~^ ^ ~^  ^ ~^  ^ ~^  ^ ~^  ^ ~ ~^  ^ ~^~ ^^ ^~ ^^^^^ ^^^^^^ ^^^^^^^ ^^^^^ ^^^^^ ^^^^^^ ^^^^\n",
      "^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^^^self._c.setattr(k, v)^\n",
      "^^^^^^^^^^^RuntimeError^: ^Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x6004b60)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x6004b60)'^^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^^cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)^^^^\n",
      "^^^^^^^^^^^ ^^^^ ^ ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "^^^    ^^^setattr(self._actual_script_module, attr, value)^^^\n",
      "^^^^^^    ^^\n",
      "return function(*args, **kwargs)^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "^^^^ ^^ ^^ ^^^ ^^  ^^     output = fn(*args, **kwargs)^^ ^\n",
      "^ ^^     ^ ^^self._modules[attr] = value^^ ^\n",
      "^^^^ ^^^ ^^^  ^^^  ^^\n",
      "  ^^  ^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "~ ^^~ \n",
      "^~ ~^ ~  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "^ ~    ^ ^^~^output = fn(*args, **kwargs)^~^\n",
      "^~^^~    ^^ ~output = fn(*args, **kwargs)^^ ~\n",
      "^^ ~^^ ^^^^^^ ^ ^ ^ ^\n",
      " ^ ^  ^ ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      " ^ ^ ^ ^\n",
      "   ^  ^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "  \n",
      "^ ^ ^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "^^^^^^    ^^    self._run(model, ckpt_path=ckpt_path)^\n",
      "^self._c.setattr(k, v)^^\n",
      "^^^^^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "^RuntimeError    ^^: cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)^^Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x649d020)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x649d020)'\n",
      "^^^\n",
      "^^^^\n",
      "^^ ^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      " \n",
      "  ^^^^^^  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "    self.strategy.setup(self)\n",
      "    cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "       cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE) \n",
      "^^^^^^ ^ ^     ^setattr(self._actual_script_module, attr, value) ^\n",
      "^^    ^^self.init_deepspeed()  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "^^\n",
      "^\n",
      "^^^^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "^  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "    self._modules[attr] = value\n",
      "     self._initialize_deepspeed_train(self.model)     setattr(self._actual_script_module, attr, value)\n",
      " \n",
      "     ~setattr(self._actual_script_module, attr, value)~\n",
      "~  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 484, in _initialize_deepspeed_train\n",
      "~  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "~~~  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "~~~~~~^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "    ) = self._init_optimizers()\n",
      "    self._c.setattr(k, v) \n",
      "      self._modules[attr] = value RuntimeError\n",
      "     : self._modules[attr] = valueExpected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x52ff010)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x52ff010)' \n",
      "\n",
      "    ^ ^  ^ ~^ ~^ ~~^~~^~~^~~~^~~^~~^~~^~~^~~^~~^~~^~^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 458, in _init_optimizers\n",
      "    self._c.setattr(k, v)    \n",
      "self._c.setattr(k, v)\n",
      "RuntimeErrorRuntimeError:     : Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x5702b60)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x5702b60)'Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x4f5e010)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x4f5e010)'optimizers, lr_schedulers = _init_optimizers_and_lr_schedulers(self.lightning_module)\n",
      "\n",
      "\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 177, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/RWKV-infctx-trainer/RWKV-v6-QT/src/model.py\", line 430, in configure_optimizers\n",
      "    cmixblock.key = QuantizedLinearModule(cmixblock.key.to(device), RWKV_CMIX_QTYPE)\n",
      "    ^^^^^^^^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 547, in __setattr__\n",
      "    setattr(self._actual_script_module, attr, value)\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 818, in __setattr__\n",
      "    self._modules[attr] = value\n",
      "    ~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/jit/_script.py\", line 255, in __setitem__\n",
      "    self._c.setattr(k, v)\n",
      "RuntimeError: Expected a value of type '__torch__.torch.nn.modules.linear.___torch_mangle_0.Linear (of Python compilation unit at: 0x6aac010)' for field 'key', but found '__torch__.src.module.Quantizer.QuantizedLinearModule (of Python compilation unit at: 0x6aac010)'\n",
      "[rank: 1] Child process with PID 818349 terminated with code 1. Forcefully terminating all other processes to avoid zombies ðŸ§Ÿ\n"
     ]
    }
   ],
   "source": [
    "# Learning rate setting\n",
    "LR_INIT=\"4e-3\"\n",
    "LR_FINAL=\"4e-4\"\n",
    "\n",
    "# Channel and timemix quantized settings\n",
    "RWKV_CMIX_QTYPE=\"nf4\"\n",
    "RWKV_TIMX_QTYPE=\"\"\n",
    "\n",
    "# Nuke python3 (for back to back run cleanup)\n",
    "!killall -9 python3\n",
    "\n",
    "# Run with the LRX setting\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    export RWKV_JIT_ON=\"0\" && \\\n",
    "    export RWKV_TORCH_COMPILE=\"0\" && \\\n",
    "    export RWKV_CMIX_QTYPE=\"{RWKV_CMIX_QTYPE}\" && \\\n",
    "    export RWKV_TIMX_QTYPE=\"{RWKV_TIMX_QTYPE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-32k-rechunk.yaml\" \\\n",
    "        --model.load_model=\"../model/L{L_SIZE}-D{D_SIZE}-world-v6base-init.pth\" \\\n",
    "        --model.lr_init=\"{LR_INIT}\" \\\n",
    "        --model.lr_final=\"{LR_FINAL}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v6-enwiki-100k-L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX}L{L_SIZE}-D{D_SIZE}-CM_{RWKV_CMIX_QTYPE} (Rechunk 4k, {DEEPSPEED_STAGE})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STAGE}\" \\\n",
    "        --trainer.microbatch_size={BATCH_SIZE} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ec74a7",
   "metadata": {
    "papermill": {
     "duration": 0.005881,
     "end_time": "2024-01-23T07:52:11.177974",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.172093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RWKV World Memory Finetune (Memory Finetune)\n",
    "\n",
    "This takes an existing RWKV world model, and finetune them specifically for the memory repeat task of various sizes.\n",
    "This test is used as an approximation of testing the model token memory size in the \"worse case scenerio\"\n",
    "\n",
    "- Using randomized data, so prior learning does not help, nor is it possible to compress the data\n",
    "- Using a variety of token lengths, to avoid overfitting to a single length\n",
    "- Based on the pretrained model (rwkv world)\n",
    "- This process does \"destroy the model\" but it helps quantify the model limits\n",
    "\n",
    "In practise however, the model may show \"attention range\" longer then what is benchmarked, as natural text is highly compressible. Unlike the pure randomized data that was being tested here.\n",
    "\n",
    "This runner has been optimized to run on 8 x 80GB vram nodes, you should allocate atleast 1TB disk space.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aac483",
   "metadata": {
    "papermill": {
     "duration": 0.004873,
     "end_time": "2024-01-23T07:52:11.188157",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.183284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c538903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:52:11.196870Z",
     "iopub.status.busy": "2024-01-23T07:52:11.196676Z",
     "iopub.status.idle": "2024-01-23T07:52:11.207800Z",
     "shell.execute_reply": "2024-01-23T07:52:11.206964Z"
    },
    "papermill": {
     "duration": 0.017119,
     "end_time": "2024-01-23T07:52:11.210197",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.193078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test\n",
      "TRAINER_DIR: /workspace/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"[8xA100] RWKV-v5-7B-World\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# The model sizing\n",
    "MODEL_NAME=\"RWKV-v5-7B-world.pth\"\n",
    "MODEL_URL=\"https://huggingface.co/BlinkDL/temp/resolve/2d905a2a30c778086a048e4f65ca75d9f7f9849d/RWKV-5-World-7B-v2-OnlyForTest_72%25_trained-20231204-ctx4096.pth?download=true\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "MEMORY_SCRIPT_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./notebook/util-scripts/memory_script\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f8e5",
   "metadata": {
    "papermill": {
     "duration": 0.00356,
     "end_time": "2024-01-23T07:52:11.220001",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.216441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f8a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:52:11.226610Z",
     "iopub.status.busy": "2024-01-23T07:52:11.225973Z",
     "iopub.status.idle": "2024-01-23T07:52:11.717381Z",
     "shell.execute_reply": "2024-01-23T07:52:11.716269Z"
    },
    "papermill": {
     "duration": 0.497049,
     "end_time": "2024-01-23T07:52:11.719954",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.222905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets wget the model files\n",
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"{PROJECT_DIR}/model\"\n",
    "!cd \"{PROJECT_DIR}/model\" && \\\n",
    "    wget -O \"{MODEL_NAME}\" -nc \"{MODEL_URL}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3928b63f",
   "metadata": {
    "papermill": {
     "duration": 0.004645,
     "end_time": "2024-01-23T07:52:11.730080",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.725435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune 1 (0 -> 2*2k) : Dataset preperation\n",
    "\n",
    "Stage 1, handles total context size of 4096. Meaning it will be tuned for memory task of approximately 2k tokens of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100d015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:52:11.737515Z",
     "iopub.status.busy": "2024-01-23T07:52:11.736355Z",
     "iopub.status.idle": "2024-01-23T07:52:15.468489Z",
     "shell.execute_reply": "2024-01-23T07:52:15.467116Z"
    },
    "papermill": {
     "duration": 3.738786,
     "end_time": "2024-01-23T07:52:15.471307",
     "exception": false,
     "start_time": "2024-01-23T07:52:11.732521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Folder and eval pip setup\n",
    "!cp -r \"{MEMORY_SCRIPT_DIR}/\" \"{NOTEBOOK_DIR}/\"\n",
    "!python3 -m pip install rwkv asyncio aiocsv aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b46d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:52:15.483048Z",
     "iopub.status.busy": "2024-01-23T07:52:15.482635Z",
     "iopub.status.idle": "2024-01-23T07:52:18.671801Z",
     "shell.execute_reply": "2024-01-23T07:52:18.670751Z"
    },
    "papermill": {
     "duration": 3.348865,
     "end_time": "2024-01-23T07:52:18.826676",
     "exception": false,
     "start_time": "2024-01-23T07:52:15.477811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# Training set for < 100 words\n",
    "# This is used to fill up as much blanks as possible\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 100 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-4-count.jsonl 4 100 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 150 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {110..200..10} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 125 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {210..4000..10} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0376d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:52:19.064200Z",
     "iopub.status.busy": "2024-01-23T07:52:19.063812Z",
     "iopub.status.idle": "2024-01-23T07:55:09.987257Z",
     "shell.execute_reply": "2024-01-23T07:55:09.985659Z"
    },
    "papermill": {
     "duration": 171.009128,
     "end_time": "2024-01-23T07:55:09.990364",
     "exception": false,
     "start_time": "2024-01-23T07:52:18.981236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "# and pack the data into 8k of length\n",
    "#\n",
    "# For the initial training, it seems to be better to do 4k chunks, batch size 16, with 8k datapacks\n",
    "# Then to do 8k chunks, batchsize 8, with 16k datapacks. Why? I dun know.\n",
    "#\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/stage-1-tune.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/stage-1-memory-finetune/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a778ddb",
   "metadata": {
    "papermill": {
     "duration": 0.114062,
     "end_time": "2024-01-23T07:55:10.231871",
     "exception": false,
     "start_time": "2024-01-23T07:55:10.117809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetune 1 (0 -> 2*2k) : The actual tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4f921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T07:55:10.463735Z",
     "iopub.status.busy": "2024-01-23T07:55:10.463303Z",
     "iopub.status.idle": "2024-01-23T11:18:37.403552Z",
     "shell.execute_reply": "2024-01-23T11:18:37.402122Z"
    },
    "papermill": {
     "duration": 12207.060283,
     "end_time": "2024-01-23T11:18:37.406917",
     "exception": false,
     "start_time": "2024-01-23T07:55:10.346634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/stage-1-tune.yaml\" \\\n",
    "        --model.load_model=\"../model/{MODEL_NAME}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/stage-1-memory-finetune/{MODEL_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=8192, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.microbatch_size=4 \\\n",
    "        --model.ctx_len=8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68ae18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T11:18:37.833406Z",
     "iopub.status.busy": "2024-01-23T11:18:37.832966Z",
     "iopub.status.idle": "2024-01-23T11:19:35.452202Z",
     "shell.execute_reply": "2024-01-23T11:19:35.450809Z"
    },
    "papermill": {
     "duration": 57.804367,
     "end_time": "2024-01-23T11:19:35.454988",
     "exception": false,
     "start_time": "2024-01-23T11:18:37.650621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/stage-1-memory-finetune/{MODEL_NAME}/last.ckpt\" \\\n",
    "        \"../model/Memory-Tune-Stage-1-{MODEL_NAME}\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Memory-Tune-Stage-1-{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30726953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T11:22:23.370145Z",
     "iopub.status.busy": "2024-01-23T11:22:23.369718Z",
     "iopub.status.idle": "2024-01-23T11:31:09.313399Z",
     "shell.execute_reply": "2024-01-23T11:31:09.312027Z"
    },
    "papermill": {
     "duration": 526.138711,
     "end_time": "2024-01-23T11:31:09.316221",
     "exception": false,
     "start_time": "2024-01-23T11:22:23.177510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets do a memory eval!\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-1-{MODEL_NAME}\"\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-1-{MODEL_NAME}\" \"none\" 1000 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec3af2",
   "metadata": {},
   "source": [
    "## Finetune 2 (0 -> 2*4k) : Dataset preperation\n",
    "\n",
    "Stage 2, handles total context size of 8k. Meaning it will be tuned for memory task of approximately 4k tokens of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# Training set for <= 100 words\n",
    "# This is used to fill up as much blanks as possible\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 100 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-4-count.jsonl 4 100 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 100+ - 3000 words dataset\n",
    "# \n",
    "for i in {110..3000..10} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 75 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 75 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 3000+ - 400 words dataset\n",
    "# \n",
    "for i in {3000..6000..25} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "# and pack the data into 8k of length\n",
    "#\n",
    "# For the initial training, it seems to be better to do 4k chunks, batch size 16, with 8k datapacks\n",
    "# Then to do 8k chunks, batchsize 8, with 16k datapacks. Why? I dun know.\n",
    "#\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/stage-2-tune.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/stage-2-memory-finetune/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1343c6",
   "metadata": {},
   "source": [
    "## Finetune 1 (0 -> 2*2k) : The actual tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/stage-2-tune.yaml\" \\\n",
    "        --model.load_model=\"../model/Memory-Tune-Stage-1-{MODEL_NAME}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/stage-2-memory-finetune/{MODEL_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=8192, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.microbatch_size=4 \\\n",
    "        --model.ctx_len=8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4041ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/stage-2-memory-finetune/{MODEL_NAME}/last.ckpt\" \\\n",
    "        \"../model/Memory-Tune-Stage-2-{MODEL_NAME}\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Memory-Tune-Stage-2-{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a793df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a memory eval!\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\"\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\" \"none\" 1000 4000\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\" \"none\" 4000 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1c2df",
   "metadata": {},
   "source": [
    "## Finetune 2 (2x2k -> 2x4k) : Dataset preperation\n",
    "\n",
    "Stage 2, handles total context size of 8k. Meaning it will be tuned for memory task of approximately 4k tokens of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae4ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 15 max words, 100 samples - at ./dataset/gen-word-15-count.jsonl\n",
      "Generated JSONL file with - 4 max words, 100 samples - at ./dataset/word-4-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 100 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 100 samples - at ./dataset/gen-word-5-count.jsonlGenerated JSONL file with - 10 max words, 100 samples - at ./dataset/gen-word-10-count.jsonl\n",
      "\n",
      "Generated JSONL file with - 30 max words, 100 samples - at ./dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 100 samples - at ./dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 100 samples - at ./dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 100 samples - at ./dataset/gen-word-20-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 100 samples - at ./dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 100 samples - at ./dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 100 samples - at ./dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 100 samples - at ./dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 100 samples - at ./dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 100 samples - at ./dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 100 samples - at ./dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 100 samples - at ./dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 100 samples - at ./dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 75 samples - at ./dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 100 samples - at ./dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 100 samples - at ./dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 75 samples - at ./dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 75 samples - at ./dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 75 samples - at ./dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 100 samples - at ./dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 75 samples - at ./dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 75 samples - at ./dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 75 samples - at ./dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 75 samples - at ./dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 75 samples - at ./dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 100 samples - at ./dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 75 samples - at ./dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 75 samples - at ./dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 75 samples - at ./dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 75 samples - at ./dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 75 samples - at ./dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 75 samples - at ./dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 75 samples - at ./dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 75 samples - at ./dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 75 samples - at ./dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 75 samples - at ./dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 75 samples - at ./dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 75 samples - at ./dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 75 samples - at ./dataset/gen-word-160-count.jsonl\n",
      "Generated a single JSONL file with 1063 samples (75 token repeat) - 170 max words - at ./dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 1288 samples (75 token repeat) - 130 max words - at ./dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 1481 samples (75 token repeat) - 110 max words - at ./dataset/shuffle-word-110-count.jsonl\n",
      "Generated JSONL file with - 470 max words, 75 samples - at ./dataset/gen-word-470-count.jsonl\n",
      "Generated a single JSONL file with 1381 samples (75 token repeat) - 120 max words - at ./dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 1158 samples (75 token repeat) - 150 max words - at ./dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 730 samples (75 token repeat) - 240 max words - at ./dataset/shuffle-word-240-count.jsonl\n",
      "Generated JSONL file with - 490 max words, 75 samples - at ./dataset/gen-word-490-count.jsonl\n",
      "Generated a single JSONL file with 797 samples (75 token repeat) - 210 max words - at ./dataset/shuffle-word-210-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 75 samples - at ./dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 75 samples - at ./dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 75 samples - at ./dataset/gen-word-390-count.jsonl\n",
      "Generated a single JSONL file with 1215 samples (75 token repeat) - 140 max words - at ./dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 685 samples (75 token repeat) - 290 max words - at ./dataset/shuffle-word-290-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 75 samples - at ./dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 530 max words, 75 samples - at ./dataset/gen-word-530-count.jsonl\n",
      "Generated a single JSONL file with 707 samples (75 token repeat) - 250 max words - at ./dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 750 samples (75 token repeat) - 230 max words - at ./dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 526 samples (75 token repeat) - 360 max words - at ./dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 528 samples (75 token repeat) - 340 max words - at ./dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 688 samples (75 token repeat) - 280 max words - at ./dataset/shuffle-word-280-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 75 samples - at ./dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 510 max words, 75 samples - at ./dataset/gen-word-510-count.jsonl\n",
      "Generated JSONL file with - 550 max words, 75 samples - at ./dataset/gen-word-550-count.jsonl\n",
      "Generated a single JSONL file with 3293 samples (100 token repeat) - 80 max words - at ./dataset/shuffle-word-80-count.jsonl\n",
      "Generated JSONL file with - 580 max words, 75 samples - at ./dataset/gen-word-580-count.jsonl\n",
      "Generated a single JSONL file with 524 samples (75 token repeat) - 370 max words - at ./dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 697 samples (75 token repeat) - 270 max words - at ./dataset/shuffle-word-270-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 75 samples - at ./dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 710 max words, 75 samples - at ./dataset/gen-word-710-count.jsonl\n",
      "Generated JSONL file with - 810 max words, 75 samples - at ./dataset/gen-word-810-count.jsonl\n",
      "Generated a single JSONL file with 2937 samples (100 token repeat) - 90 max words - at ./dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 3532 samples (100 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 2671 samples (100 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 3130 samples (100 token repeat) - 85 max words - at ./dataset/shuffle-word-85-count.jsonl\n",
      "Generated JSONL file with - 700 max words, 75 samples - at ./dataset/gen-word-700-count.jsonl\n",
      "Generated a single JSONL file with 375 samples (75 token repeat) - 510 max words - at ./dataset/shuffle-word-510-count.jsonl\n",
      "Generated JSONL file with - 730 max words, 75 samples - at ./dataset/gen-word-730-count.jsonl\n",
      "Generated JSONL file with - 1030 max words, 75 samples - at ./dataset/gen-word-1030-count.jsonl\n",
      "Generated JSONL file with - 570 max words, 75 samples - at ./dataset/gen-word-570-count.jsonl\n",
      "Generated JSONL file with - 630 max words, 75 samples - at ./dataset/gen-word-630-count.jsonl\n",
      "Generated JSONL file with - 870 max words, 75 samples - at ./dataset/gen-word-870-count.jsonl\n",
      "Generated a single JSONL file with 375 samples (75 token repeat) - 520 max words - at ./dataset/shuffle-word-520-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 75 samples - at ./dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 75 samples - at ./dataset/gen-word-420-count.jsonl\n",
      "Generated a single JSONL file with 2794 samples (100 token repeat) - 95 max words - at ./dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 1037 samples (75 token repeat) - 180 max words - at ./dataset/shuffle-word-180-count.jsonl\n",
      "Generated JSONL file with - 780 max words, 75 samples - at ./dataset/gen-word-780-count.jsonl\n",
      "Generated a single JSONL file with 437 samples (75 token repeat) - 480 max words - at ./dataset/shuffle-word-480-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 75 samples - at ./dataset/gen-word-440-count.jsonl\n",
      "Generated a single JSONL file with 521 samples (75 token repeat) - 380 max words - at ./dataset/shuffle-word-380-count.jsonl\n",
      "Generated JSONL file with - 590 max words, 75 samples - at ./dataset/gen-word-590-count.jsonl\n",
      "Generated JSONL file with - 760 max words, 75 samples - at ./dataset/gen-word-760-count.jsonl\n",
      "Generated JSONL file with - 640 max words, 75 samples - at ./dataset/gen-word-640-count.jsonl\n",
      "Generated a single JSONL file with 3770 samples (100 token repeat) - 70 max words - at ./dataset/shuffle-word-70-count.jsonl\n",
      "Generated JSONL file with - 480 max words, 75 samples - at ./dataset/gen-word-480-count.jsonl\n",
      "Generated JSONL file with - 740 max words, 75 samples - at ./dataset/gen-word-740-count.jsonl\n",
      "Generated JSONL file with - 770 max words, 75 samples - at ./dataset/gen-word-770-count.jsonl\n",
      "Generated JSONL file with - 880 max words, 75 samples - at ./dataset/gen-word-880-count.jsonl\n",
      "Generated a single JSONL file with 4805 samples (100 token repeat) - 55 max words - at ./dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 4074 samples (100 token repeat) - 65 max words - at ./dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 529 samples (75 token repeat) - 330 max words - at ./dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 437 samples (75 token repeat) - 490 max words - at ./dataset/shuffle-word-490-count.jsonl\n",
      "Generated a single JSONL file with 5231 samples (100 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 75 samples - at ./dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 610 max words, 75 samples - at ./dataset/gen-word-610-count.jsonl\n",
      "Generated JSONL file with - 750 max words, 75 samples - at ./dataset/gen-word-750-count.jsonl\n",
      "Generated JSONL file with - 860 max words, 75 samples - at ./dataset/gen-word-860-count.jsonl\n",
      "Generated JSONL file with - 500 max words, 75 samples - at ./dataset/gen-word-500-count.jsonl\n",
      "Generated a single JSONL file with 682 samples (75 token repeat) - 300 max words - at ./dataset/shuffle-word-300-count.jsonl\n",
      "Generated JSONL file with - 680 max words, 75 samples - at ./dataset/gen-word-680-count.jsonl\n",
      "Generated a single JSONL file with 7562 samples (100 token repeat) - 35 max words - at ./dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 540 max words, 75 samples - at ./dataset/gen-word-540-count.jsonl\n",
      "Generated a single JSONL file with 754 samples (75 token repeat) - 220 max words - at ./dataset/shuffle-word-220-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 75 samples - at ./dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 460 max words, 75 samples - at ./dataset/gen-word-460-count.jsonl\n",
      "Generated JSONL file with - 520 max words, 75 samples - at ./dataset/gen-word-520-count.jsonl\n",
      "Generated a single JSONL file with 434 samples (75 token repeat) - 460 max words - at ./dataset/shuffle-word-460-count.jsonl\n",
      "Generated a single JSONL file with 10604 samples (100 token repeat) - 25 max words - at ./dataset/shuffle-word-25-count.jsonl\n",
      "Generated JSONL file with - 560 max words, 75 samples - at ./dataset/gen-word-560-count.jsonl\n",
      "Generated a single JSONL file with 527 samples (75 token repeat) - 400 max words - at ./dataset/shuffle-word-400-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 75 samples - at ./dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 690 max words, 75 samples - at ./dataset/gen-word-690-count.jsonl\n",
      "Generated a single JSONL file with 8750 samples (100 token repeat) - 30 max words - at ./dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 830 max words, 75 samples - at ./dataset/gen-word-830-count.jsonl\n",
      "Generated a single JSONL file with 6559 samples (100 token repeat) - 40 max words - at ./dataset/shuffle-word-40-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 75 samples - at ./dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 910 max words, 75 samples - at ./dataset/gen-word-910-count.jsonl\n",
      "Generated JSONL file with - 650 max words, 75 samples - at ./dataset/gen-word-650-count.jsonl\n",
      "Generated JSONL file with - 790 max words, 75 samples - at ./dataset/gen-word-790-count.jsonl\n",
      "Generated a single JSONL file with 375 samples (75 token repeat) - 600 max words - at ./dataset/shuffle-word-600-count.jsonl\n",
      "Generated JSONL file with - 1120 max words, 75 samples - at ./dataset/gen-word-1120-count.jsonl\n",
      "Generated JSONL file with - 720 max words, 75 samples - at ./dataset/gen-word-720-count.jsonl\n",
      "Generated JSONL file with - 820 max words, 75 samples - at ./dataset/gen-word-820-count.jsonl\n",
      "Generated a single JSONL file with 540 samples (75 token repeat) - 310 max words - at ./dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 750 max words - at ./dataset/shuffle-word-750-count.jsonl\n",
      "Generated a single JSONL file with 4388 samples (100 token repeat) - 60 max words - at ./dataset/shuffle-word-60-count.jsonl\n",
      "Generated JSONL file with - 930 max words, 75 samples - at ./dataset/gen-word-930-count.jsonl\n",
      "Generated a single JSONL file with 1023 samples (75 token repeat) - 190 max words - at ./dataset/shuffle-word-190-count.jsonl\n",
      "Generated JSONL file with - 600 max words, 75 samples - at ./dataset/gen-word-600-count.jsonl\n",
      "Generated JSONL file with - 620 max words, 75 samples - at ./dataset/gen-word-620-count.jsonl\n",
      "Generated a single JSONL file with 440 samples (75 token repeat) - 450 max words - at ./dataset/shuffle-word-450-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 680 max words - at ./dataset/shuffle-word-680-count.jsonl\n",
      "Generated JSONL file with - 920 max words, 75 samples - at ./dataset/gen-word-920-count.jsonl\n",
      "Generated JSONL file with - 670 max words, 75 samples - at ./dataset/gen-word-670-count.jsonl\n",
      "Generated JSONL file with - 970 max words, 75 samples - at ./dataset/gen-word-970-count.jsonl\n",
      "Generated JSONL file with - 990 max words, 75 samples - at ./dataset/gen-word-990-count.jsonl\n",
      "Generated a single JSONL file with 528 samples (75 token repeat) - 350 max words - at ./dataset/shuffle-word-350-count.jsonl\n",
      "Generated JSONL file with - 660 max words, 75 samples - at ./dataset/gen-word-660-count.jsonl\n",
      "Generated JSONL file with - 800 max words, 75 samples - at ./dataset/gen-word-800-count.jsonl\n",
      "Generated JSONL file with - 1070 max words, 75 samples - at ./dataset/gen-word-1070-count.jsonl\n",
      "Generated JSONL file with - 980 max words, 75 samples - at ./dataset/gen-word-980-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 980 max words - at ./dataset/shuffle-word-980-count.jsonl\n",
      "Generated JSONL file with - 960 max words, 75 samples - at ./dataset/gen-word-960-count.jsonl\n",
      "Generated a single JSONL file with 5882 samples (100 token repeat) - 45 max words - at ./dataset/shuffle-word-45-count.jsonl\n",
      "Generated a single JSONL file with 1013 samples (75 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated JSONL file with - 1000 max words, 75 samples - at ./dataset/gen-word-1000-count.jsonl\n",
      "Generated a single JSONL file with 303 samples (75 token repeat) - 630 max words - at ./dataset/shuffle-word-630-count.jsonl\n",
      "Generated JSONL file with - 900 max words, 75 samples - at ./dataset/gen-word-900-count.jsonl\n",
      "Generated a single JSONL file with 526 samples (75 token repeat) - 390 max words - at ./dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 13011 samples (100 token repeat) - 20 max words - at ./dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 374 samples (75 token repeat) - 530 max words - at ./dataset/shuffle-word-530-count.jsonl\n",
      "Generated JSONL file with - 890 max words, 75 samples - at ./dataset/gen-word-890-count.jsonl\n",
      "Generated a single JSONL file with 370 samples (75 token repeat) - 560 max words - at ./dataset/shuffle-word-560-count.jsonl\n",
      "Generated JSONL file with - 1010 max words, 75 samples - at ./dataset/gen-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 235 samples (75 token repeat) - 870 max words - at ./dataset/shuffle-word-870-count.jsonl\n",
      "Generated a single JSONL file with 221 samples (75 token repeat) - 1140 max words - at ./dataset/shuffle-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1100 max words, 75 samples - at ./dataset/gen-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 373 samples (75 token repeat) - 570 max words - at ./dataset/shuffle-word-570-count.jsonl\n",
      "Generated a single JSONL file with 235 samples (75 token repeat) - 900 max words - at ./dataset/shuffle-word-900-count.jsonl\n",
      "Generated JSONL file with - 1150 max words, 75 samples - at ./dataset/gen-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 960 max words - at ./dataset/shuffle-word-960-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 990 max words - at ./dataset/shuffle-word-990-count.jsonl\n",
      "Generated a single JSONL file with 301 samples (75 token repeat) - 650 max words - at ./dataset/shuffle-word-650-count.jsonl\n",
      "Generated a single JSONL file with 301 samples (75 token repeat) - 700 max words - at ./dataset/shuffle-word-700-count.jsonl\n",
      "Generated a single JSONL file with 449 samples (75 token repeat) - 410 max words - at ./dataset/shuffle-word-410-count.jsonl\n",
      "Generated JSONL file with - 840 max words, 75 samples - at ./dataset/gen-word-840-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 970 max words - at ./dataset/shuffle-word-970-count.jsonl\n",
      "Generated a single JSONL file with 693 samples (75 token repeat) - 260 max words - at ./dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 298 samples (75 token repeat) - 770 max words - at ./dataset/shuffle-word-770-count.jsonlGenerated a single JSONL file with 443 samples (75 token repeat) - 440 max words - at ./dataset/shuffle-word-440-count.jsonl\n",
      "\n",
      "Generated a single JSONL file with 373 samples (75 token repeat) - 540 max words - at ./dataset/shuffle-word-540-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 800 max words - at ./dataset/shuffle-word-800-count.jsonl\n",
      "Generated a single JSONL file with 1097 samples (75 token repeat) - 160 max words - at ./dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 529 samples (75 token repeat) - 320 max words - at ./dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 299 samples (75 token repeat) - 730 max words - at ./dataset/shuffle-word-730-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 670 max words - at ./dataset/shuffle-word-670-count.jsonl\n",
      "Generated a single JSONL file with 373 samples (75 token repeat) - 590 max words - at ./dataset/shuffle-word-590-count.jsonl\n",
      "Generated a single JSONL file with 435 samples (75 token repeat) - 470 max words - at ./dataset/shuffle-word-470-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 940 max words - at ./dataset/shuffle-word-940-count.jsonl\n",
      "Generated a single JSONL file with 299 samples (75 token repeat) - 720 max words - at ./dataset/shuffle-word-720-count.jsonl\n",
      "Generated a single JSONL file with 303 samples (75 token repeat) - 610 max words - at ./dataset/shuffle-word-610-count.jsonl\n",
      "Generated JSONL file with - 940 max words, 75 samples - at ./dataset/gen-word-940-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 950 max words - at ./dataset/shuffle-word-950-count.jsonl\n",
      "Generated a single JSONL file with 302 samples (75 token repeat) - 640 max words - at ./dataset/shuffle-word-640-count.jsonl\n",
      "Generated a single JSONL file with 299 samples (75 token repeat) - 740 max words - at ./dataset/shuffle-word-740-count.jsonl\n",
      "Generated a single JSONL file with 442 samples (75 token repeat) - 430 max words - at ./dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1050 max words - at ./dataset/shuffle-word-1050-count.jsonl\n",
      "Generated JSONL file with - 1050 max words, 75 samples - at ./dataset/gen-word-1050-count.jsonl\n",
      "Generated a single JSONL file with 374 samples (75 token repeat) - 580 max words - at ./dataset/shuffle-word-580-count.jsonl\n",
      "Generated a single JSONL file with 17788 samples (100 token repeat) - 15 max words - at ./dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 298 samples (75 token repeat) - 760 max words - at ./dataset/shuffle-word-760-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 910 max words - at ./dataset/shuffle-word-910-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 710 max words - at ./dataset/shuffle-word-710-count.jsonl\n",
      "Generated a single JSONL file with 234 samples (75 token repeat) - 820 max words - at ./dataset/shuffle-word-820-count.jsonl\n",
      "Generated a single JSONL file with 296 samples (75 token repeat) - 780 max words - at ./dataset/shuffle-word-780-count.jsonl\n",
      "Generated a single JSONL file with 242 samples (75 token repeat) - 880 max words - at ./dataset/shuffle-word-880-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1000 max words - at ./dataset/shuffle-word-1000-count.jsonl\n",
      "Generated JSONL file with - 1090 max words, 75 samples - at ./dataset/gen-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 239 samples (75 token repeat) - 860 max words - at ./dataset/shuffle-word-860-count.jsonl\n",
      "Generated JSONL file with - 1110 max words, 75 samples - at ./dataset/gen-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 238 samples (75 token repeat) - 890 max words - at ./dataset/shuffle-word-890-count.jsonl\n",
      "Generated JSONL file with - 850 max words, 75 samples - at ./dataset/gen-word-850-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1030 max words - at ./dataset/shuffle-word-1030-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1060 max words - at ./dataset/shuffle-word-1060-count.jsonl\n",
      "Generated JSONL file with - 950 max words, 75 samples - at ./dataset/gen-word-950-count.jsonl\n",
      "Generated a single JSONL file with 234 samples (75 token repeat) - 840 max words - at ./dataset/shuffle-word-840-count.jsonl\n",
      "Generated a single JSONL file with 224 samples (75 token repeat) - 1120 max words - at ./dataset/shuffle-word-1120-count.jsonl\n",
      "Generated JSONL file with - 1080 max words, 75 samples - at ./dataset/gen-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 297 samples (75 token repeat) - 790 max words - at ./dataset/shuffle-word-790-count.jsonl\n",
      "Generated JSONL file with - 1140 max words, 75 samples - at ./dataset/gen-word-1140-count.jsonl\n",
      "Generated JSONL file with - 1210 max words, 75 samples - at ./dataset/gen-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 660 max words - at ./dataset/shuffle-word-660-count.jsonl\n",
      "Generated a single JSONL file with 239 samples (75 token repeat) - 850 max words - at ./dataset/shuffle-word-850-count.jsonl\n",
      "Generated a single JSONL file with 303 samples (75 token repeat) - 620 max words - at ./dataset/shuffle-word-620-count.jsonl\n",
      "Generated a single JSONL file with 222 samples (75 token repeat) - 1110 max words - at ./dataset/shuffle-word-1110-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1070 max words - at ./dataset/shuffle-word-1070-count.jsonl\n",
      "Generated JSONL file with - 1160 max words, 75 samples - at ./dataset/gen-word-1160-count.jsonl\n",
      "Generated a single JSONL file with 437 samples (75 token repeat) - 500 max words - at ./dataset/shuffle-word-500-count.jsonl\n",
      "Generated JSONL file with - 1190 max words, 75 samples - at ./dataset/gen-word-1190-count.jsonl\n",
      "Generated JSONL file with - 1060 max words, 75 samples - at ./dataset/gen-word-1060-count.jsonl\n",
      "Generated JSONL file with - 1290 max words, 75 samples - at ./dataset/gen-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 920 max words - at ./dataset/shuffle-word-920-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 930 max words - at ./dataset/shuffle-word-930-count.jsonl\n",
      "Generated JSONL file with - 1300 max words, 75 samples - at ./dataset/gen-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 373 samples (75 token repeat) - 550 max words - at ./dataset/shuffle-word-550-count.jsonl\n",
      "Generated JSONL file with - 1280 max words, 75 samples - at ./dataset/gen-word-1280-count.jsonl\n",
      "Generated JSONL file with - 1040 max words, 75 samples - at ./dataset/gen-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 446 samples (75 token repeat) - 420 max words - at ./dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 237 samples (75 token repeat) - 830 max words - at ./dataset/shuffle-word-830-count.jsonl\n",
      "Generated JSONL file with - 1200 max words, 75 samples - at ./dataset/gen-word-1200-count.jsonl\n",
      "Generated JSONL file with - 1820 max words, 75 samples - at ./dataset/gen-word-1820-count.jsonl\n",
      "Generated JSONL file with - 1170 max words, 75 samples - at ./dataset/gen-word-1170-count.jsonl\n",
      "Generated JSONL file with - 1260 max words, 75 samples - at ./dataset/gen-word-1260-count.jsonl\n",
      "Generated a single JSONL file with 151 samples (75 token repeat) - 1310 max words - at ./dataset/shuffle-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 224 samples (75 token repeat) - 1150 max words - at ./dataset/shuffle-word-1150-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1750 max words - at ./dataset/shuffle-word-1750-count.jsonl\n",
      "Generated JSONL file with - 1330 max words, 75 samples - at ./dataset/gen-word-1330-count.jsonl\n",
      "Generated JSONL file with - 1240 max words, 75 samples - at ./dataset/gen-word-1240-count.jsonl\n",
      "Generated JSONL file with - 2900 max words, 75 samples - at ./dataset/gen-word-2900-count.jsonl\n",
      "Generated JSONL file with - 1020 max words, 75 samples - at ./dataset/gen-word-1020-count.jsonl\n",
      "Generated JSONL file with - 1180 max words, 75 samples - at ./dataset/gen-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1170 max words - at ./dataset/shuffle-word-1170-count.jsonl\n",
      "Generated JSONL file with - 1130 max words, 75 samples - at ./dataset/gen-word-1130-count.jsonl\n",
      "Generated JSONL file with - 1320 max words, 75 samples - at ./dataset/gen-word-1320-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1460 max words - at ./dataset/shuffle-word-1460-count.jsonl\n",
      "Generated JSONL file with - 1230 max words, 75 samples - at ./dataset/gen-word-1230-count.jsonl\n",
      "Generated a single JSONL file with 235 samples (75 token repeat) - 810 max words - at ./dataset/shuffle-word-810-count.jsonl\n",
      "Generated a single JSONL file with 224 samples (75 token repeat) - 1160 max words - at ./dataset/shuffle-word-1160-count.jsonl\n",
      "Generated JSONL file with - 1760 max words, 75 samples - at ./dataset/gen-word-1760-count.jsonl\n",
      "Generated a single JSONL file with 152 samples (75 token repeat) - 1360 max words - at ./dataset/shuffle-word-1360-count.jsonl\n",
      "Generated JSONL file with - 1220 max words, 75 samples - at ./dataset/gen-word-1220-count.jsonl\n",
      "Generated JSONL file with - 1270 max words, 75 samples - at ./dataset/gen-word-1270-count.jsonl\n",
      "Generated a single JSONL file with 300 samples (75 token repeat) - 690 max words - at ./dataset/shuffle-word-690-count.jsonl\n",
      "Generated JSONL file with - 1250 max words, 75 samples - at ./dataset/gen-word-1250-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1040 max words - at ./dataset/shuffle-word-1040-count.jsonl\n",
      "Generated a single JSONL file with 26100 samples (100 token repeat) - 10 max words - at ./dataset/shuffle-word-10-count.jsonl\n",
      "Generated a single JSONL file with 223 samples (75 token repeat) - 1130 max words - at ./dataset/shuffle-word-1130-count.jsonl\n",
      "Generated JSONL file with - 1360 max words, 75 samples - at ./dataset/gen-word-1360-count.jsonl\n",
      "Generated JSONL file with - 1370 max words, 75 samples - at ./dataset/gen-word-1370-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1010 max words - at ./dataset/shuffle-word-1010-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (75 token repeat) - 1240 max words - at ./dataset/shuffle-word-1240-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1720 max words - at ./dataset/shuffle-word-1720-count.jsonl\n",
      "Generated JSONL file with - 1390 max words, 75 samples - at ./dataset/gen-word-1390-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1020 max words - at ./dataset/shuffle-word-1020-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1180 max words - at ./dataset/shuffle-word-1180-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1500 max words - at ./dataset/shuffle-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1380 max words, 75 samples - at ./dataset/gen-word-1380-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1090 max words - at ./dataset/shuffle-word-1090-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (75 token repeat) - 1210 max words - at ./dataset/shuffle-word-1210-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1080 max words - at ./dataset/shuffle-word-1080-count.jsonl\n",
      "Generated a single JSONL file with 194 samples (75 token repeat) - 1290 max words - at ./dataset/shuffle-word-1290-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (75 token repeat) - 1270 max words - at ./dataset/shuffle-word-1270-count.jsonl\n",
      "Generated a single JSONL file with 151 samples (75 token repeat) - 1330 max words - at ./dataset/shuffle-word-1330-count.jsonl\n",
      "Generated a single JSONL file with 225 samples (75 token repeat) - 1100 max words - at ./dataset/shuffle-word-1100-count.jsonl\n",
      "Generated a single JSONL file with 153 samples (75 token repeat) - 1320 max words - at ./dataset/shuffle-word-1320-count.jsonl\n",
      "Generated JSONL file with - 1340 max words, 75 samples - at ./dataset/gen-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 193 samples (75 token repeat) - 1300 max words - at ./dataset/shuffle-word-1300-count.jsonl\n",
      "Generated a single JSONL file with 224 samples (75 token repeat) - 1200 max words - at ./dataset/shuffle-word-1200-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2750 max words - at ./dataset/shuffle-word-2750-count.jsonl\n",
      "Generated JSONL file with - 1560 max words, 75 samples - at ./dataset/gen-word-1560-count.jsonl\n",
      "Generated JSONL file with - 1350 max words, 75 samples - at ./dataset/gen-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1410 max words, 75 samples - at ./dataset/gen-word-1410-count.jsonl\n",
      "Generated a single JSONL file with 187 samples (75 token repeat) - 1230 max words - at ./dataset/shuffle-word-1230-count.jsonl\n",
      "Generated JSONL file with - 1420 max words, 75 samples - at ./dataset/gen-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (75 token repeat) - 1250 max words - at ./dataset/shuffle-word-1250-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (75 token repeat) - 1280 max words - at ./dataset/shuffle-word-1280-count.jsonl\n",
      "Generated JSONL file with - 1710 max words, 75 samples - at ./dataset/gen-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 222 samples (75 token repeat) - 1190 max words - at ./dataset/shuffle-word-1190-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1600 max words - at ./dataset/shuffle-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1440 max words, 75 samples - at ./dataset/gen-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1310 max words, 75 samples - at ./dataset/gen-word-1310-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (75 token repeat) - 1220 max words - at ./dataset/shuffle-word-1220-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (75 token repeat) - 1260 max words - at ./dataset/shuffle-word-1260-count.jsonl\n",
      "Generated JSONL file with - 1430 max words, 75 samples - at ./dataset/gen-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1450 max words, 75 samples - at ./dataset/gen-word-1450-count.jsonl\n",
      "Generated a single JSONL file with 151 samples (75 token repeat) - 1370 max words - at ./dataset/shuffle-word-1370-count.jsonl\n",
      "Generated JSONL file with - 1400 max words, 75 samples - at ./dataset/gen-word-1400-count.jsonl\n",
      "Generated a single JSONL file with 151 samples (75 token repeat) - 1340 max words - at ./dataset/shuffle-word-1340-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1740 max words - at ./dataset/shuffle-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1420 max words - at ./dataset/shuffle-word-1420-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1570 max words - at ./dataset/shuffle-word-1570-count.jsonl\n",
      "Generated a single JSONL file with 153 samples (75 token repeat) - 1390 max words - at ./dataset/shuffle-word-1390-count.jsonl\n",
      "Generated JSONL file with - 1470 max words, 75 samples - at ./dataset/gen-word-1470-count.jsonl\n",
      "Generated a single JSONL file with 153 samples (75 token repeat) - 1380 max words - at ./dataset/shuffle-word-1380-count.jsonl\n",
      "Generated JSONL file with - 1500 max words, 75 samples - at ./dataset/gen-word-1500-count.jsonl\n",
      "Generated JSONL file with - 1850 max words, 75 samples - at ./dataset/gen-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1440 max words - at ./dataset/shuffle-word-1440-count.jsonl\n",
      "Generated JSONL file with - 1490 max words, 75 samples - at ./dataset/gen-word-1490-count.jsonl\n",
      "Generated a single JSONL file with 153 samples (75 token repeat) - 1400 max words - at ./dataset/shuffle-word-1400-count.jsonl\n",
      "Generated JSONL file with - 1480 max words, 75 samples - at ./dataset/gen-word-1480-count.jsonl\n",
      "Generated JSONL file with - 1580 max words, 75 samples - at ./dataset/gen-word-1580-count.jsonl\n",
      "Generated JSONL file with - 1600 max words, 75 samples - at ./dataset/gen-word-1600-count.jsonl\n",
      "Generated JSONL file with - 1550 max words, 75 samples - at ./dataset/gen-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1480 max words - at ./dataset/shuffle-word-1480-count.jsonl\n",
      "Generated JSONL file with - 1530 max words, 75 samples - at ./dataset/gen-word-1530-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1410 max words - at ./dataset/shuffle-word-1410-count.jsonl\n",
      "Generated JSONL file with - 1510 max words, 75 samples - at ./dataset/gen-word-1510-count.jsonl\n",
      "Generated JSONL file with - 1660 max words, 75 samples - at ./dataset/gen-word-1660-count.jsonl\n",
      "Generated JSONL file with - 1540 max words, 75 samples - at ./dataset/gen-word-1540-count.jsonl\n",
      "Generated JSONL file with - 1520 max words, 75 samples - at ./dataset/gen-word-1520-count.jsonl\n",
      "Generated JSONL file with - 1460 max words, 75 samples - at ./dataset/gen-word-1460-count.jsonl\n",
      "Generated JSONL file with - 1570 max words, 75 samples - at ./dataset/gen-word-1570-count.jsonl\n",
      "Generated JSONL file with - 1670 max words, 75 samples - at ./dataset/gen-word-1670-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1450 max words - at ./dataset/shuffle-word-1450-count.jsonlGenerated a single JSONL file with 150 samples (75 token repeat) - 1470 max words - at ./dataset/shuffle-word-1470-count.jsonl\n",
      "\n",
      "Generated a single JSONL file with 153 samples (75 token repeat) - 1350 max words - at ./dataset/shuffle-word-1350-count.jsonl\n",
      "Generated JSONL file with - 1630 max words, 75 samples - at ./dataset/gen-word-1630-count.jsonl\n",
      "Generated JSONL file with - 1650 max words, 75 samples - at ./dataset/gen-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1540 max words - at ./dataset/shuffle-word-1540-count.jsonl\n",
      "Generated JSONL file with - 1680 max words, 75 samples - at ./dataset/gen-word-1680-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1430 max words - at ./dataset/shuffle-word-1430-count.jsonl\n",
      "Generated JSONL file with - 1700 max words, 75 samples - at ./dataset/gen-word-1700-count.jsonl\n",
      "Generated JSONL file with - 3225 max words, 100 samples - at ./dataset/gen-word-3225-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1640 max words - at ./dataset/shuffle-word-1640-count.jsonl\n",
      "Generated JSONL file with - 1620 max words, 75 samples - at ./dataset/gen-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1520 max words - at ./dataset/shuffle-word-1520-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1490 max words - at ./dataset/shuffle-word-1490-count.jsonl\n",
      "Generated JSONL file with - 1640 max words, 75 samples - at ./dataset/gen-word-1640-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1530 max words - at ./dataset/shuffle-word-1530-count.jsonl\n",
      "Generated JSONL file with - 1720 max words, 75 samples - at ./dataset/gen-word-1720-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1510 max words - at ./dataset/shuffle-word-1510-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1610 max words - at ./dataset/shuffle-word-1610-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1580 max words - at ./dataset/shuffle-word-1580-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1550 max words - at ./dataset/shuffle-word-1550-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1560 max words - at ./dataset/shuffle-word-1560-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1700 max words - at ./dataset/shuffle-word-1700-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1620 max words - at ./dataset/shuffle-word-1620-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1590 max words - at ./dataset/shuffle-word-1590-count.jsonl\n",
      "Generated JSONL file with - 1690 max words, 75 samples - at ./dataset/gen-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1650 max words - at ./dataset/shuffle-word-1650-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1630 max words - at ./dataset/shuffle-word-1630-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1670 max words - at ./dataset/shuffle-word-1670-count.jsonl\n",
      "Generated JSONL file with - 1770 max words, 75 samples - at ./dataset/gen-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1830 max words, 75 samples - at ./dataset/gen-word-1830-count.jsonl\n",
      "Generated JSONL file with - 1730 max words, 75 samples - at ./dataset/gen-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1770 max words - at ./dataset/shuffle-word-1770-count.jsonl\n",
      "Generated JSONL file with - 1800 max words, 75 samples - at ./dataset/gen-word-1800-count.jsonl\n",
      "Generated JSONL file with - 1810 max words, 75 samples - at ./dataset/gen-word-1810-count.jsonl\n",
      "Generated JSONL file with - 1590 max words, 75 samples - at ./dataset/gen-word-1590-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3600 max words - at ./dataset/shuffle-word-3600-count.jsonl\n",
      "Generated JSONL file with - 1840 max words, 75 samples - at ./dataset/gen-word-1840-count.jsonl\n",
      "Generated JSONL file with - 1780 max words, 75 samples - at ./dataset/gen-word-1780-count.jsonl\n",
      "Generated JSONL file with - 1610 max words, 75 samples - at ./dataset/gen-word-1610-count.jsonl\n",
      "Generated JSONL file with - 1790 max words, 75 samples - at ./dataset/gen-word-1790-count.jsonl\n",
      "Generated JSONL file with - 1750 max words, 75 samples - at ./dataset/gen-word-1750-count.jsonl\n",
      "Generated JSONL file with - 1740 max words, 75 samples - at ./dataset/gen-word-1740-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1730 max words - at ./dataset/shuffle-word-1730-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1690 max words - at ./dataset/shuffle-word-1690-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1660 max words - at ./dataset/shuffle-word-1660-count.jsonl\n",
      "Generated JSONL file with - 1900 max words, 75 samples - at ./dataset/gen-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1680 max words - at ./dataset/shuffle-word-1680-count.jsonl\n",
      "Generated JSONL file with - 1860 max words, 75 samples - at ./dataset/gen-word-1860-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1710 max words - at ./dataset/shuffle-word-1710-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2000 max words - at ./dataset/shuffle-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1840 max words - at ./dataset/shuffle-word-1840-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1830 max words - at ./dataset/shuffle-word-1830-count.jsonl\n",
      "Generated JSONL file with - 2220 max words, 75 samples - at ./dataset/gen-word-2220-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1850 max words - at ./dataset/shuffle-word-1850-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1760 max words - at ./dataset/shuffle-word-1760-count.jsonl\n",
      "Generated JSONL file with - 3550 max words, 100 samples - at ./dataset/gen-word-3550-count.jsonl\n",
      "Generated JSONL file with - 1920 max words, 75 samples - at ./dataset/gen-word-1920-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1780 max words - at ./dataset/shuffle-word-1780-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1790 max words - at ./dataset/shuffle-word-1790-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1810 max words - at ./dataset/shuffle-word-1810-count.jsonl\n",
      "Generated JSONL file with - 1880 max words, 75 samples - at ./dataset/gen-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1860 max words - at ./dataset/shuffle-word-1860-count.jsonl\n",
      "Generated JSONL file with - 1910 max words, 75 samples - at ./dataset/gen-word-1910-count.jsonl\n",
      "Generated JSONL file with - 1930 max words, 75 samples - at ./dataset/gen-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1800 max words - at ./dataset/shuffle-word-1800-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1920 max words - at ./dataset/shuffle-word-1920-count.jsonl\n",
      "Generated JSONL file with - 1870 max words, 75 samples - at ./dataset/gen-word-1870-count.jsonl\n",
      "Generated JSONL file with - 1890 max words, 75 samples - at ./dataset/gen-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1940 max words - at ./dataset/shuffle-word-1940-count.jsonl\n",
      "Generated JSONL file with - 3175 max words, 100 samples - at ./dataset/gen-word-3175-count.jsonl\n",
      "Generated JSONL file with - 1990 max words, 75 samples - at ./dataset/gen-word-1990-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1890 max words - at ./dataset/shuffle-word-1890-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1880 max words - at ./dataset/shuffle-word-1880-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5200 max words - at ./dataset/shuffle-word-5200-count.jsonl\n",
      "Generated JSONL file with - 2000 max words, 75 samples - at ./dataset/gen-word-2000-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1820 max words - at ./dataset/shuffle-word-1820-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1910 max words - at ./dataset/shuffle-word-1910-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1930 max words - at ./dataset/shuffle-word-1930-count.jsonl\n",
      "Generated a single JSONL file with 55753 samples (100 token repeat) - 5 max words - at ./dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1870 max words - at ./dataset/shuffle-word-1870-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1990 max words - at ./dataset/shuffle-word-1990-count.jsonl\n",
      "Generated JSONL file with - 1970 max words, 75 samples - at ./dataset/gen-word-1970-count.jsonl\n",
      "Generated JSONL file with - 2020 max words, 75 samples - at ./dataset/gen-word-2020-count.jsonl\n",
      "Generated JSONL file with - 2040 max words, 75 samples - at ./dataset/gen-word-2040-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1960 max words - at ./dataset/shuffle-word-1960-count.jsonl\n",
      "Generated JSONL file with - 2070 max words, 75 samples - at ./dataset/gen-word-2070-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2030 max words - at ./dataset/shuffle-word-2030-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1980 max words - at ./dataset/shuffle-word-1980-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1950 max words - at ./dataset/shuffle-word-1950-count.jsonl\n",
      "Generated JSONL file with - 2030 max words, 75 samples - at ./dataset/gen-word-2030-count.jsonl\n",
      "Generated JSONL file with - 2090 max words, 75 samples - at ./dataset/gen-word-2090-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2020 max words - at ./dataset/shuffle-word-2020-count.jsonl\n",
      "Generated JSONL file with - 1950 max words, 75 samples - at ./dataset/gen-word-1950-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2040 max words - at ./dataset/shuffle-word-2040-count.jsonl\n",
      "Generated JSONL file with - 3825 max words, 100 samples - at ./dataset/gen-word-3825-count.jsonl\n",
      "Generated JSONL file with - 2150 max words, 75 samples - at ./dataset/gen-word-2150-count.jsonl\n",
      "Generated JSONL file with - 2390 max words, 75 samples - at ./dataset/gen-word-2390-count.jsonl\n",
      "Generated JSONL file with - 2120 max words, 75 samples - at ./dataset/gen-word-2120-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2010 max words - at ./dataset/shuffle-word-2010-count.jsonl\n",
      "Generated a single JSONL file with 89 samples (75 token repeat) - 2690 max words - at ./dataset/shuffle-word-2690-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1900 max words - at ./dataset/shuffle-word-1900-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2070 max words - at ./dataset/shuffle-word-2070-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (75 token repeat) - 2410 max words - at ./dataset/shuffle-word-2410-count.jsonl\n",
      "Generated a single JSONL file with 149 samples (75 token repeat) - 2390 max words - at ./dataset/shuffle-word-2390-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2100 max words - at ./dataset/shuffle-word-2100-count.jsonl\n",
      "Generated JSONL file with - 2140 max words, 75 samples - at ./dataset/gen-word-2140-count.jsonl\n",
      "Generated a single JSONL file with 84 samples (75 token repeat) - 2700 max words - at ./dataset/shuffle-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 147 samples (75 token repeat) - 2350 max words - at ./dataset/shuffle-word-2350-count.jsonl\n",
      "Generated JSONL file with - 2200 max words, 75 samples - at ./dataset/gen-word-2200-count.jsonl\n",
      "Generated JSONL file with - 2270 max words, 75 samples - at ./dataset/gen-word-2270-count.jsonl\n",
      "Generated JSONL file with - 4400 max words, 100 samples - at ./dataset/gen-word-4400-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (75 token repeat) - 2790 max words - at ./dataset/shuffle-word-2790-count.jsonl\n",
      "Generated a single JSONL file with 110 samples (75 token repeat) - 2540 max words - at ./dataset/shuffle-word-2540-count.jsonl\n",
      "Generated JSONL file with - 2190 max words, 75 samples - at ./dataset/gen-word-2190-count.jsonl\n",
      "Generated JSONL file with - 2480 max words, 75 samples - at ./dataset/gen-word-2480-count.jsonl\n",
      "Generated JSONL file with - 2580 max words, 75 samples - at ./dataset/gen-word-2580-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3300 max words - at ./dataset/shuffle-word-3300-count.jsonl\n",
      "Generated JSONL file with - 2280 max words, 75 samples - at ./dataset/gen-word-2280-count.jsonl\n",
      "Generated JSONL file with - 2100 max words, 75 samples - at ./dataset/gen-word-2100-count.jsonl\n",
      "Generated JSONL file with - 1980 max words, 75 samples - at ./dataset/gen-word-1980-count.jsonl\n",
      "Generated JSONL file with - 2050 max words, 75 samples - at ./dataset/gen-word-2050-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2800 max words - at ./dataset/shuffle-word-2800-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2160 max words - at ./dataset/shuffle-word-2160-count.jsonl\n",
      "Generated JSONL file with - 2080 max words, 75 samples - at ./dataset/gen-word-2080-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2080 max words - at ./dataset/shuffle-word-2080-count.jsonl\n",
      "Generated a single JSONL file with 149 samples (75 token repeat) - 2270 max words - at ./dataset/shuffle-word-2270-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2060 max words - at ./dataset/shuffle-word-2060-count.jsonl\n",
      "Generated JSONL file with - 2010 max words, 75 samples - at ./dataset/gen-word-2010-count.jsonl\n",
      "Generated JSONL file with - 2110 max words, 75 samples - at ./dataset/gen-word-2110-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2140 max words - at ./dataset/shuffle-word-2140-count.jsonl\n",
      "Generated JSONL file with - 4150 max words, 100 samples - at ./dataset/gen-word-4150-count.jsonl\n",
      "Generated JSONL file with - 1960 max words, 75 samples - at ./dataset/gen-word-1960-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2090 max words - at ./dataset/shuffle-word-2090-count.jsonlGenerated a single JSONL file with 150 samples (75 token repeat) - 2130 max words - at ./dataset/shuffle-word-2130-count.jsonl\n",
      "\n",
      "Generated JSONL file with - 2130 max words, 75 samples - at ./dataset/gen-word-2130-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2050 max words - at ./dataset/shuffle-word-2050-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2120 max words - at ./dataset/shuffle-word-2120-count.jsonl\n",
      "Generated JSONL file with - 2170 max words, 75 samples - at ./dataset/gen-word-2170-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2110 max words - at ./dataset/shuffle-word-2110-count.jsonl\n",
      "Generated JSONL file with - 4275 max words, 100 samples - at ./dataset/gen-word-4275-count.jsonl\n",
      "Generated JSONL file with - 2160 max words, 75 samples - at ./dataset/gen-word-2160-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2170 max words - at ./dataset/shuffle-word-2170-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4575 max words - at ./dataset/shuffle-word-4575-count.jsonl\n",
      "Generated JSONL file with - 4500 max words, 100 samples - at ./dataset/gen-word-4500-count.jsonl\n",
      "Generated JSONL file with - 2180 max words, 75 samples - at ./dataset/gen-word-2180-count.jsonl\n",
      "Generated JSONL file with - 2410 max words, 75 samples - at ./dataset/gen-word-2410-count.jsonl\n",
      "Generated JSONL file with - 2400 max words, 75 samples - at ./dataset/gen-word-2400-count.jsonl\n",
      "Generated JSONL file with - 2560 max words, 75 samples - at ./dataset/gen-word-2560-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2200 max words - at ./dataset/shuffle-word-2200-count.jsonl\n",
      "Generated JSONL file with - 4750 max words, 100 samples - at ./dataset/gen-word-4750-count.jsonl\n",
      "Generated JSONL file with - 2570 max words, 75 samples - at ./dataset/gen-word-2570-count.jsonl\n",
      "Generated JSONL file with - 3275 max words, 100 samples - at ./dataset/gen-word-3275-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2210 max words - at ./dataset/shuffle-word-2210-count.jsonl\n",
      "Generated JSONL file with - 3300 max words, 100 samples - at ./dataset/gen-word-3300-count.jsonl\n",
      "Generated a single JSONL file with 137 samples (75 token repeat) - 2470 max words - at ./dataset/shuffle-word-2470-count.jsonl\n",
      "Generated JSONL file with - 2250 max words, 75 samples - at ./dataset/gen-word-2250-count.jsonl\n",
      "Generated JSONL file with - 1940 max words, 75 samples - at ./dataset/gen-word-1940-count.jsonl\n",
      "Generated JSONL file with - 2260 max words, 75 samples - at ./dataset/gen-word-2260-count.jsonl\n",
      "Generated JSONL file with - 2060 max words, 75 samples - at ./dataset/gen-word-2060-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2150 max words - at ./dataset/shuffle-word-2150-count.jsonl\n",
      "Generated JSONL file with - 2230 max words, 75 samples - at ./dataset/gen-word-2230-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2180 max words - at ./dataset/shuffle-word-2180-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2230 max words - at ./dataset/shuffle-word-2230-count.jsonl\n",
      "Generated JSONL file with - 6000 max words, 100 samples - at ./dataset/gen-word-6000-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2190 max words - at ./dataset/shuffle-word-2190-count.jsonl\n",
      "Generated JSONL file with - 2660 max words, 75 samples - at ./dataset/gen-word-2660-count.jsonl\n",
      "Generated JSONL file with - 2760 max words, 75 samples - at ./dataset/gen-word-2760-count.jsonl\n",
      "Generated JSONL file with - 2720 max words, 75 samples - at ./dataset/gen-word-2720-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2220 max words - at ./dataset/shuffle-word-2220-count.jsonl\n",
      "Generated a single JSONL file with 86 samples (75 token repeat) - 2620 max words - at ./dataset/shuffle-word-2620-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2860 max words - at ./dataset/shuffle-word-2860-count.jsonl\n",
      "Generated a single JSONL file with 148 samples (75 token repeat) - 2310 max words - at ./dataset/shuffle-word-2310-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 1970 max words - at ./dataset/shuffle-word-1970-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2280 max words - at ./dataset/shuffle-word-2280-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2900 max words - at ./dataset/shuffle-word-2900-count.jsonl\n",
      "Generated JSONL file with - 2330 max words, 75 samples - at ./dataset/gen-word-2330-count.jsonl\n",
      "Generated JSONL file with - 2350 max words, 75 samples - at ./dataset/gen-word-2350-count.jsonl\n",
      "Generated JSONL file with - 2310 max words, 75 samples - at ./dataset/gen-word-2310-count.jsonl\n",
      "Generated JSONL file with - 2370 max words, 75 samples - at ./dataset/gen-word-2370-count.jsonl\n",
      "Generated a single JSONL file with 149 samples (75 token repeat) - 2340 max words - at ./dataset/shuffle-word-2340-count.jsonl\n",
      "Generated JSONL file with - 2380 max words, 75 samples - at ./dataset/gen-word-2380-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2320 max words - at ./dataset/shuffle-word-2320-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2250 max words - at ./dataset/shuffle-word-2250-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3375 max words - at ./dataset/shuffle-word-3375-count.jsonl\n",
      "Generated a single JSONL file with 149 samples (75 token repeat) - 2360 max words - at ./dataset/shuffle-word-2360-count.jsonl\n",
      "Generated JSONL file with - 2320 max words, 75 samples - at ./dataset/gen-word-2320-count.jsonl\n",
      "Generated a single JSONL file with 148 samples (75 token repeat) - 2370 max words - at ./dataset/shuffle-word-2370-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3125 max words - at ./dataset/shuffle-word-3125-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2300 max words - at ./dataset/shuffle-word-2300-count.jsonl\n",
      "Generated JSONL file with - 2210 max words, 75 samples - at ./dataset/gen-word-2210-count.jsonl\n",
      "Generated JSONL file with - 3350 max words, 100 samples - at ./dataset/gen-word-3350-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (75 token repeat) - 2430 max words - at ./dataset/shuffle-word-2430-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (75 token repeat) - 2440 max words - at ./dataset/shuffle-word-2440-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2290 max words - at ./dataset/shuffle-word-2290-count.jsonl\n",
      "Generated a single JSONL file with 149 samples (75 token repeat) - 2400 max words - at ./dataset/shuffle-word-2400-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2330 max words - at ./dataset/shuffle-word-2330-count.jsonl\n",
      "Generated JSONL file with - 2240 max words, 75 samples - at ./dataset/gen-word-2240-count.jsonl\n",
      "Generated JSONL file with - 2450 max words, 75 samples - at ./dataset/gen-word-2450-count.jsonl\n",
      "Generated JSONL file with - 2290 max words, 75 samples - at ./dataset/gen-word-2290-count.jsonl\n",
      "Generated JSONL file with - 2430 max words, 75 samples - at ./dataset/gen-word-2430-count.jsonl\n",
      "Generated JSONL file with - 4175 max words, 100 samples - at ./dataset/gen-word-4175-count.jsonl\n",
      "Generated JSONL file with - 3575 max words, 100 samples - at ./dataset/gen-word-3575-count.jsonl\n",
      "Generated JSONL file with - 2510 max words, 75 samples - at ./dataset/gen-word-2510-count.jsonl\n",
      "Generated JSONL file with - 2470 max words, 75 samples - at ./dataset/gen-word-2470-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2240 max words - at ./dataset/shuffle-word-2240-count.jsonl\n",
      "Generated a single JSONL file with 134 samples (75 token repeat) - 2450 max words - at ./dataset/shuffle-word-2450-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (75 token repeat) - 2510 max words - at ./dataset/shuffle-word-2510-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (75 token repeat) - 2460 max words - at ./dataset/shuffle-word-2460-count.jsonl\n",
      "Generated JSONL file with - 2420 max words, 75 samples - at ./dataset/gen-word-2420-count.jsonl\n",
      "Generated JSONL file with - 2440 max words, 75 samples - at ./dataset/gen-word-2440-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (75 token repeat) - 2500 max words - at ./dataset/shuffle-word-2500-count.jsonl\n",
      "Generated a single JSONL file with 136 samples (75 token repeat) - 2490 max words - at ./dataset/shuffle-word-2490-count.jsonl\n",
      "Generated JSONL file with - 2360 max words, 75 samples - at ./dataset/gen-word-2360-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (75 token repeat) - 2420 max words - at ./dataset/shuffle-word-2420-count.jsonl\n",
      "Generated a single JSONL file with 112 samples (75 token repeat) - 2520 max words - at ./dataset/shuffle-word-2520-count.jsonl\n",
      "Generated JSONL file with - 2460 max words, 75 samples - at ./dataset/gen-word-2460-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2260 max words - at ./dataset/shuffle-word-2260-count.jsonl\n",
      "Generated a single JSONL file with 132 samples (75 token repeat) - 2480 max words - at ./dataset/shuffle-word-2480-count.jsonl\n",
      "Generated JSONL file with - 2340 max words, 75 samples - at ./dataset/gen-word-2340-count.jsonl\n",
      "Generated JSONL file with - 2300 max words, 75 samples - at ./dataset/gen-word-2300-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (75 token repeat) - 2550 max words - at ./dataset/shuffle-word-2550-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (75 token repeat) - 2580 max words - at ./dataset/shuffle-word-2580-count.jsonl\n",
      "Generated JSONL file with - 2530 max words, 75 samples - at ./dataset/gen-word-2530-count.jsonl\n",
      "Generated JSONL file with - 2520 max words, 75 samples - at ./dataset/gen-word-2520-count.jsonl\n",
      "Generated JSONL file with - 2550 max words, 75 samples - at ./dataset/gen-word-2550-count.jsonl\n",
      "Generated JSONL file with - 2600 max words, 75 samples - at ./dataset/gen-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 150 samples (75 token repeat) - 2380 max words - at ./dataset/shuffle-word-2380-count.jsonl\n",
      "Generated a single JSONL file with 111 samples (75 token repeat) - 2570 max words - at ./dataset/shuffle-word-2570-count.jsonl\n",
      "Generated JSONL file with - 2610 max words, 75 samples - at ./dataset/gen-word-2610-count.jsonl\n",
      "Generated JSONL file with - 2540 max words, 75 samples - at ./dataset/gen-word-2540-count.jsonl\n",
      "Generated a single JSONL file with 90 samples (75 token repeat) - 2650 max words - at ./dataset/shuffle-word-2650-count.jsonl\n",
      "Generated JSONL file with - 2640 max words, 75 samples - at ./dataset/gen-word-2640-count.jsonl\n",
      "Generated JSONL file with - 2590 max words, 75 samples - at ./dataset/gen-word-2590-count.jsonl\n",
      "Generated JSONL file with - 2650 max words, 75 samples - at ./dataset/gen-word-2650-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4075 max words - at ./dataset/shuffle-word-4075-count.jsonl\n",
      "Generated JSONL file with - 2710 max words, 75 samples - at ./dataset/gen-word-2710-count.jsonl\n",
      "Generated JSONL file with - 2670 max words, 75 samples - at ./dataset/gen-word-2670-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4550 max words - at ./dataset/shuffle-word-4550-count.jsonl\n",
      "Generated a single JSONL file with 89 samples (75 token repeat) - 2670 max words - at ./dataset/shuffle-word-2670-count.jsonl\n",
      "Generated JSONL file with - 2690 max words, 75 samples - at ./dataset/gen-word-2690-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2710 max words - at ./dataset/shuffle-word-2710-count.jsonl\n",
      "Generated a single JSONL file with 87 samples (75 token repeat) - 2610 max words - at ./dataset/shuffle-word-2610-count.jsonl\n",
      "Generated a single JSONL file with 86 samples (75 token repeat) - 2640 max words - at ./dataset/shuffle-word-2640-count.jsonl\n",
      "Generated JSONL file with - 2620 max words, 75 samples - at ./dataset/gen-word-2620-count.jsonl\n",
      "Generated a single JSONL file with 88 samples (75 token repeat) - 2660 max words - at ./dataset/shuffle-word-2660-count.jsonl\n",
      "Generated JSONL file with - 2630 max words, 75 samples - at ./dataset/gen-word-2630-count.jsonl\n",
      "Generated a single JSONL file with 113 samples (75 token repeat) - 2600 max words - at ./dataset/shuffle-word-2600-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2730 max words - at ./dataset/shuffle-word-2730-count.jsonl\n",
      "Generated JSONL file with - 2740 max words, 75 samples - at ./dataset/gen-word-2740-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5350 max words - at ./dataset/shuffle-word-5350-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (75 token repeat) - 2720 max words - at ./dataset/shuffle-word-2720-count.jsonl\n",
      "Generated JSONL file with - 2680 max words, 75 samples - at ./dataset/gen-word-2680-count.jsonl\n",
      "Generated a single JSONL file with 111 samples (75 token repeat) - 2590 max words - at ./dataset/shuffle-word-2590-count.jsonl\n",
      "Generated a single JSONL file with 114 samples (75 token repeat) - 2530 max words - at ./dataset/shuffle-word-2530-count.jsonl\n",
      "Generated JSONL file with - 2750 max words, 75 samples - at ./dataset/gen-word-2750-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (75 token repeat) - 2760 max words - at ./dataset/shuffle-word-2760-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2740 max words - at ./dataset/shuffle-word-2740-count.jsonl\n",
      "Generated a single JSONL file with 90 samples (75 token repeat) - 2680 max words - at ./dataset/shuffle-word-2680-count.jsonl\n",
      "Generated a single JSONL file with 90 samples (75 token repeat) - 2630 max words - at ./dataset/shuffle-word-2630-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (75 token repeat) - 2810 max words - at ./dataset/shuffle-word-2810-count.jsonl\n",
      "Generated JSONL file with - 2730 max words, 75 samples - at ./dataset/gen-word-2730-count.jsonl\n",
      "Generated a single JSONL file with 77 samples (75 token repeat) - 2770 max words - at ./dataset/shuffle-word-2770-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5625 max words - at ./dataset/shuffle-word-5625-count.jsonl\n",
      "Generated a single JSONL file with 78 samples (75 token repeat) - 2780 max words - at ./dataset/shuffle-word-2780-count.jsonl\n",
      "Generated JSONL file with - 2780 max words, 75 samples - at ./dataset/gen-word-2780-count.jsonl\n",
      "Generated JSONL file with - 4250 max words, 100 samples - at ./dataset/gen-word-4250-count.jsonl\n",
      "Generated JSONL file with - 2790 max words, 75 samples - at ./dataset/gen-word-2790-count.jsonl\n",
      "Generated JSONL file with - 4075 max words, 100 samples - at ./dataset/gen-word-4075-count.jsonl\n",
      "Generated JSONL file with - 4025 max words, 100 samples - at ./dataset/gen-word-4025-count.jsonl\n",
      "Generated JSONL file with - 2800 max words, 75 samples - at ./dataset/gen-word-2800-count.jsonl\n",
      "Generated JSONL file with - 4225 max words, 100 samples - at ./dataset/gen-word-4225-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2850 max words - at ./dataset/shuffle-word-2850-count.jsonl\n",
      "Generated JSONL file with - 4375 max words, 100 samples - at ./dataset/gen-word-4375-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2830 max words - at ./dataset/shuffle-word-2830-count.jsonl\n",
      "Generated JSONL file with - 2830 max words, 75 samples - at ./dataset/gen-word-2830-count.jsonl\n",
      "Generated JSONL file with - 2810 max words, 75 samples - at ./dataset/gen-word-2810-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (75 token repeat) - 2820 max words - at ./dataset/shuffle-word-2820-count.jsonl\n",
      "Generated JSONL file with - 4300 max words, 100 samples - at ./dataset/gen-word-4300-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2880 max words - at ./dataset/shuffle-word-2880-count.jsonl\n",
      "Generated JSONL file with - 2840 max words, 75 samples - at ./dataset/gen-word-2840-count.jsonl\n",
      "Generated JSONL file with - 2500 max words, 75 samples - at ./dataset/gen-word-2500-count.jsonl\n",
      "Generated JSONL file with - 4475 max words, 100 samples - at ./dataset/gen-word-4475-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2840 max words - at ./dataset/shuffle-word-2840-count.jsonl\n",
      "Generated JSONL file with - 2850 max words, 75 samples - at ./dataset/gen-word-2850-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2910 max words - at ./dataset/shuffle-word-2910-count.jsonl\n",
      "Generated JSONL file with - 2490 max words, 75 samples - at ./dataset/gen-word-2490-count.jsonl\n",
      "Generated JSONL file with - 2820 max words, 75 samples - at ./dataset/gen-word-2820-count.jsonl\n",
      "Generated JSONL file with - 2860 max words, 75 samples - at ./dataset/gen-word-2860-count.jsonl\n",
      "Generated a single JSONL file with 76 samples (75 token repeat) - 2870 max words - at ./dataset/shuffle-word-2870-count.jsonl\n",
      "Generated JSONL file with - 2770 max words, 75 samples - at ./dataset/gen-word-2770-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (75 token repeat) - 2560 max words - at ./dataset/shuffle-word-2560-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2950 max words - at ./dataset/shuffle-word-2950-count.jsonl\n",
      "Generated JSONL file with - 3200 max words, 100 samples - at ./dataset/gen-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2970 max words - at ./dataset/shuffle-word-2970-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2930 max words - at ./dataset/shuffle-word-2930-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2890 max words - at ./dataset/shuffle-word-2890-count.jsonl\n",
      "Generated JSONL file with - 4675 max words, 100 samples - at ./dataset/gen-word-4675-count.jsonl\n",
      "Generated JSONL file with - 2700 max words, 75 samples - at ./dataset/gen-word-2700-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2980 max words - at ./dataset/shuffle-word-2980-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2940 max words - at ./dataset/shuffle-word-2940-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2960 max words - at ./dataset/shuffle-word-2960-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2920 max words - at ./dataset/shuffle-word-2920-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3075 max words - at ./dataset/shuffle-word-3075-count.jsonl\n",
      "Generated JSONL file with - 4825 max words, 100 samples - at ./dataset/gen-word-4825-count.jsonl\n",
      "Generated JSONL file with - 2920 max words, 75 samples - at ./dataset/gen-word-2920-count.jsonl\n",
      "Generated JSONL file with - 2870 max words, 75 samples - at ./dataset/gen-word-2870-count.jsonl\n",
      "Generated JSONL file with - 2940 max words, 75 samples - at ./dataset/gen-word-2940-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3025 max words - at ./dataset/shuffle-word-3025-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 3000 max words - at ./dataset/shuffle-word-3000-count.jsonl\n",
      "Generated a single JSONL file with 75 samples (75 token repeat) - 2990 max words - at ./dataset/shuffle-word-2990-count.jsonl\n",
      "Generated JSONL file with - 2880 max words, 75 samples - at ./dataset/gen-word-2880-count.jsonl\n",
      "Generated JSONL file with - 2890 max words, 75 samples - at ./dataset/gen-word-2890-count.jsonl\n",
      "Generated JSONL file with - 2950 max words, 75 samples - at ./dataset/gen-word-2950-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3150 max words - at ./dataset/shuffle-word-3150-count.jsonl\n",
      "Generated JSONL file with - 3000 max words, 75 samples - at ./dataset/gen-word-3000-count.jsonl\n",
      "Generated JSONL file with - 3125 max words, 100 samples - at ./dataset/gen-word-3125-count.jsonl\n",
      "Generated JSONL file with - 2990 max words, 75 samples - at ./dataset/gen-word-2990-count.jsonl\n",
      "Generated JSONL file with - 2910 max words, 75 samples - at ./dataset/gen-word-2910-count.jsonl\n",
      "Generated JSONL file with - 2970 max words, 75 samples - at ./dataset/gen-word-2970-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3175 max words - at ./dataset/shuffle-word-3175-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3550 max words - at ./dataset/shuffle-word-3550-count.jsonl\n",
      "Generated JSONL file with - 2980 max words, 75 samples - at ./dataset/gen-word-2980-count.jsonl\n",
      "Generated JSONL file with - 3075 max words, 100 samples - at ./dataset/gen-word-3075-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4100 max words - at ./dataset/shuffle-word-4100-count.jsonl\n",
      "Generated JSONL file with - 2960 max words, 75 samples - at ./dataset/gen-word-2960-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3050 max words - at ./dataset/shuffle-word-3050-count.jsonl\n",
      "Generated JSONL file with - 3150 max words, 100 samples - at ./dataset/gen-word-3150-count.jsonl\n",
      "Generated JSONL file with - 2930 max words, 75 samples - at ./dataset/gen-word-2930-count.jsonl\n",
      "Generated JSONL file with - 5475 max words, 100 samples - at ./dataset/gen-word-5475-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3725 max words - at ./dataset/shuffle-word-3725-count.jsonl\n",
      "Generated JSONL file with - 3050 max words, 100 samples - at ./dataset/gen-word-3050-count.jsonl\n",
      "Generated JSONL file with - 3025 max words, 100 samples - at ./dataset/gen-word-3025-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3100 max words - at ./dataset/shuffle-word-3100-count.jsonl\n",
      "Generated JSONL file with - 4200 max words, 100 samples - at ./dataset/gen-word-4200-count.jsonl\n",
      "Generated JSONL file with - 3375 max words, 100 samples - at ./dataset/gen-word-3375-count.jsonl\n",
      "Generated JSONL file with - 3800 max words, 100 samples - at ./dataset/gen-word-3800-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3350 max words - at ./dataset/shuffle-word-3350-count.jsonl\n",
      "Generated JSONL file with - 3100 max words, 100 samples - at ./dataset/gen-word-3100-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3225 max words - at ./dataset/shuffle-word-3225-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3325 max words - at ./dataset/shuffle-word-3325-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3275 max words - at ./dataset/shuffle-word-3275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3200 max words - at ./dataset/shuffle-word-3200-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3425 max words - at ./dataset/shuffle-word-3425-count.jsonl\n",
      "Generated JSONL file with - 3250 max words, 100 samples - at ./dataset/gen-word-3250-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3400 max words - at ./dataset/shuffle-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3250 max words - at ./dataset/shuffle-word-3250-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3575 max words - at ./dataset/shuffle-word-3575-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3475 max words - at ./dataset/shuffle-word-3475-count.jsonlGenerated a single JSONL file with 100 samples (100 token repeat) - 3450 max words - at ./dataset/shuffle-word-3450-count.jsonl\n",
      "\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3650 max words - at ./dataset/shuffle-word-3650-count.jsonl\n",
      "Generated JSONL file with - 3450 max words, 100 samples - at ./dataset/gen-word-3450-count.jsonl\n",
      "Generated JSONL file with - 3475 max words, 100 samples - at ./dataset/gen-word-3475-count.jsonl\n",
      "Generated JSONL file with - 3400 max words, 100 samples - at ./dataset/gen-word-3400-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3700 max words - at ./dataset/shuffle-word-3700-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3625 max words - at ./dataset/shuffle-word-3625-count.jsonl\n",
      "Generated JSONL file with - 3425 max words, 100 samples - at ./dataset/gen-word-3425-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3675 max words - at ./dataset/shuffle-word-3675-count.jsonl\n",
      "Generated JSONL file with - 3625 max words, 100 samples - at ./dataset/gen-word-3625-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3825 max words - at ./dataset/shuffle-word-3825-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3900 max words - at ./dataset/shuffle-word-3900-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4225 max words - at ./dataset/shuffle-word-4225-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4050 max words - at ./dataset/shuffle-word-4050-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3950 max words - at ./dataset/shuffle-word-3950-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3500 max words - at ./dataset/shuffle-word-3500-count.jsonl\n",
      "Generated JSONL file with - 4100 max words, 100 samples - at ./dataset/gen-word-4100-count.jsonl\n",
      "Generated JSONL file with - 3500 max words, 100 samples - at ./dataset/gen-word-3500-count.jsonl\n",
      "Generated JSONL file with - 3325 max words, 100 samples - at ./dataset/gen-word-3325-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3775 max words - at ./dataset/shuffle-word-3775-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3875 max words - at ./dataset/shuffle-word-3875-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4300 max words - at ./dataset/shuffle-word-4300-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4200 max words - at ./dataset/shuffle-word-4200-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3800 max words - at ./dataset/shuffle-word-3800-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3975 max words - at ./dataset/shuffle-word-3975-count.jsonl\n",
      "Generated JSONL file with - 3650 max words, 100 samples - at ./dataset/gen-word-3650-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4000 max words - at ./dataset/shuffle-word-4000-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4125 max words - at ./dataset/shuffle-word-4125-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4025 max words - at ./dataset/shuffle-word-4025-count.jsonl\n",
      "Generated JSONL file with - 3600 max words, 100 samples - at ./dataset/gen-word-3600-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4400 max words - at ./dataset/shuffle-word-4400-count.jsonl\n",
      "Generated JSONL file with - 4125 max words, 100 samples - at ./dataset/gen-word-4125-count.jsonl\n",
      "Generated JSONL file with - 3675 max words, 100 samples - at ./dataset/gen-word-3675-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3925 max words - at ./dataset/shuffle-word-3925-count.jsonl\n",
      "Generated JSONL file with - 3875 max words, 100 samples - at ./dataset/gen-word-3875-count.jsonl\n",
      "Generated JSONL file with - 3525 max words, 100 samples - at ./dataset/gen-word-3525-count.jsonl\n",
      "Generated JSONL file with - 3775 max words, 100 samples - at ./dataset/gen-word-3775-count.jsonl\n",
      "Generated JSONL file with - 3700 max words, 100 samples - at ./dataset/gen-word-3700-count.jsonl\n",
      "Generated JSONL file with - 3725 max words, 100 samples - at ./dataset/gen-word-3725-count.jsonl\n",
      "Generated JSONL file with - 4350 max words, 100 samples - at ./dataset/gen-word-4350-count.jsonl\n",
      "Generated JSONL file with - 4050 max words, 100 samples - at ./dataset/gen-word-4050-count.jsonl\n",
      "Generated JSONL file with - 3900 max words, 100 samples - at ./dataset/gen-word-3900-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4325 max words - at ./dataset/shuffle-word-4325-count.jsonl\n",
      "Generated JSONL file with - 3850 max words, 100 samples - at ./dataset/gen-word-3850-count.jsonl\n",
      "Generated JSONL file with - 3925 max words, 100 samples - at ./dataset/gen-word-3925-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4375 max words - at ./dataset/shuffle-word-4375-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4925 max words - at ./dataset/shuffle-word-4925-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4175 max words - at ./dataset/shuffle-word-4175-count.jsonl\n",
      "Generated JSONL file with - 4000 max words, 100 samples - at ./dataset/gen-word-4000-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4825 max words - at ./dataset/shuffle-word-4825-count.jsonl\n",
      "Generated JSONL file with - 3950 max words, 100 samples - at ./dataset/gen-word-3950-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3850 max words - at ./dataset/shuffle-word-3850-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5125 max words - at ./dataset/shuffle-word-5125-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4350 max words - at ./dataset/shuffle-word-4350-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5175 max words - at ./dataset/shuffle-word-5175-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4700 max words - at ./dataset/shuffle-word-4700-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3525 max words - at ./dataset/shuffle-word-3525-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4450 max words - at ./dataset/shuffle-word-4450-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4250 max words - at ./dataset/shuffle-word-4250-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4425 max words - at ./dataset/shuffle-word-4425-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4150 max words - at ./dataset/shuffle-word-4150-count.jsonl\n",
      "Generated JSONL file with - 3975 max words, 100 samples - at ./dataset/gen-word-3975-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4275 max words - at ./dataset/shuffle-word-4275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5425 max words - at ./dataset/shuffle-word-5425-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4850 max words - at ./dataset/shuffle-word-4850-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4975 max words - at ./dataset/shuffle-word-4975-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 3750 max words - at ./dataset/shuffle-word-3750-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5525 max words - at ./dataset/shuffle-word-5525-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4475 max words - at ./dataset/shuffle-word-4475-count.jsonl\n",
      "Generated JSONL file with - 5350 max words, 100 samples - at ./dataset/gen-word-5350-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4500 max words - at ./dataset/shuffle-word-4500-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4525 max words - at ./dataset/shuffle-word-4525-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4675 max words - at ./dataset/shuffle-word-4675-count.jsonl\n",
      "Generated JSONL file with - 3750 max words, 100 samples - at ./dataset/gen-word-3750-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4625 max words - at ./dataset/shuffle-word-4625-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4725 max words - at ./dataset/shuffle-word-4725-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5925 max words - at ./dataset/shuffle-word-5925-count.jsonl\n",
      "Generated JSONL file with - 4325 max words, 100 samples - at ./dataset/gen-word-4325-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4600 max words - at ./dataset/shuffle-word-4600-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4775 max words - at ./dataset/shuffle-word-4775-count.jsonl\n",
      "Generated JSONL file with - 4600 max words, 100 samples - at ./dataset/gen-word-4600-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4650 max words - at ./dataset/shuffle-word-4650-count.jsonl\n",
      "Generated JSONL file with - 4450 max words, 100 samples - at ./dataset/gen-word-4450-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5075 max words - at ./dataset/shuffle-word-5075-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4750 max words - at ./dataset/shuffle-word-4750-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4900 max words - at ./dataset/shuffle-word-4900-count.jsonl\n",
      "Generated JSONL file with - 4550 max words, 100 samples - at ./dataset/gen-word-4550-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5000 max words - at ./dataset/shuffle-word-5000-count.jsonl\n",
      "Generated JSONL file with - 4575 max words, 100 samples - at ./dataset/gen-word-4575-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4875 max words - at ./dataset/shuffle-word-4875-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4950 max words - at ./dataset/shuffle-word-4950-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 4800 max words - at ./dataset/shuffle-word-4800-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5150 max words - at ./dataset/shuffle-word-5150-count.jsonl\n",
      "Generated JSONL file with - 4650 max words, 100 samples - at ./dataset/gen-word-4650-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5050 max words - at ./dataset/shuffle-word-5050-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5225 max words - at ./dataset/shuffle-word-5225-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5250 max words - at ./dataset/shuffle-word-5250-count.jsonl\n",
      "Generated JSONL file with - 5325 max words, 100 samples - at ./dataset/gen-word-5325-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5300 max words - at ./dataset/shuffle-word-5300-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5550 max words - at ./dataset/shuffle-word-5550-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5275 max words - at ./dataset/shuffle-word-5275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5400 max words - at ./dataset/shuffle-word-5400-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5325 max words - at ./dataset/shuffle-word-5325-count.jsonl\n",
      "Generated JSONL file with - 4425 max words, 100 samples - at ./dataset/gen-word-4425-count.jsonl\n",
      "Generated JSONL file with - 4775 max words, 100 samples - at ./dataset/gen-word-4775-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5375 max words - at ./dataset/shuffle-word-5375-count.jsonlGenerated a single JSONL file with 100 samples (100 token repeat) - 5475 max words - at ./dataset/shuffle-word-5475-count.jsonl\n",
      "\n",
      "Generated JSONL file with - 5425 max words, 100 samples - at ./dataset/gen-word-5425-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5575 max words - at ./dataset/shuffle-word-5575-count.jsonl\n",
      "Generated JSONL file with - 4800 max words, 100 samples - at ./dataset/gen-word-4800-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5825 max words - at ./dataset/shuffle-word-5825-count.jsonl\n",
      "Generated JSONL file with - 4525 max words, 100 samples - at ./dataset/gen-word-4525-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5850 max words - at ./dataset/shuffle-word-5850-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5750 max words - at ./dataset/shuffle-word-5750-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5700 max words - at ./dataset/shuffle-word-5700-count.jsonl\n",
      "Generated JSONL file with - 4625 max words, 100 samples - at ./dataset/gen-word-4625-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5450 max words - at ./dataset/shuffle-word-5450-count.jsonl\n",
      "Generated JSONL file with - 4850 max words, 100 samples - at ./dataset/gen-word-4850-count.jsonl\n",
      "Generated JSONL file with - 4875 max words, 100 samples - at ./dataset/gen-word-4875-count.jsonl\n",
      "Generated JSONL file with - 4700 max words, 100 samples - at ./dataset/gen-word-4700-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5650 max words - at ./dataset/shuffle-word-5650-count.jsonl\n",
      "Generated JSONL file with - 4900 max words, 100 samples - at ./dataset/gen-word-4900-count.jsonl\n",
      "Generated JSONL file with - 4725 max words, 100 samples - at ./dataset/gen-word-4725-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5725 max words - at ./dataset/shuffle-word-5725-count.jsonl\n",
      "Generated JSONL file with - 4975 max words, 100 samples - at ./dataset/gen-word-4975-count.jsonl\n",
      "Generated JSONL file with - 4925 max words, 100 samples - at ./dataset/gen-word-4925-count.jsonl\n",
      "Generated JSONL file with - 5250 max words, 100 samples - at ./dataset/gen-word-5250-count.jsonl\n",
      "Generated JSONL file with - 5275 max words, 100 samples - at ./dataset/gen-word-5275-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5775 max words - at ./dataset/shuffle-word-5775-count.jsonl\n",
      "Generated JSONL file with - 5900 max words, 100 samples - at ./dataset/gen-word-5900-count.jsonl\n",
      "Generated JSONL file with - 4950 max words, 100 samples - at ./dataset/gen-word-4950-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5875 max words - at ./dataset/shuffle-word-5875-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5900 max words - at ./dataset/shuffle-word-5900-count.jsonl\n",
      "Generated JSONL file with - 5650 max words, 100 samples - at ./dataset/gen-word-5650-count.jsonl\n",
      "Generated JSONL file with - 5000 max words, 100 samples - at ./dataset/gen-word-5000-count.jsonl\n",
      "Generated JSONL file with - 5025 max words, 100 samples - at ./dataset/gen-word-5025-count.jsonl\n",
      "Generated JSONL file with - 5150 max words, 100 samples - at ./dataset/gen-word-5150-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5600 max words - at ./dataset/shuffle-word-5600-count.jsonl\n",
      "Generated JSONL file with - 5225 max words, 100 samples - at ./dataset/gen-word-5225-count.jsonl\n",
      "Generated JSONL file with - 5200 max words, 100 samples - at ./dataset/gen-word-5200-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5800 max words - at ./dataset/shuffle-word-5800-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5675 max words - at ./dataset/shuffle-word-5675-count.jsonl\n",
      "Generated JSONL file with - 5100 max words, 100 samples - at ./dataset/gen-word-5100-count.jsonl\n",
      "Generated JSONL file with - 5125 max words, 100 samples - at ./dataset/gen-word-5125-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5500 max words - at ./dataset/shuffle-word-5500-count.jsonl\n",
      "Generated JSONL file with - 5175 max words, 100 samples - at ./dataset/gen-word-5175-count.jsonl\n",
      "Generated JSONL file with - 5050 max words, 100 samples - at ./dataset/gen-word-5050-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5100 max words - at ./dataset/shuffle-word-5100-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5025 max words - at ./dataset/shuffle-word-5025-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 6000 max words - at ./dataset/shuffle-word-6000-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5975 max words - at ./dataset/shuffle-word-5975-count.jsonl\n",
      "Generated a single JSONL file with 100 samples (100 token repeat) - 5950 max words - at ./dataset/shuffle-word-5950-count.jsonl\n",
      "Generated JSONL file with - 5300 max words, 100 samples - at ./dataset/gen-word-5300-count.jsonl\n",
      "Generated JSONL file with - 5450 max words, 100 samples - at ./dataset/gen-word-5450-count.jsonl\n",
      "Generated JSONL file with - 5400 max words, 100 samples - at ./dataset/gen-word-5400-count.jsonl\n",
      "Generated JSONL file with - 5500 max words, 100 samples - at ./dataset/gen-word-5500-count.jsonl\n",
      "Generated JSONL file with - 5525 max words, 100 samples - at ./dataset/gen-word-5525-count.jsonl\n",
      "Generated JSONL file with - 5775 max words, 100 samples - at ./dataset/gen-word-5775-count.jsonl\n",
      "Generated JSONL file with - 5825 max words, 100 samples - at ./dataset/gen-word-5825-count.jsonl\n",
      "Generated JSONL file with - 5550 max words, 100 samples - at ./dataset/gen-word-5550-count.jsonl\n",
      "Generated JSONL file with - 5850 max words, 100 samples - at ./dataset/gen-word-5850-count.jsonl\n",
      "Generated JSONL file with - 5375 max words, 100 samples - at ./dataset/gen-word-5375-count.jsonl\n",
      "Generated JSONL file with - 5750 max words, 100 samples - at ./dataset/gen-word-5750-count.jsonl\n",
      "Generated JSONL file with - 5725 max words, 100 samples - at ./dataset/gen-word-5725-count.jsonl\n",
      "Generated JSONL file with - 5800 max words, 100 samples - at ./dataset/gen-word-5800-count.jsonl\n",
      "Generated JSONL file with - 5075 max words, 100 samples - at ./dataset/gen-word-5075-count.jsonl\n",
      "Generated JSONL file with - 5625 max words, 100 samples - at ./dataset/gen-word-5625-count.jsonl\n",
      "Generated JSONL file with - 5700 max words, 100 samples - at ./dataset/gen-word-5700-count.jsonl\n",
      "Generated JSONL file with - 5875 max words, 100 samples - at ./dataset/gen-word-5875-count.jsonl\n",
      "Generated JSONL file with - 5675 max words, 100 samples - at ./dataset/gen-word-5675-count.jsonl\n",
      "Generated JSONL file with - 5975 max words, 100 samples - at ./dataset/gen-word-5975-count.jsonl\n",
      "Generated JSONL file with - 5600 max words, 100 samples - at ./dataset/gen-word-5600-count.jsonl\n",
      "Generated JSONL file with - 5575 max words, 100 samples - at ./dataset/gen-word-5575-count.jsonl\n",
      "Generated JSONL file with - 5925 max words, 100 samples - at ./dataset/gen-word-5925-count.jsonl\n",
      "Generated JSONL file with - 5950 max words, 100 samples - at ./dataset/gen-word-5950-count.jsonl\n",
      "## Done ##\n",
      "total 1.8G\n",
      "drwxr-xr-x 2 root root   40K Jan 23 22:18 .\n",
      "drwxr-xr-x 5 root root  4.0K Jan 23 22:16 ..\n",
      "-rw-r--r-- 1 root root   20K Jan 23 22:18 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  106K Jan 23 22:18 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  736K Jan 23 22:18 gen-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  738K Jan 23 22:18 gen-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root  752K Jan 23 22:18 gen-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root  750K Jan 23 22:18 gen-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root  759K Jan 23 22:18 gen-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root  778K Jan 23 22:18 gen-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root  779K Jan 23 22:18 gen-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root  786K Jan 23 22:18 gen-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root  790K Jan 23 22:18 gen-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root  798K Jan 23 22:18 gen-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root   86K Jan 23 22:18 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  805K Jan 23 22:18 gen-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  814K Jan 23 22:18 gen-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root  822K Jan 23 22:18 gen-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root  841K Jan 23 22:18 gen-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root  838K Jan 23 22:18 gen-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root  841K Jan 23 22:18 gen-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root  848K Jan 23 22:18 gen-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root  859K Jan 23 22:18 gen-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root  865K Jan 23 22:18 gen-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root  872K Jan 23 22:18 gen-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root   95K Jan 23 22:18 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  879K Jan 23 22:18 gen-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  893K Jan 23 22:18 gen-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root  904K Jan 23 22:18 gen-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root  893K Jan 23 22:18 gen-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root  912K Jan 23 22:18 gen-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root  918K Jan 23 22:18 gen-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root  935K Jan 23 22:18 gen-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root  938K Jan 23 22:18 gen-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root  946K Jan 23 22:18 gen-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root  952K Jan 23 22:18 gen-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root  103K Jan 23 22:18 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  956K Jan 23 22:18 gen-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  965K Jan 23 22:18 gen-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root  968K Jan 23 22:18 gen-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root  964K Jan 23 22:18 gen-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root  980K Jan 23 22:18 gen-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root  990K Jan 23 22:18 gen-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root  990K Jan 23 22:18 gen-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root 1005K Jan 23 22:18 gen-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root 1017K Jan 23 22:18 gen-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root 1017K Jan 23 22:18 gen-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root  110K Jan 23 22:18 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root   25K Jan 23 22:18 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  115K Jan 23 22:18 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1510-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1520-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1530-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.1M Jan 23 22:18 gen-word-1540-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1550-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1560-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1570-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1580-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1590-count.jsonl\n",
      "-rw-r--r-- 1 root root  125K Jan 23 22:18 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1610-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1620-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1630-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1640-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1650-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1660-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.2M Jan 23 22:18 gen-word-1670-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1680-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1690-count.jsonl\n",
      "-rw-r--r-- 1 root root  133K Jan 23 22:18 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1710-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1720-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1730-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1740-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1750-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1760-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1770-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1780-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1790-count.jsonl\n",
      "-rw-r--r-- 1 root root  145K Jan 23 22:18 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.3M Jan 23 22:18 gen-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1810-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1820-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1830-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1840-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1850-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1860-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1870-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1880-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1890-count.jsonl\n",
      "-rw-r--r-- 1 root root  143K Jan 23 22:18 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1910-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1920-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1930-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1940-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.4M Jan 23 22:18 gen-word-1950-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-1960-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-1970-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-1980-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-1990-count.jsonl\n",
      "-rw-r--r-- 1 root root   30K Jan 23 22:18 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  153K Jan 23 22:18 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2010-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2020-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2030-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2040-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2050-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2060-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2070-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2080-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2090-count.jsonl\n",
      "-rw-r--r-- 1 root root  159K Jan 23 22:18 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.5M Jan 23 22:18 gen-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2110-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2120-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2130-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2140-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2150-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2160-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2170-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2180-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2190-count.jsonl\n",
      "-rw-r--r-- 1 root root  170K Jan 23 22:18 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2210-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2220-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.6M Jan 23 22:18 gen-word-2230-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2240-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2250-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2260-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2270-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2280-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2290-count.jsonl\n",
      "-rw-r--r-- 1 root root  180K Jan 23 22:18 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2310-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2320-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2330-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2340-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2350-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2360-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2370-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.7M Jan 23 22:18 gen-word-2380-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2390-count.jsonl\n",
      "-rw-r--r-- 1 root root  184K Jan 23 22:18 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2410-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2420-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2430-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2440-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2450-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2460-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2470-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2480-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2490-count.jsonl\n",
      "-rw-r--r-- 1 root root   34K Jan 23 22:18 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  191K Jan 23 22:18 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2510-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.8M Jan 23 22:18 gen-word-2520-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2530-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2540-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2550-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2560-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2570-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2580-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2590-count.jsonl\n",
      "-rw-r--r-- 1 root root  199K Jan 23 22:18 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2610-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2620-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2630-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2640-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2660-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 gen-word-2670-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2680-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2690-count.jsonl\n",
      "-rw-r--r-- 1 root root  206K Jan 23 22:18 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2710-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2720-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2730-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2740-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2760-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2770-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2780-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2790-count.jsonl\n",
      "-rw-r--r-- 1 root root  209K Jan 23 22:18 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2810-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2820-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 gen-word-2830-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2840-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2860-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2870-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2880-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2890-count.jsonl\n",
      "-rw-r--r-- 1 root root  220K Jan 23 22:18 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2910-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2920-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2930-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 gen-word-2940-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-2950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-2960-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-2970-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-2980-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-2990-count.jsonl\n",
      "-rw-r--r-- 1 root root   39K Jan 23 22:18 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  221K Jan 23 22:18 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.2M Jan 23 22:18 gen-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jan 23 22:18 gen-word-3025-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 gen-word-3050-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 gen-word-3075-count.jsonl\n",
      "-rw-r--r-- 1 root root  236K Jan 23 22:18 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 gen-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 gen-word-3125-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3150-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3175-count.jsonl\n",
      "-rw-r--r-- 1 root root  240K Jan 23 22:18 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3225-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3250-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 gen-word-3275-count.jsonl\n",
      "-rw-r--r-- 1 root root  252K Jan 23 22:18 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.2M Jan 23 22:18 gen-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.2M Jan 23 22:18 gen-word-3325-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 gen-word-3350-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 gen-word-3375-count.jsonl\n",
      "-rw-r--r-- 1 root root  258K Jan 23 22:18 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 gen-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 gen-word-3425-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 gen-word-3450-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 gen-word-3475-count.jsonl\n",
      "-rw-r--r-- 1 root root   43K Jan 23 22:18 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  267K Jan 23 22:18 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 gen-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 gen-word-3525-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 gen-word-3550-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 gen-word-3575-count.jsonl\n",
      "-rw-r--r-- 1 root root  269K Jan 23 22:18 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Jan 23 22:18 gen-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Jan 23 22:18 gen-word-3625-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Jan 23 22:18 gen-word-3650-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.5M Jan 23 22:18 gen-word-3675-count.jsonl\n",
      "-rw-r--r-- 1 root root  277K Jan 23 22:18 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Jan 23 22:18 gen-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Jan 23 22:18 gen-word-3725-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Jan 23 22:18 gen-word-3750-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Jan 23 22:18 gen-word-3775-count.jsonl\n",
      "-rw-r--r-- 1 root root  285K Jan 23 22:18 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Jan 23 22:18 gen-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Jan 23 22:18 gen-word-3825-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Jan 23 22:18 gen-word-3850-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.7M Jan 23 22:18 gen-word-3875-count.jsonl\n",
      "-rw-r--r-- 1 root root  296K Jan 23 22:18 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jan 23 22:18 gen-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jan 23 22:18 gen-word-3925-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jan 23 22:18 gen-word-3950-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jan 23 22:18 gen-word-3975-count.jsonl\n",
      "-rw-r--r-- 1 root root   49K Jan 23 22:18 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  298K Jan 23 22:18 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Jan 23 22:18 gen-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Jan 23 22:18 gen-word-4025-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Jan 23 22:18 gen-word-4050-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Jan 23 22:18 gen-word-4075-count.jsonl\n",
      "-rw-r--r-- 1 root root  308K Jan 23 22:18 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.9M Jan 23 22:18 gen-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Jan 23 22:18 gen-word-4125-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Jan 23 22:18 gen-word-4150-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Jan 23 22:18 gen-word-4175-count.jsonl\n",
      "-rw-r--r-- 1 root root  314K Jan 23 22:18 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.0M Jan 23 22:18 gen-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Jan 23 22:18 gen-word-4225-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Jan 23 22:18 gen-word-4250-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Jan 23 22:18 gen-word-4275-count.jsonl\n",
      "-rw-r--r-- 1 root root  319K Jan 23 22:18 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.1M Jan 23 22:18 gen-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.2M Jan 23 22:18 gen-word-4325-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.2M Jan 23 22:18 gen-word-4350-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.2M Jan 23 22:18 gen-word-4375-count.jsonl\n",
      "-rw-r--r-- 1 root root  328K Jan 23 22:18 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jan 23 22:18 gen-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jan 23 22:18 gen-word-4425-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jan 23 22:18 gen-word-4450-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jan 23 22:18 gen-word-4475-count.jsonl\n",
      "-rw-r--r-- 1 root root   55K Jan 23 22:18 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  336K Jan 23 22:18 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.3M Jan 23 22:18 gen-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Jan 23 22:18 gen-word-4525-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Jan 23 22:18 gen-word-4550-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Jan 23 22:18 gen-word-4575-count.jsonl\n",
      "-rw-r--r-- 1 root root  341K Jan 23 22:18 gen-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Jan 23 22:18 gen-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.4M Jan 23 22:18 gen-word-4625-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.5M Jan 23 22:18 gen-word-4650-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.5M Jan 23 22:18 gen-word-4675-count.jsonl\n",
      "-rw-r--r-- 1 root root  351K Jan 23 22:18 gen-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.5M Jan 23 22:18 gen-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Jan 23 22:18 gen-word-4725-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Jan 23 22:18 gen-word-4750-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Jan 23 22:18 gen-word-4775-count.jsonl\n",
      "-rw-r--r-- 1 root root  358K Jan 23 22:18 gen-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Jan 23 22:18 gen-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.6M Jan 23 22:18 gen-word-4825-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.7M Jan 23 22:18 gen-word-4850-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.7M Jan 23 22:18 gen-word-4875-count.jsonl\n",
      "-rw-r--r-- 1 root root  362K Jan 23 22:18 gen-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.7M Jan 23 22:18 gen-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.7M Jan 23 22:18 gen-word-4925-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Jan 23 22:18 gen-word-4950-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Jan 23 22:18 gen-word-4975-count.jsonl\n",
      "-rw-r--r-- 1 root root   15K Jan 23 22:18 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root   58K Jan 23 22:18 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  370K Jan 23 22:18 gen-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Jan 23 22:18 gen-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.8M Jan 23 22:18 gen-word-5025-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.9M Jan 23 22:18 gen-word-5050-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.9M Jan 23 22:18 gen-word-5075-count.jsonl\n",
      "-rw-r--r-- 1 root root  388K Jan 23 22:18 gen-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.9M Jan 23 22:18 gen-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.9M Jan 23 22:18 gen-word-5125-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jan 23 22:18 gen-word-5150-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jan 23 22:18 gen-word-5175-count.jsonl\n",
      "-rw-r--r-- 1 root root  386K Jan 23 22:18 gen-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jan 23 22:18 gen-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jan 23 22:18 gen-word-5225-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Jan 23 22:18 gen-word-5250-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Jan 23 22:18 gen-word-5275-count.jsonl\n",
      "-rw-r--r-- 1 root root  386K Jan 23 22:18 gen-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Jan 23 22:18 gen-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Jan 23 22:18 gen-word-5325-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.1M Jan 23 22:18 gen-word-5350-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Jan 23 22:18 gen-word-5375-count.jsonl\n",
      "-rw-r--r-- 1 root root  404K Jan 23 22:18 gen-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Jan 23 22:18 gen-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Jan 23 22:18 gen-word-5425-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.2M Jan 23 22:18 gen-word-5450-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Jan 23 22:18 gen-word-5475-count.jsonl\n",
      "-rw-r--r-- 1 root root   63K Jan 23 22:18 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  411K Jan 23 22:18 gen-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Jan 23 22:18 gen-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Jan 23 22:18 gen-word-5525-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.3M Jan 23 22:18 gen-word-5550-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Jan 23 22:18 gen-word-5575-count.jsonl\n",
      "-rw-r--r-- 1 root root  415K Jan 23 22:18 gen-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Jan 23 22:18 gen-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Jan 23 22:18 gen-word-5625-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Jan 23 22:18 gen-word-5650-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.4M Jan 23 22:18 gen-word-5675-count.jsonl\n",
      "-rw-r--r-- 1 root root  421K Jan 23 22:18 gen-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Jan 23 22:18 gen-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Jan 23 22:18 gen-word-5725-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Jan 23 22:18 gen-word-5750-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.5M Jan 23 22:18 gen-word-5775-count.jsonl\n",
      "-rw-r--r-- 1 root root  435K Jan 23 22:18 gen-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Jan 23 22:18 gen-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Jan 23 22:18 gen-word-5825-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Jan 23 22:18 gen-word-5850-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Jan 23 22:18 gen-word-5875-count.jsonl\n",
      "-rw-r--r-- 1 root root  431K Jan 23 22:18 gen-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.6M Jan 23 22:18 gen-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.7M Jan 23 22:18 gen-word-5925-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.7M Jan 23 22:18 gen-word-5950-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.7M Jan 23 22:18 gen-word-5975-count.jsonl\n",
      "-rw-r--r-- 1 root root   71K Jan 23 22:18 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  444K Jan 23 22:18 gen-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.8M Jan 23 22:18 gen-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root  458K Jan 23 22:18 gen-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root  468K Jan 23 22:18 gen-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root  466K Jan 23 22:18 gen-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root  471K Jan 23 22:18 gen-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root   76K Jan 23 22:18 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  479K Jan 23 22:18 gen-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root  486K Jan 23 22:18 gen-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root  495K Jan 23 22:18 gen-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root  508K Jan 23 22:18 gen-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root  499K Jan 23 22:18 gen-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root   80K Jan 23 22:18 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  521K Jan 23 22:18 gen-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root  529K Jan 23 22:18 gen-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root  533K Jan 23 22:18 gen-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root  543K Jan 23 22:18 gen-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root  548K Jan 23 22:18 gen-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root   84K Jan 23 22:18 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  553K Jan 23 22:18 gen-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root  561K Jan 23 22:18 gen-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root  568K Jan 23 22:18 gen-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root  577K Jan 23 22:18 gen-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root  583K Jan 23 22:18 gen-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root   88K Jan 23 22:18 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  595K Jan 23 22:18 gen-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root  599K Jan 23 22:18 gen-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  612K Jan 23 22:18 gen-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  610K Jan 23 22:18 gen-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  618K Jan 23 22:18 gen-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root   93K Jan 23 22:18 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  623K Jan 23 22:18 gen-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  639K Jan 23 22:18 gen-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  639K Jan 23 22:18 gen-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  642K Jan 23 22:18 gen-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  668K Jan 23 22:18 gen-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root   98K Jan 23 22:18 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  662K Jan 23 22:18 gen-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  667K Jan 23 22:18 gen-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  672K Jan 23 22:18 gen-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  677K Jan 23 22:18 gen-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  700K Jan 23 22:18 gen-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  101K Jan 23 22:18 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  695K Jan 23 22:18 gen-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  707K Jan 23 22:18 gen-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  719K Jan 23 22:18 gen-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  719K Jan 23 22:18 gen-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  733K Jan 23 22:18 gen-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root  5.0M Jan 23 22:18 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Jan 23 22:18 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1010-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1020-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1030-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1040-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1060-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1070-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1080-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1090-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1110-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1160-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1170-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1180-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1190-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1210-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1220-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1230-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1240-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1260-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1270-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-1280-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1290-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.1M Jan 23 22:18 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1310-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1320-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-1330-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1340-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1360-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1370-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1380-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1390-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1410-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1420-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1430-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1440-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1460-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1470-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1480-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1490-count.jsonl\n",
      "-rw-r--r-- 1 root root  4.2M Jan 23 22:18 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1500-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1510-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1520-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1530-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1540-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1550-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1560-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1570-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1580-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1590-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1600-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-1610-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1620-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1630-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1640-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-1650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1660-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1670-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1680-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1690-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1710-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1720-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1730-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1740-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1760-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1770-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1780-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1790-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1810-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1820-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1830-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1840-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1860-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1870-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1880-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1890-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1910-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1920-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1930-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1940-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1960-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1970-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1980-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-1990-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.8M Jan 23 22:18 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2010-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2020-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2030-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2040-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2060-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2070-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2080-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2090-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2110-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2120-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2130-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2140-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2150-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2160-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2170-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2180-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2190-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2210-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2220-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2230-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2240-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2260-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2270-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2280-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2290-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2310-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2320-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2330-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2340-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2350-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2360-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2370-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2380-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2390-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2400-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2410-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2420-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2430-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2440-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2450-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2460-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2470-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2480-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2490-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.6M Jan 23 22:18 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-2500-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2510-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2520-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2530-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2540-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2550-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2560-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2570-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2580-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2590-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2600-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2610-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2620-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2630-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2640-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2650-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2660-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2670-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2680-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2690-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2700-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2710-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2720-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2730-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2740-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2750-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2760-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2770-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2780-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2790-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2800-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2810-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2820-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2830-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2840-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2850-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2860-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2870-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2880-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2890-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2900-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2910-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2920-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2930-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2940-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2950-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2960-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2970-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2980-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-2990-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.4M Jan 23 22:18 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root  1.9M Jan 23 22:18 shuffle-word-3000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3025-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3075-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3175-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3225-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3275-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3325-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3375-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3425-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3475-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.3M Jan 23 22:18 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3500-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3525-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3550-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3575-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3600-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3625-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3675-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3725-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3775-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3825-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3875-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3925-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-3975-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4025-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4075-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4175-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4225-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4275-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4325-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4375-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4425-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4475-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4500-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4525-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4550-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4575-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-460-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4600-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4625-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4675-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-470-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4725-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4775-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-480-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4825-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4875-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-490-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4925-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-4975-count.jsonl\n",
      "-rw-r--r-- 1 root root  7.9M Jan 23 22:18 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.1M Jan 23 22:18 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-500-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5025-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5050-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5075-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-510-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5100-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5125-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5150-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5175-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-520-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5200-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5225-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5250-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5275-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-530-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5300-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5325-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5350-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5375-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-540-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5400-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5425-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5450-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5475-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-550-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5500-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5525-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5550-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5575-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-560-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5600-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5625-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5675-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-570-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5725-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5775-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-580-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5825-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5875-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-590-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5925-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-5975-count.jsonl\n",
      "-rw-r--r-- 1 root root  3.0M Jan 23 22:18 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-600-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.5M Jan 23 22:18 shuffle-word-6000-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-610-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-620-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-630-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-640-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jan 23 22:18 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-650-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-660-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-670-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-680-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-690-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jan 23 22:18 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-700-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-710-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-720-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-730-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-740-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jan 23 22:18 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-750-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-760-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-770-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-780-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-790-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.9M Jan 23 22:18 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-800-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-810-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-820-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-830-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-840-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Jan 23 22:18 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-850-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-860-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-870-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-880-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-890-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Jan 23 22:18 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-900-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-910-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-920-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-930-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-940-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.8M Jan 23 22:18 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-950-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-960-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-970-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-980-count.jsonl\n",
      "-rw-r--r-- 1 root root  2.0M Jan 23 22:18 shuffle-word-990-count.jsonl\n",
      "-rw-r--r-- 1 root root   12K Jan 23 22:18 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root   15K Jan 23 22:18 word-4-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# Training set for <= 100 words\n",
    "# This is used to fill up as much blanks as possible\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 100 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-4-count.jsonl 4 100 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 100+ - 3000 words dataset\n",
    "# \n",
    "for i in {110..3000..10} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 75 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 75 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 3000+ - 400 words dataset\n",
    "# \n",
    "for i in {3025..6000..25} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 100 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 100 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da287711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|| 862/862 [00:00<00:00, 107543.06it/s]\n",
      "Saving the dataset (4/4 shards): 100%|| 36851/36851 [00:01<00:00, 19869.65 exam\n",
      "Saving the dataset (1/1 shards): 100%|| 1547/1547 [00:00<00:00, 30397.64 exampl\n"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "# and pack the data into 8k of length\n",
    "#\n",
    "# For the initial training, it seems to be better to do 4k chunks, batch size 16, with 8k datapacks\n",
    "# Then to do 8k chunks, batchsize 8, with 16k datapacks. Why? I dun know.\n",
    "#\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/stage-2-tune.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/stage-2-memory-finetune/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1bf84",
   "metadata": {},
   "source": [
    "## Finetune 2 (2x2k -> 2x4k) : The actual tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c6af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-23 22:32:27,860] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test/stage-2-tune.yaml', '--model.load_model=../model/Memory-Tune-Stage-1-RWKV-v5-7B-world.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/stage-2-memory-finetune/RWKV-v5-7B-world.pth/', '--trainer.logger.init_args.name=[8xA100] RWKV-v5-7B-World - Mem-Finetune-2 (bs=256, train-ctx=8192, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto', '--trainer.microbatch_size=4', '--model.ctx_len=8192'], args=['fit', '-c', '/workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test/stage-2-tune.yaml', '--model.load_model=../model/Memory-Tune-Stage-1-RWKV-v5-7B-world.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/stage-2-memory-finetune/RWKV-v5-7B-world.pth/', '--trainer.logger.init_args.name=[8xA100] RWKV-v5-7B-World - Mem-Finetune-2 (bs=256, train-ctx=8192, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto', '--trainer.microbatch_size=4', '--model.ctx_len=8192'].\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 1430867974\n",
      "Seed set to 1430867974\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:558: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "[rank: 0] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-01-23 22:33:36,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,802] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-23 22:33:36,803] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[rank: 3] Seed set to 1430867974\n",
      "[rank: 5] Seed set to 1430867974\n",
      "[rank: 2] Seed set to 1430867974\n",
      "[rank: 6] Seed set to 1430867974\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 1430867974\n",
      "[rank: 1] Seed set to 1430867974\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 1430867974\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[rank: 2] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[rank: 5] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[rank: 1] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 3] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[rank: 4] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[rank: 7] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[rank: 6] Seed set to 1430867974\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240123_223508-bdvkilfd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m[8xA100] RWKV-v5-7B-World - Mem-Finetune-2 (bs=256, train-ctx=8192, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-Memory-Experiment/runs/bdvkilfd\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  2.000e-04 (0.0002)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.016431331634521484 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10186290740966797 seconds\n",
      "Time to load fused_adam op: 0.10216116905212402 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10233497619628906 seconds\n",
      "Time to load fused_adam op: 0.10216617584228516 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10257244110107422 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.1023705005645752 seconds\n",
      "Time to load fused_adam op: 0.10263657569885254 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 268 M \n",
      "1 | blocks | ModuleList | 7.0 B \n",
      "2 | ln_out | LayerNorm  | 8.2 K \n",
      "3 | head   | Linear     | 268 M \n",
      "--------------------------------------\n",
      "7.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 B     Total params\n",
      "30,072.177Total estimated model params size (MB)\n",
      "Epoch 0:  17%|| 200/1152 [43:43<3:28:08,  0.08it/s, v_num=ilfd, train/loss=0.00/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|| 1152/1152 [3:52:43<00:00,  0.08it/s, v_num=ilfd, train/loss=0.7\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/194 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/194 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/194 [00:00<00:59,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                 | 2/194 [00:00<00:48,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|                 | 3/194 [00:00<00:44,  4.29it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|                 | 4/194 [00:00<00:44,  4.24it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|                 | 5/194 [00:01<00:42,  4.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|                 | 6/194 [00:01<00:41,  4.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|                 | 7/194 [00:01<00:42,  4.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|                 | 8/194 [00:01<00:41,  4.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|                 | 9/194 [00:01<00:40,  4.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|                | 10/194 [00:02<00:40,  4.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|                | 11/194 [00:02<00:39,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|                | 12/194 [00:02<00:42,  4.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|               | 13/194 [00:02<00:41,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|               | 14/194 [00:03<00:40,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|               | 15/194 [00:03<00:40,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|               | 16/194 [00:03<00:39,  4.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|               | 17/194 [00:03<00:39,  4.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|               | 18/194 [00:04<00:43,  4.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|               | 19/194 [00:04<00:45,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|               | 20/194 [00:05<00:44,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|               | 21/194 [00:05<00:44,  3.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|               | 22/194 [00:05<00:43,  3.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|               | 23/194 [00:05<00:43,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|               | 24/194 [00:06<00:43,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|              | 25/194 [00:06<00:42,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|              | 26/194 [00:06<00:44,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|              | 27/194 [00:07<00:43,  3.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|              | 28/194 [00:07<00:43,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|              | 29/194 [00:07<00:42,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|              | 30/194 [00:07<00:42,  3.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|              | 31/194 [00:08<00:46,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|              | 32/194 [00:09<00:46,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|              | 33/194 [00:09<00:45,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|              | 34/194 [00:09<00:46,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|              | 35/194 [00:10<00:45,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|             | 36/194 [00:10<00:45,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|             | 37/194 [00:10<00:44,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|             | 38/194 [00:11<00:46,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|             | 39/194 [00:11<00:46,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|             | 40/194 [00:12<00:46,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|             | 41/194 [00:12<00:47,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|             | 42/194 [00:12<00:46,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|             | 43/194 [00:13<00:46,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|             | 44/194 [00:13<00:45,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|             | 45/194 [00:13<00:45,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|             | 46/194 [00:13<00:44,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|             | 47/194 [00:14<00:44,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|            | 48/194 [00:14<00:43,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|            | 49/194 [00:14<00:43,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|            | 50/194 [00:14<00:42,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|            | 51/194 [00:15<00:43,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|            | 52/194 [00:15<00:43,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|            | 53/194 [00:15<00:42,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|            | 54/194 [00:16<00:41,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|            | 55/194 [00:16<00:41,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|            | 56/194 [00:16<00:40,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|            | 57/194 [00:16<00:40,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|            | 58/194 [00:16<00:39,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|           | 59/194 [00:17<00:39,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|           | 60/194 [00:17<00:38,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|           | 61/194 [00:17<00:38,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|           | 62/194 [00:17<00:37,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|           | 63/194 [00:17<00:37,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|           | 64/194 [00:18<00:37,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|           | 65/194 [00:18<00:36,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|           | 66/194 [00:18<00:36,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|           | 67/194 [00:19<00:36,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|           | 68/194 [00:19<00:35,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|           | 69/194 [00:19<00:35,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|          | 70/194 [00:19<00:34,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|          | 71/194 [00:19<00:34,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|          | 72/194 [00:20<00:34,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|          | 73/194 [00:20<00:34,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|          | 74/194 [00:21<00:34,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|          | 75/194 [00:21<00:33,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|          | 76/194 [00:21<00:33,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|          | 77/194 [00:22<00:33,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|          | 78/194 [00:22<00:33,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|          | 79/194 [00:22<00:33,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|          | 80/194 [00:22<00:32,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|          | 81/194 [00:23<00:32,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|         | 82/194 [00:23<00:32,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|         | 83/194 [00:23<00:31,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|         | 84/194 [00:23<00:31,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|         | 85/194 [00:24<00:30,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|         | 86/194 [00:24<00:30,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|         | 87/194 [00:24<00:30,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|         | 88/194 [00:24<00:29,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|         | 89/194 [00:24<00:29,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|         | 90/194 [00:25<00:29,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|         | 91/194 [00:26<00:29,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|         | 92/194 [00:26<00:29,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|        | 93/194 [00:26<00:28,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|        | 94/194 [00:26<00:28,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|        | 95/194 [00:26<00:28,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|        | 96/194 [00:27<00:27,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|        | 97/194 [00:27<00:27,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|        | 98/194 [00:28<00:27,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|        | 99/194 [00:28<00:27,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|       | 100/194 [00:28<00:26,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|       | 101/194 [00:28<00:26,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|       | 102/194 [00:28<00:26,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|       | 103/194 [00:29<00:25,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|       | 104/194 [00:29<00:25,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|       | 105/194 [00:29<00:24,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|       | 106/194 [00:30<00:25,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|       | 107/194 [00:30<00:24,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|       | 108/194 [00:30<00:24,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|       | 109/194 [00:31<00:24,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|       | 110/194 [00:31<00:23,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|      | 111/194 [00:31<00:23,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|      | 112/194 [00:31<00:23,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|      | 113/194 [00:31<00:22,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|      | 114/194 [00:32<00:22,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|      | 115/194 [00:32<00:22,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|      | 116/194 [00:33<00:22,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|      | 117/194 [00:33<00:22,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|      | 118/194 [00:34<00:22,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|      | 119/194 [00:34<00:21,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|      | 120/194 [00:34<00:21,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|      | 121/194 [00:34<00:21,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|      | 122/194 [00:35<00:20,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|     | 123/194 [00:35<00:20,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|     | 124/194 [00:35<00:20,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|     | 125/194 [00:35<00:19,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|     | 126/194 [00:36<00:19,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|     | 127/194 [00:36<00:19,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|     | 128/194 [00:36<00:19,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|     | 129/194 [00:37<00:18,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|     | 130/194 [00:37<00:18,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|     | 131/194 [00:37<00:18,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|     | 132/194 [00:37<00:17,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|     | 133/194 [00:38<00:17,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|     | 134/194 [00:38<00:17,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|    | 135/194 [00:38<00:16,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|    | 136/194 [00:39<00:16,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|    | 137/194 [00:39<00:16,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|    | 138/194 [00:39<00:15,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|    | 139/194 [00:39<00:15,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|    | 140/194 [00:40<00:15,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|    | 141/194 [00:40<00:15,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|    | 142/194 [00:40<00:14,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|    | 143/194 [00:41<00:14,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|    | 144/194 [00:41<00:14,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|    | 145/194 [00:41<00:14,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|    | 146/194 [00:41<00:13,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|    | 147/194 [00:42<00:13,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|   | 148/194 [00:42<00:13,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|   | 149/194 [00:42<00:12,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|   | 150/194 [00:43<00:12,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|   | 151/194 [00:43<00:12,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|   | 152/194 [00:43<00:12,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|   | 153/194 [00:43<00:11,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|   | 154/194 [00:44<00:11,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|   | 155/194 [00:44<00:11,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|   | 156/194 [00:44<00:10,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|   | 157/194 [00:44<00:10,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|   | 158/194 [00:44<00:10,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|   | 159/194 [00:45<00:09,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|  | 160/194 [00:45<00:09,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|  | 161/194 [00:46<00:09,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|  | 162/194 [00:46<00:09,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|  | 163/194 [00:46<00:08,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|  | 164/194 [00:46<00:08,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|  | 165/194 [00:47<00:08,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|  | 166/194 [00:47<00:08,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|  | 167/194 [00:47<00:07,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|  | 168/194 [00:48<00:07,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|  | 169/194 [00:48<00:07,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|  | 170/194 [00:48<00:06,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 172/194 [00:49<00:06,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 173/194 [00:49<00:05,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 174/194 [00:49<00:05,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 175/194 [00:49<00:05,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 176/194 [00:49<00:05,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 177/194 [00:50<00:04,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%| | 178/194 [00:50<00:04,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%| | 179/194 [00:50<00:04,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%| | 180/194 [00:50<00:03,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%| | 181/194 [00:50<00:03,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%| | 182/194 [00:51<00:03,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%| | 183/194 [00:51<00:03,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 184/194 [00:51<00:02,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 185/194 [00:52<00:02,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 186/194 [00:52<00:02,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 187/194 [00:52<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 188/194 [00:53<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 189/194 [00:53<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 190/194 [00:53<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 191/194 [00:53<00:00,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 192/194 [00:54<00:00,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 193/194 [00:54<00:00,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 194/194 [00:54<00:00,  3.56it/s]\u001b[A\n",
      "Epoch 0: 100%|| 1152/1152 [3:53:54<00:00,  0.08it/s, v_num=ilfd, train/loss=0.7`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|| 1152/1152 [3:53:54<00:00,  0.08it/s, v_num=ilfd, train/loss=0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.028 MB of 0.033 MB uploaded (0.004 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ctx_len \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/tokens \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            validation/loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 1151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 2.34026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 9208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ctx_len 6141.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 0.74609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 0.74609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/tokens 1982.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.00019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            validation/loss 0.03095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33m[8xA100] RWKV-v5-7B-World - Mem-Finetune-2 (bs=256, train-ctx=8192, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-Memory-Experiment/runs/bdvkilfd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-Memory-Experiment/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMjg2NzkwMQ==/version_details/v5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240123_223508-bdvkilfd/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/stage-2-tune.yaml\" \\\n",
    "        --model.load_model=\"../model/Memory-Tune-Stage-1-{MODEL_NAME}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/stage-2-memory-finetune/{MODEL_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=8192, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --trainer.microbatch_size=4 \\\n",
    "        --model.ctx_len=8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af140de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-24 02:30:37,480] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/stage-2-memory-finetune/RWKV-v5-7B-world.pth/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.12.6\n",
      "Reconstructed fp32 state dict with 710 params 7518044160 elements\n",
      "Saving bf16 state dict to ../model/Memory-Tune-Stage-2-RWKV-v5-7B-world.pth\n",
      "-rw-r--r-- 1 root root 15G Jan 24 02:31 ../model/Memory-Tune-Stage-2-RWKV-v5-7B-world.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/stage-2-memory-finetune/{MODEL_NAME}/last.ckpt\" \\\n",
    "        \"../model/Memory-Tune-Stage-2-{MODEL_NAME}\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Memory-Tune-Stage-2-{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ab1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRIPT_DIR:  /workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test/memory_script\n",
      "PROJECT_DIR:  /workspace/RWKV-infctx-trainer\n",
      "MODEL_CODE_DIR:  /workspace/RWKV-infctx-trainer/RWKV-v5\n",
      "[2024-01-24 02:31:46,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "/workspace/RWKV-infctx-trainer/RWKV-v5/src/model.py:1390: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = torch.tensor(\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "## Model validation for 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "## Model validation for 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "## Model validation for 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "## Model validation for 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "## Model validation for 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "## Model validation for 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "## Model validation for 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "## Model validation for 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "## Model validation for 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "## Model validation for 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "## Model validation for 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "## Model validation for 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "## Model validation for 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "## Model validation for 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "## Model validation for 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "## Model validation for 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "## Model validation for 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "## Model validation for 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "## Model validation for 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "## Model validation for 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "## Model validation for 110 tokens : 100.0% similarity, with 110 matched token, and 0 token mismatch\n",
      "## Model validation for 115 tokens : 100.0% similarity, with 115 matched token, and 0 token mismatch\n",
      "## Model validation for 120 tokens : 100.0% similarity, with 120 matched token, and 0 token mismatch\n",
      "## Model validation for 125 tokens : 100.0% similarity, with 125 matched token, and 0 token mismatch\n",
      "## Model validation for 130 tokens : 100.0% similarity, with 130 matched token, and 0 token mismatch\n",
      "## Model validation for 135 tokens : 100.0% similarity, with 135 matched token, and 0 token mismatch\n",
      "## Model validation for 140 tokens : 100.0% similarity, with 140 matched token, and 0 token mismatch\n",
      "## Model validation for 145 tokens : 100.0% similarity, with 145 matched token, and 0 token mismatch\n",
      "## Model validation for 150 tokens : 100.0% similarity, with 150 matched token, and 0 token mismatch\n",
      "## Model validation for 160 tokens : 100.0% similarity, with 160 matched token, and 0 token mismatch\n",
      "## Model validation for 170 tokens : 100.0% similarity, with 170 matched token, and 0 token mismatch\n",
      "## Model validation for 180 tokens : 100.0% similarity, with 180 matched token, and 0 token mismatch\n",
      "## Model validation for 190 tokens : 100.0% similarity, with 190 matched token, and 0 token mismatch\n",
      "## Model validation for 200 tokens : 100.0% similarity, with 200 matched token, and 0 token mismatch\n",
      "## Model validation for 210 tokens : 100.0% similarity, with 210 matched token, and 0 token mismatch\n",
      "## Model validation for 220 tokens : 100.0% similarity, with 220 matched token, and 0 token mismatch\n",
      "## Model validation for 230 tokens : 100.0% similarity, with 230 matched token, and 0 token mismatch\n",
      "## Model validation for 240 tokens : 100.0% similarity, with 240 matched token, and 0 token mismatch\n",
      "## Model validation for 250 tokens : 100.0% similarity, with 250 matched token, and 0 token mismatch\n",
      "## Model validation for 260 tokens : 100.0% similarity, with 260 matched token, and 0 token mismatch\n",
      "## Model validation for 270 tokens : 100.0% similarity, with 270 matched token, and 0 token mismatch\n",
      "## Model validation for 280 tokens : 100.0% similarity, with 280 matched token, and 0 token mismatch\n",
      "## Model validation for 290 tokens : 100.0% similarity, with 290 matched token, and 0 token mismatch\n",
      "## Model validation for 300 tokens : 100.0% similarity, with 300 matched token, and 0 token mismatch\n",
      "## Model validation for 325 tokens : 100.0% similarity, with 325 matched token, and 0 token mismatch\n",
      "## Model validation for 350 tokens : 100.0% similarity, with 350 matched token, and 0 token mismatch\n",
      "## Model validation for 375 tokens : 100.0% similarity, with 375 matched token, and 0 token mismatch\n",
      "## Model validation for 400 tokens : 100.0% similarity, with 400 matched token, and 0 token mismatch\n",
      "## Model validation for 425 tokens : 100.0% similarity, with 425 matched token, and 0 token mismatch\n",
      "## Model validation for 450 tokens : 100.0% similarity, with 450 matched token, and 0 token mismatch\n",
      "## Model validation for 475 tokens : 100.0% similarity, with 475 matched token, and 0 token mismatch\n",
      "## Model validation for 500 tokens : 100.0% similarity, with 500 matched token, and 0 token mismatch\n",
      "## Model validation for 525 tokens : 100.0% similarity, with 525 matched token, and 0 token mismatch\n",
      "## Model validation for 550 tokens : 100.0% similarity, with 550 matched token, and 0 token mismatch\n",
      "## Model validation for 575 tokens : 100.0% similarity, with 575 matched token, and 0 token mismatch\n",
      "## Model validation for 600 tokens : 100.0% similarity, with 600 matched token, and 0 token mismatch\n",
      "## Model validation for 625 tokens : 100.0% similarity, with 625 matched token, and 0 token mismatch\n",
      "## Model validation for 650 tokens : 100.0% similarity, with 650 matched token, and 0 token mismatch\n",
      "## Model validation for 675 tokens : 100.0% similarity, with 675 matched token, and 0 token mismatch\n",
      "## Model validation for 700 tokens : 100.0% similarity, with 700 matched token, and 0 token mismatch\n",
      "## Model validation for 750 tokens : 99.86666666666667% similarity, with 749 matched token, and 1 token mismatch\n",
      "## Model validation for 800 tokens : 99.875% similarity, with 799 matched token, and 1 token mismatch\n",
      "## Model validation for 850 tokens : 99.88235294117646% similarity, with 849 matched token, and 1 token mismatch\n",
      "## Model validation for 900 tokens : 99.77777777777777% similarity, with 898 matched token, and 2 token mismatch\n",
      "## Model validation for 950 tokens : 99.78947368421053% similarity, with 948 matched token, and 2 token mismatch\n",
      "## Model validation for 1000 tokens : 99.5% similarity, with 995 matched token, and 5 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n",
      "SCRIPT_DIR:  /workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test/memory_script\n",
      "PROJECT_DIR:  /workspace/RWKV-infctx-trainer\n",
      "MODEL_CODE_DIR:  /workspace/RWKV-infctx-trainer/RWKV-v5\n",
      "[2024-01-24 02:35:10,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "/workspace/RWKV-infctx-trainer/RWKV-v5/src/model.py:1390: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = torch.tensor(\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 1000 tokens : 99.5% similarity, with 995 matched token, and 5 token mismatch\n",
      "## Model validation for 1050 tokens : 99.61904761904762% similarity, with 1046 matched token, and 4 token mismatch\n",
      "## Model validation for 1100 tokens : 99.63636363636364% similarity, with 1096 matched token, and 4 token mismatch\n",
      "## Model validation for 1150 tokens : 99.47826086956522% similarity, with 1144 matched token, and 6 token mismatch\n",
      "## Model validation for 1200 tokens : 99.58333333333333% similarity, with 1195 matched token, and 5 token mismatch\n",
      "## Model validation for 1250 tokens : 99.44% similarity, with 1243 matched token, and 7 token mismatch\n",
      "## Model validation for 1300 tokens : 99.38461538461539% similarity, with 1292 matched token, and 8 token mismatch\n",
      "## Model validation for 1350 tokens : 99.25925925925925% similarity, with 1340 matched token, and 10 token mismatch\n",
      "## Model validation for 1400 tokens : 99.21428571428571% similarity, with 1389 matched token, and 11 token mismatch\n",
      "## Model validation for 1450 tokens : 99.17241379310346% similarity, with 1438 matched token, and 12 token mismatch\n",
      "## Model validation for 1500 tokens : 99.06666666666666% similarity, with 1486 matched token, and 14 token mismatch\n",
      "## Model validation for 1550 tokens : 99.16129032258064% similarity, with 1537 matched token, and 13 token mismatch\n",
      "## Model validation for 1600 tokens : 98.6875% similarity, with 1579 matched token, and 21 token mismatch\n",
      "## Model validation for 1650 tokens : 98.84848484848486% similarity, with 1631 matched token, and 19 token mismatch\n",
      "## Model validation for 1700 tokens : 98.70588235294117% similarity, with 1678 matched token, and 22 token mismatch\n",
      "## Model validation for 1750 tokens : 98.28571428571429% similarity, with 1720 matched token, and 30 token mismatch\n",
      "## Model validation for 1800 tokens : 98.38888888888889% similarity, with 1771 matched token, and 29 token mismatch\n",
      "## Model validation for 1850 tokens : 98.27027027027026% similarity, with 1818 matched token, and 32 token mismatch\n",
      "## Model validation for 1900 tokens : 98.15789473684211% similarity, with 1865 matched token, and 35 token mismatch\n",
      "## Model validation for 1950 tokens : 97.8974358974359% similarity, with 1909 matched token, and 41 token mismatch\n",
      "## Model validation for 2000 tokens : 97.89999999999999% similarity, with 1958 matched token, and 42 token mismatch\n",
      "## Model validation for 2050 tokens : 97.7560975609756% similarity, with 2004 matched token, and 46 token mismatch\n",
      "## Model validation for 2100 tokens : 97.38095238095238% similarity, with 2045 matched token, and 55 token mismatch\n",
      "## Model validation for 2150 tokens : 97.34883720930233% similarity, with 2093 matched token, and 57 token mismatch\n",
      "## Model validation for 2200 tokens : 97.22727272727273% similarity, with 2139 matched token, and 61 token mismatch\n",
      "## Model validation for 2250 tokens : 96.93333333333334% similarity, with 2181 matched token, and 69 token mismatch\n",
      "## Model validation for 2300 tokens : 96.60869565217392% similarity, with 2222 matched token, and 78 token mismatch\n",
      "## Model validation for 2350 tokens : 96.42553191489361% similarity, with 2266 matched token, and 84 token mismatch\n",
      "## Model validation for 2400 tokens : 96.16666666666667% similarity, with 2308 matched token, and 92 token mismatch\n",
      "## Model validation for 2450 tokens : 95.83673469387755% similarity, with 2348 matched token, and 102 token mismatch\n",
      "## Model validation for 2500 tokens : 95.67999999999999% similarity, with 2392 matched token, and 108 token mismatch\n",
      "## Model validation for 2550 tokens : 95.72549019607844% similarity, with 2441 matched token, and 109 token mismatch\n",
      "## Model validation for 2600 tokens : 95.46153846153847% similarity, with 2482 matched token, and 118 token mismatch\n",
      "## Model validation for 2650 tokens : 95.20754716981132% similarity, with 2523 matched token, and 127 token mismatch\n",
      "## Model validation for 2700 tokens : 94.74074074074073% similarity, with 2558 matched token, and 142 token mismatch\n",
      "## Model validation for 2750 tokens : 94.07272727272728% similarity, with 2587 matched token, and 163 token mismatch\n",
      "## Model validation for 2800 tokens : 93.89285714285714% similarity, with 2629 matched token, and 171 token mismatch\n",
      "## Model validation for 2850 tokens : 93.47368421052632% similarity, with 2664 matched token, and 186 token mismatch\n",
      "## Model validation for 2900 tokens : 93.24137931034483% similarity, with 2704 matched token, and 196 token mismatch\n",
      "## Model validation for 2950 tokens : 92.71186440677967% similarity, with 2735 matched token, and 215 token mismatch\n",
      "## Model validation for 3000 tokens : 92.4% similarity, with 2772 matched token, and 228 token mismatch\n",
      "## Model validation for 3050 tokens : 91.83606557377048% similarity, with 2801 matched token, and 249 token mismatch\n",
      "## Model validation for 3100 tokens : 91.51612903225806% similarity, with 2837 matched token, and 263 token mismatch\n",
      "## Model validation for 3150 tokens : 90.60317460317461% similarity, with 2854 matched token, and 296 token mismatch\n",
      "## Model validation for 3200 tokens : 90.0% similarity, with 2880 matched token, and 320 token mismatch\n",
      "## Model validation for 3250 tokens : 89.35384615384615% similarity, with 2904 matched token, and 346 token mismatch\n",
      "## Model validation for 3300 tokens : 88.48484848484848% similarity, with 2920 matched token, and 380 token mismatch\n",
      "## Model validation for 3350 tokens : 87.5223880597015% similarity, with 2932 matched token, and 418 token mismatch\n",
      "## Model validation for 3400 tokens : 86.70588235294117% similarity, with 2948 matched token, and 452 token mismatch\n",
      "## Model validation for 3450 tokens : 86.17391304347825% similarity, with 2973 matched token, and 477 token mismatch\n",
      "## Model validation for 3500 tokens : 84.65714285714286% similarity, with 2963 matched token, and 537 token mismatch\n",
      "## Model validation for 3550 tokens : 83.6056338028169% similarity, with 2968 matched token, and 582 token mismatch\n",
      "## Model validation for 3600 tokens : 82.72222222222221% similarity, with 2978 matched token, and 622 token mismatch\n",
      "## Model validation for 3650 tokens : 81.45205479452055% similarity, with 2973 matched token, and 677 token mismatch\n",
      "## Model validation for 3700 tokens : 80.13513513513514% similarity, with 2965 matched token, and 735 token mismatch\n",
      "## Model validation for 3750 tokens : 78.93333333333334% similarity, with 2960 matched token, and 790 token mismatch\n",
      "## Model validation for 3800 tokens : 77.6842105263158% similarity, with 2952 matched token, and 848 token mismatch\n",
      "## Model validation for 3850 tokens : 76.18181818181819% similarity, with 2933 matched token, and 917 token mismatch\n",
      "## Model validation for 3900 tokens : 74.7948717948718% similarity, with 2917 matched token, and 983 token mismatch\n",
      "## Model validation for 3950 tokens : 73.44303797468355% similarity, with 2901 matched token, and 1049 token mismatch\n",
      "## Model validation for 4000 tokens : 72.0% similarity, with 2880 matched token, and 1120 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n",
      "SCRIPT_DIR:  /workspace/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/memory-test/memory_script\n",
      "PROJECT_DIR:  /workspace/RWKV-infctx-trainer\n",
      "MODEL_CODE_DIR:  /workspace/RWKV-infctx-trainer/RWKV-v5\n",
      "[2024-01-24 02:43:26,145] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "/workspace/RWKV-infctx-trainer/RWKV-v5/src/model.py:1390: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = torch.tensor(\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 4000 tokens : 72.0% similarity, with 2880 matched token, and 1120 token mismatch\n",
      "## Model validation for 4050 tokens : 70.32098765432099% similarity, with 2848 matched token, and 1202 token mismatch\n",
      "## Model validation for 4100 tokens : 68.73170731707317% similarity, with 2818 matched token, and 1282 token mismatch\n",
      "## Model validation for 4150 tokens : 67.5421686746988% similarity, with 2803 matched token, and 1347 token mismatch\n",
      "## Model validation for 4200 tokens : 65.88095238095238% similarity, with 2767 matched token, and 1433 token mismatch\n",
      "## Model validation for 4250 tokens : 64.68235294117648% similarity, with 2749 matched token, and 1501 token mismatch\n",
      "## Model validation for 4300 tokens : 62.8139534883721% similarity, with 2701 matched token, and 1599 token mismatch\n",
      "## Model validation for 4350 tokens : 61.88505747126437% similarity, with 2692 matched token, and 1658 token mismatch\n",
      "## Model validation for 4400 tokens : 60.56818181818182% similarity, with 2665 matched token, and 1735 token mismatch\n",
      "## Model validation for 4450 tokens : 59.21348314606741% similarity, with 2635 matched token, and 1815 token mismatch\n",
      "## Model validation for 4500 tokens : 57.37777777777778% similarity, with 2582 matched token, and 1918 token mismatch\n",
      "## Model validation for 4550 tokens : 56.35164835164835% similarity, with 2564 matched token, and 1986 token mismatch\n",
      "## Model validation for 4600 tokens : 54.630434782608695% similarity, with 2513 matched token, and 2087 token mismatch\n",
      "## Model validation for 4650 tokens : 53.61290322580645% similarity, with 2493 matched token, and 2157 token mismatch\n",
      "## Model validation for 4700 tokens : 52.12765957446809% similarity, with 2450 matched token, and 2250 token mismatch\n",
      "## Model validation for 4750 tokens : 50.94736842105263% similarity, with 2420 matched token, and 2330 token mismatch\n",
      "## Model validation for 4800 tokens : 48.9375% similarity, with 2349 matched token, and 2451 token mismatch\n",
      "## Model validation for 4850 tokens : 47.52577319587628% similarity, with 2305 matched token, and 2545 token mismatch\n",
      "## Model validation for 4900 tokens : 46.6530612244898% similarity, with 2286 matched token, and 2614 token mismatch\n",
      "## Model validation for 4950 tokens : 45.05050505050505% similarity, with 2230 matched token, and 2720 token mismatch\n",
      "## Model validation for 5000 tokens : 44.14% similarity, with 2207 matched token, and 2793 token mismatch\n",
      "## Model validation for 5050 tokens : 42.99009900990099% similarity, with 2171 matched token, and 2879 token mismatch\n",
      "## Model validation for 5100 tokens : 41.78431372549019% similarity, with 2131 matched token, and 2969 token mismatch\n",
      "## Model validation for 5150 tokens : 40.9126213592233% similarity, with 2107 matched token, and 3043 token mismatch\n",
      "## Model validation for 5200 tokens : 39.48076923076923% similarity, with 2053 matched token, and 3147 token mismatch\n",
      "## Model validation for 5250 tokens : 38.476190476190474% similarity, with 2020 matched token, and 3230 token mismatch\n",
      "## Model validation for 5300 tokens : 37.9811320754717% similarity, with 2013 matched token, and 3287 token mismatch\n",
      "## Model validation for 5350 tokens : 36.69158878504673% similarity, with 1963 matched token, and 3387 token mismatch\n",
      "## Model validation for 5400 tokens : 36.129629629629626% similarity, with 1951 matched token, and 3449 token mismatch\n",
      "## Model validation for 5450 tokens : 34.91743119266055% similarity, with 1903 matched token, and 3547 token mismatch\n",
      "## Model validation for 5500 tokens : 33.945454545454545% similarity, with 1867 matched token, and 3633 token mismatch\n",
      "## Model validation for 5550 tokens : 33.2972972972973% similarity, with 1848 matched token, and 3702 token mismatch\n",
      "## Model validation for 5600 tokens : 32.535714285714285% similarity, with 1822 matched token, and 3778 token mismatch\n",
      "## Model validation for 5650 tokens : 31.46902654867257% similarity, with 1778 matched token, and 3872 token mismatch\n",
      "## Model validation for 5700 tokens : 30.280701754385962% similarity, with 1726 matched token, and 3974 token mismatch\n",
      "## Model validation for 5750 tokens : 29.791304347826085% similarity, with 1713 matched token, and 4037 token mismatch\n",
      "## Model validation for 5800 tokens : 28.913793103448278% similarity, with 1677 matched token, and 4123 token mismatch\n",
      "## Model validation for 5850 tokens : 27.794871794871796% similarity, with 1626 matched token, and 4224 token mismatch\n",
      "## Model validation for 5900 tokens : 27.54237288135593% similarity, with 1625 matched token, and 4275 token mismatch\n",
      "## Model validation for 5950 tokens : 26.571428571428573% similarity, with 1581 matched token, and 4369 token mismatch\n",
      "## Model validation for 6000 tokens : 26.0% similarity, with 1560 matched token, and 4440 token mismatch\n",
      "## Model validation for 6050 tokens : 25.42148760330579% similarity, with 1538 matched token, and 4512 token mismatch\n",
      "## Model validation for 6100 tokens : 24.508196721311474% similarity, with 1495 matched token, and 4605 token mismatch\n",
      "## Model validation for 6150 tokens : 23.902439024390244% similarity, with 1470 matched token, and 4680 token mismatch\n",
      "## Model validation for 6200 tokens : 23.032258064516128% similarity, with 1428 matched token, and 4772 token mismatch\n",
      "## Model validation for 6250 tokens : 22.368% similarity, with 1398 matched token, and 4852 token mismatch\n",
      "## Model validation for 6300 tokens : 21.80952380952381% similarity, with 1374 matched token, and 4926 token mismatch\n",
      "## Model validation for 6350 tokens : 21.590551181102363% similarity, with 1371 matched token, and 4979 token mismatch\n",
      "## Model validation for 6400 tokens : 20.921875% similarity, with 1339 matched token, and 5061 token mismatch\n",
      "## Model validation for 6450 tokens : 20.4031007751938% similarity, with 1316 matched token, and 5134 token mismatch\n",
      "## Model validation for 6500 tokens : 19.93846153846154% similarity, with 1296 matched token, and 5204 token mismatch\n",
      "## Model validation for 6550 tokens : 19.267175572519086% similarity, with 1262 matched token, and 5288 token mismatch\n",
      "## Model validation for 6600 tokens : 18.742424242424242% similarity, with 1237 matched token, and 5363 token mismatch\n",
      "## Model validation for 6650 tokens : 18.466165413533833% similarity, with 1228 matched token, and 5422 token mismatch\n",
      "## Model validation for 6700 tokens : 18.208955223880597% similarity, with 1220 matched token, and 5480 token mismatch\n",
      "## Model validation for 6750 tokens : 17.62962962962963% similarity, with 1190 matched token, and 5560 token mismatch\n",
      "## Model validation for 6800 tokens : 17.36764705882353% similarity, with 1181 matched token, and 5619 token mismatch\n",
      "## Model validation for 6850 tokens : 16.934306569343065% similarity, with 1160 matched token, and 5690 token mismatch\n",
      "## Model validation for 6900 tokens : 16.579710144927535% similarity, with 1144 matched token, and 5756 token mismatch\n",
      "## Model validation for 6950 tokens : 16.014388489208635% similarity, with 1113 matched token, and 5837 token mismatch\n",
      "## Model validation for 7000 tokens : 15.6% similarity, with 1092 matched token, and 5908 token mismatch\n",
      "## Model validation for 7050 tokens : 15.134751773049645% similarity, with 1067 matched token, and 5983 token mismatch\n",
      "## Model validation for 7100 tokens : 14.830985915492958% similarity, with 1053 matched token, and 6047 token mismatch\n",
      "## Model validation for 7150 tokens : 14.503496503496505% similarity, with 1037 matched token, and 6113 token mismatch\n",
      "## Model validation for 7200 tokens : 14.208333333333334% similarity, with 1023 matched token, and 6177 token mismatch\n",
      "## Model validation for 7250 tokens : 13.73793103448276% similarity, with 996 matched token, and 6254 token mismatch\n",
      "## Model validation for 7300 tokens : 13.547945205479452% similarity, with 989 matched token, and 6311 token mismatch\n",
      "## Model validation for 7350 tokens : 12.979591836734695% similarity, with 954 matched token, and 6396 token mismatch\n",
      "## Model validation for 7400 tokens : 12.81081081081081% similarity, with 948 matched token, and 6452 token mismatch\n",
      "## Model validation for 7450 tokens : 12.550335570469798% similarity, with 935 matched token, and 6515 token mismatch\n",
      "## Model validation for 7500 tokens : 12.413333333333334% similarity, with 931 matched token, and 6569 token mismatch\n",
      "## Model validation for 7550 tokens : 12.158940397350994% similarity, with 918 matched token, and 6632 token mismatch\n",
      "## Model validation for 7600 tokens : 11.973684210526315% similarity, with 910 matched token, and 6690 token mismatch\n",
      "## Model validation for 7650 tokens : 11.529411764705882% similarity, with 882 matched token, and 6768 token mismatch\n",
      "## Model validation for 7700 tokens : 11.35064935064935% similarity, with 874 matched token, and 6826 token mismatch\n",
      "## Model validation for 7750 tokens : 10.967741935483872% similarity, with 850 matched token, and 6900 token mismatch\n",
      "## Model validation for 7800 tokens : 10.76923076923077% similarity, with 840 matched token, and 6960 token mismatch\n",
      "## Model validation for 7850 tokens : 10.420382165605096% similarity, with 818 matched token, and 7032 token mismatch\n",
      "## Model validation for 7900 tokens : 10.329113924050633% similarity, with 816 matched token, and 7084 token mismatch\n",
      "## Model validation for 7950 tokens : 10.10062893081761% similarity, with 803 matched token, and 7147 token mismatch\n",
      "## Model validation for 8000 tokens : 9.9625% similarity, with 797 matched token, and 7203 token mismatch\n",
      "###\n",
      "### Model validation end ###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval!\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\"\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\" \"none\" 1000 4000\n",
    "!python3 ./memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/Memory-Tune-Stage-2-{MODEL_NAME}\" \"none\" 4000 8000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13139.889587,
   "end_time": "2024-01-23T11:31:10.160375",
   "environment_variables": {},
   "exception": null,
   "input_path": "./World-7B-mem-finetune.ipynb",
   "output_path": "./World-7B-mem-finetune.output.ipynb",
   "parameters": {},
   "start_time": "2024-01-23T07:52:10.270788",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

#
# Settings used for the final datapack, more specifically its data storage location, etc
#
datapack:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # If using relative path, this should be relative to the trainer script path
  data_path: ../datapath/v5-validation/example-datapack/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Mixing mode to use, this is used to alternate between datasets
  #
  # - batch  : Meaning one dataset worth per batch, partial batches are discarded
  # - sample : Dataset is mixed on a per sample level
  mixing_mode: "shuffle"


#
# Default settings used across all datasets in the datapack
# These settings can be overriden by the dataset specific settings
#
default:

  # dataset_path for the prebuilt dataset, to save into using HF `save _to_disk()`
  #
  # Datapath here is entirely optional, and only used if you intend to save each individual dataset
  # seperately (makes it easier to tweak and rebuild the datapack if it crash mid-way)
  #
  # The dataset index will be appended to the default value, if set
  # ---
  data_path: ../datapath/v5-validation/example-datapack-cache/

  # Data path storage options, this is used to support cloud storage
  # via the huggingface dataset API. See:
  # https://huggingface.co/docs/datasets/v2.16.1/en/filesystems#amazon-s3
  #
  # Note: As of Jan 2023, these options has been only tested to work with AWS S3, and backblaze. YMMV
  #       For S3 bucket support you will also need to install s3fs `python3 -m pip install s3fs`
  #
  # If you want to reduce the risk of accidental key/secret commits, you can use
  # `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables instead
  #
  # For datapath, it should use the `s3://bucket-name/subpath` format
  # ---
  # data_path_storage_options:
  #   key: <example S3 key>
  #   secret: <example S3 secret>
  #   endpoint_url: <example S3 endpoint>

  # Additional source dataset params, used to grab subsets of the dataset
  # ---
  # source_dataset_params:
  #   language: en

  # Sort the dataset by length, useful to reduce gpu waiting time (also useful for RWKV long context coherence)
  # ---
  # sort_by_length: false
  # sort_asc: true # Sort in ascending order, true = shortest first, false = longest first

  # Limit the document count, to an offset/length limit
  # If an int value is used, it is interprated as document count
  # If a floating value (<1.0) is used, it is interprated as a percentage of the dataset
  # ---
  # dataset_offset: -1
  # dataset_length: -1

  # Use data_dir, if you are using source=text/json/etc
  # If using relative path, this should be relative to the trainer script path
  # source_data_dir: ../dataset-text/

  # After loading the dataset, split out test data used for validation, 
  # This process is skipped if the dataset includes a test split
  #
  # If given a float value, a percentage of the dataset is used (1.0 being 100%)
  # If given an int value, the number of data sample is used.
  #
  # Due to the limitaitons in the trainer process, there is always a minimum of 1 test sample
  test_split: 0.01
  test_split_shuffle: true

  # Tokenizer to use, use either the inbuilt 'neox', or 'world' tokenizer
  # If using a custom tokenizer, provide the HF tokenizer name/path
  # ---
  tokenizer: world

  # Minimum / Maximum token size of the dataset to use
  # useful for filtering out small noisy data samples from large datasets
  # (eg. removal of small articles of less then 1024 tokens from wikipedia)
  #
  # This is ignored, if set to -1
  # ---
  # min_token_size: 1024
  # max_token_size: -1

  # Custom text column to use, useful for dataset with alternative training columns labels
  # This is checked before multi column merging, default is null (disabled)
  # eg: 'code'
  # ---
  # custom_text_key: 'code'

  # Multi Column merging process, default setting is used to support and merge
  # "instruction", "input", "output", datasets. To disable set multi_column_keys to []
  #
  # A minimum of 2 columns is required, with non empty data, for the merge to occur
  # If no match is found, this will fallback to the default prompt/completion or text column, 
  # or throw an error if the default fallback is not found
  #
  # IMPORTANT NOTE: as newlines are commonly used for multi_column_suffix, etc. 
  #                 you should use single quotes to ensure such values dun get escaped.
  #                 eg. multi_column_suffix: ['\n\n']
  #
  # See: https://github.com/RWKV/RWKV-infctx-trainer/issues/34
  # Need to use " or the new lines won't be tokenized properly
  # ---
  # multi_column_keys: ["instruction", "input", "output"]
  # multi_column_prefix: ["Instruction:\n", "Input:\n", "Output:\n"]
  # multi_column_suffix: ["\n\n", "\n\n", "\n\n"]
  # multi_column_train_mask: [true, false, true]
  # multi_column_separator: "\n\n"
  
  # Conversation merging process
  # useful for merging full conversational datasets, into single documents
  # default is off, (or set conversation_key to [])
  # conversation_formatting supports "iopairs" or "sender" for now.
  # ---
  # conversation_format: 'iopairs'
  # conversation_key: 'conversation'
  # conversation_end_of_conversation: "\n\nUser:"

  # Iopairs specific config
  # This means that every object in the conversation object is a pair of input output.
  # In future it will also support a format where one of the keys dictates the format style
  # if conversation_key is set to null, it will use the root object as the conversation object
  # ---
  # conversation_input_key_prefix_map: {'input': "\n\nUser: ", 'output': "\n\nAssistant: "}
  # conversation_input_key_mask: {'input': false, 'output': true}
  # conversation_sender_suffix: {'input': "", 'output': ""}

  # Sender specific config
  # This means that every object in the conversation object is a single message (with sender and message keys - or similar)
  # The output is dictated by the input key map, the rest of the "sender_" config is keyed by the value of the sender key
  # conversation_input_key_map: {'message': "\n\n{sender}: ", 'context': ''}
  # conversation_sender_key: 'sender'
  # conversation_sender_value_map: {'user': 'User', 'assistant': 'Assistant', 'system': 'System'}
  # conversation_sender_mask: {'user': false, 'assistant': true, 'system': false}
  # conversation_sender_suffix: {'user': "", 'assistant': "", 'system': ""}

  # If processing prompt/completion jsonl pairs, the prompt is masked by default
  # use this flag to disable this default behaviour
  # ---
  # disable_prompt_completion_mask: false

  # ----------------------------
  # Rechunking support
  # ----------------------------

  # Rechunking of text dataset, this is done only when source is set as 'text'
  # and will merge the various sentencees, into larger chunks up to the target size
  #
  # Defaults to 2048
  #
  # This is ignored, if source is not set as text (unless text_rechunk_force)
  # This is ignored, if set to zero / -1
  # ---
  text_rechunk_size: 4096

  # Apply text rechunk to the dataset, even if its not a 'text' source
  # This is done only after dataset filtering, and if source is not 'text'
  # ---
  text_rechunk_force: False

  # Used to disable the automated text rechunkin for text files, if set as false
  # ---
  text_rechunk_auto: True

  # ----------------------------
  # Dataset packing support
  # Recommended to be used with mixed documents sized finetuning
  # For foundation model "from scratch", rechunking is typically used instead
  # ----------------------------

  # Boolean flag to enable / disable dataset packing
  packing_enable: True

  # Used to ensure all training samples wihin this batch size is the same length
  # Ideally this should align exactly with your real "batch size"
  #
  # Uses, `8 * (3 * 4 * 5 * 6 * 7) = 20160` for default, as it should align across
  # a large number of batch size combinations. This helps reduce the amount of
  # misaligned batches, and thus reduce the amount of wasted training time.
  #
  # This is tagged to datapack.batchsize, unless overriden here or on a dataset level
  # ---
  # packing_batchsize: 20160

  # Chunking size to align within each batch, this ideally should be equal to
  # the training context length used.
  packing_chunksize: 4096

  # Minimum size to pack up to, this should be a multiple of packing_chunksize
  # defautls to -1, which equals to packing_chunksize
  packing_min_ctx_len: -1

  # Pack the data sequentially if possible, in accordance to the dataset sequence
  # this can be used together with sort_by_length, otherwise a shuffle will be done
  packing_in_sequence: False

  # ----------------------------
  # Specal use caes flags
  # ----------------------------

  # Reverse the training dataset order before saving, this is useful for,
  # optimizing dataset packing process, when using packing_in_sequence
  # and sort_by_length desc order together
  reverse_train_dataset_before_save: False

  # # (NOT IMPLEMENTED) Oversampling logic used
  # # - none : No oversampling is done, dataset is skipped when exhausted
  # # - halt : Halt the generation of the datapack, when exhausted
  # # - loop : Loop the dataset, when exhausted
  # # - shuffle : Reset and shuffle, when exhausted
  # oversampling: "none"

#
# The dataset specific settings
# 
dataset:
  
  # ---
  # Each dataset should be unique 
  # ---

  - # provide the source path, which is used as huggingface dataset path
    # this will be used to populate the dataset_path
    #
    # Use either the following
    # - hugging face dataset 
    # - local dataset mode (ie: text,json,csv - use source_data_dir, to configure the path then)
    # - null
    #
    # If source is disabled, all other params, you will be expected to provide data_path
    # which will be used instead
    #
    # source: null                 # Disabled
    # source: text                 # Text mode, used with source_data_dir
    source: "teven/enwiki_10k"   # Hugging face dataset

    # Datapath to use, if source is disabled
    # alternatively if source is provided, save to the following datapath instead
    # ---
    # datapath: null

    # Advance datapath storage options, see previous examples for more details
    # ---
    # data_path_storage_options:
    #   key: <example S3 key>
    #   secret: <example S3 secret>
    #   endpoint_url: <example S3 endpoint>

    # Optional, provide a name for the dataset
    name: "enwiki_10k"

    # All other settings found in default can be overriden here
    # ---
    text_rechunk_force: True
    text_rechunk_size: 8192
    
    # # (NOT IMPLEMENTED) 
    # # Increase the dataset weightage, to increase the amount of data used
    # # Defaults to 1.0 
    # dataset_weight: 1.0

  - # provide the source path, which is used as huggingface dataset path
    # this will be used to populate the dataset_path
    #
    # Use either the following
    # - hugging face dataset 
    # - local dataset mode (ie: text,json,csv - use source_data_dir, to configure the path then)
    # - null
    #
    # If source is disabled, all other params, you will be expected to provide data_path
    # which will be used instead
    #
    # source: null                 # Disabled
    # source: text                 # Text mode, used with source_data_dir
    source: "teknium/openhermes"   # Hugging face dataset

    # Datapath to use, if source is disabled
    # alternatively if source is provided, save to the following datapath instead
    # ---
    # datapath: null

    # Advance datapath storage options, see previous examples for more details
    # ---
    # data_path_storage_options:
    #   key: <example S3 key>
    #   secret: <example S3 secret>
    #   endpoint_url: <example S3 endpoint>

    # Optional, provide a name for the dataset
    name: "openhermes"

    # All other settings found in default can be overriden here
    # ---
    packing_enable: True
    packing_min_ctx_len: 8192

    # # (NOT IMPLEMENTED) 
    # # Increase the dataset weightage, to increase the amount of data used
    # # Defaults to 1.0 
    # dataset_weight: 1.0



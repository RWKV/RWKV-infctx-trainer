{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short enwiki batch testing\n",
    "\n",
    "Test that the training code, batching mode, works with\n",
    "\n",
    "**1B5 model / L24-D2048 model with**\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "## The following are the performance numbers on a single AWS A10G\n",
    "\n",
    "| Batch Size | Peak Tokens / Sec | Peak VRAM  | Time taken (include setup/validation) |\n",
    "|------------|-------------------|------------|---------------------------------------|\n",
    "| 8          | 53.71 kt/sec      | ~ 20.09 GB | 71.45 sec                             |\n",
    "| 4          | 44.17 kt/sec      | ~ 13.24 GB | 83.63 sec                             |\n",
    "| 2          | 32.51 kt/sec      | ~ 7.74 GB  | 108.472 sec                           |\n",
    "| 1          | 21.73 kt/sec      | ~ 4.91 GB  | 153.902 sec                           |\n",
    "\n",
    "The general advice is increase your microbatch_size size until your vram maxes out. Then increase target_batch_size to match, or scale in multiples of GPU count & microbatch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"infctx-v5-validation\"\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories\n",
    "!mkdir -p \"{PROJECT_DIR}/model/\"\n",
    "!mkdir -p \"{PROJECT_DIR}/datapath/\"\n",
    "!mkdir -p \"{PROJECT_DIR}/checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 12:25:47,003] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Creating extension directory /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF wkv5_op.o.d -DTORCH_EXTENSION_NAME=wkv5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/module/cuda/wkv5_op.cpp -o wkv5_op.o \n",
      "/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/module/cuda/wkv5_op.cpp:48:51: note: #pragma message: No SIMD is supported\n",
      "   48 |             #pragma message(\"No SIMD is supported\")\n",
      "      |                                                   ^\n",
      "[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -D_N_=64 -std=c++17 -c /home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/module/cuda/wkv5_cuda.cu -o wkv5_cuda.cuda.o \n",
      "ptxas info    : 3 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiiPfPKT_S3_S3_PKfS5_S3_S3_PS1_S6_S6_S6_S6_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardIfEviiiiPfPKT_S3_S3_PKfS5_S3_S3_PS1_S6_S6_S6_S6_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 168 registers, 1536 bytes smem, 472 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardIN3c104HalfEEviiiiPfPKT_S5_S5_PKfS7_S5_S5_PS3_S8_S8_S8_S8_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardIN3c104HalfEEviiiiPfPKT_S5_S5_PKfS7_S5_S5_PS3_S8_S8_S8_S8_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 168 registers, 1536 bytes smem, 472 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardIN3c108BFloat16EEviiiiPfPKT_S5_S5_PKfS7_S5_S5_PS3_S8_S8_S8_S8_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardIN3c108BFloat16EEviiiiPfPKT_S5_S5_PKfS7_S5_S5_PS3_S8_S8_S8_S8_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 168 registers, 1536 bytes smem, 472 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z24kernel_forward_inferenceIfEviiiiPfPKT_S3_S3_PKfS3_PS1_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z24kernel_forward_inferenceIfEviiiiPfPKT_S3_S3_PKfS3_PS1_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 102 registers, 1024 bytes smem, 424 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z24kernel_forward_inferenceIN3c104HalfEEviiiiPfPKT_S5_S5_PKfS5_PS3_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z24kernel_forward_inferenceIN3c104HalfEEviiiiPfPKT_S5_S5_PKfS5_PS3_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 102 registers, 1024 bytes smem, 424 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z24kernel_forward_inferenceIN3c108BFloat16EEviiiiPfPKT_S5_S5_PKfS5_PS3_' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z24kernel_forward_inferenceIN3c108BFloat16EEviiiiPfPKT_S5_S5_PKfS5_PS3_\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 102 registers, 1024 bytes smem, 424 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z15kernelc_mm8_oneIfEvyyPKT_PKhyPfPKfS7_yy' for 'sm_89'\n",
      "ptxas info    : Function properties for _Z15kernelc_mm8_oneIfEvyyPKT_PKhyPfPKfS7_yy\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 432 bytes cmem[0]\n",
      "[3/3] c++ wkv5_op.o wkv5_cuda.cuda.o -shared -L/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv5.so\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "65536 2048  -0.0001 emb.weight\n",
      "2048  2048  1.0  blocks.0.att.receptance.weight\n",
      "2048  2048  1.0  blocks.0.att.key.weight\n",
      "2048  2048  1.0  blocks.0.att.value.weight\n",
      "2048  2048  0    blocks.0.att.output.weight\n",
      "2048  2048  1.0  blocks.0.att.gate.weight\n",
      "7168  2048  1.0  blocks.0.ffn.key.weight\n",
      "2048  2048  0    blocks.0.ffn.receptance.weight\n",
      "2048  7168  0    blocks.0.ffn.value.weight\n",
      "2048  2048  1.0  blocks.1.att.receptance.weight\n",
      "2048  2048  1.0  blocks.1.att.key.weight\n",
      "2048  2048  1.0  blocks.1.att.value.weight\n",
      "2048  2048  0    blocks.1.att.output.weight\n",
      "2048  2048  1.0  blocks.1.att.gate.weight\n",
      "7168  2048  1.0  blocks.1.ffn.key.weight\n",
      "2048  2048  0    blocks.1.ffn.receptance.weight\n",
      "2048  7168  0    blocks.1.ffn.value.weight\n",
      "2048  2048  1.0  blocks.2.att.receptance.weight\n",
      "2048  2048  1.0  blocks.2.att.key.weight\n",
      "2048  2048  1.0  blocks.2.att.value.weight\n",
      "2048  2048  0    blocks.2.att.output.weight\n",
      "2048  2048  1.0  blocks.2.att.gate.weight\n",
      "7168  2048  1.0  blocks.2.ffn.key.weight\n",
      "2048  2048  0    blocks.2.ffn.receptance.weight\n",
      "2048  7168  0    blocks.2.ffn.value.weight\n",
      "2048  2048  1.0  blocks.3.att.receptance.weight\n",
      "2048  2048  1.0  blocks.3.att.key.weight\n",
      "2048  2048  1.0  blocks.3.att.value.weight\n",
      "2048  2048  0    blocks.3.att.output.weight\n",
      "2048  2048  1.0  blocks.3.att.gate.weight\n",
      "7168  2048  1.0  blocks.3.ffn.key.weight\n",
      "2048  2048  0    blocks.3.ffn.receptance.weight\n",
      "2048  7168  0    blocks.3.ffn.value.weight\n",
      "2048  2048  1.0  blocks.4.att.receptance.weight\n",
      "2048  2048  1.0  blocks.4.att.key.weight\n",
      "2048  2048  1.0  blocks.4.att.value.weight\n",
      "2048  2048  0    blocks.4.att.output.weight\n",
      "2048  2048  1.0  blocks.4.att.gate.weight\n",
      "7168  2048  1.0  blocks.4.ffn.key.weight\n",
      "2048  2048  0    blocks.4.ffn.receptance.weight\n",
      "2048  7168  0    blocks.4.ffn.value.weight\n",
      "2048  2048  1.0  blocks.5.att.receptance.weight\n",
      "2048  2048  1.0  blocks.5.att.key.weight\n",
      "2048  2048  1.0  blocks.5.att.value.weight\n",
      "2048  2048  0    blocks.5.att.output.weight\n",
      "2048  2048  1.0  blocks.5.att.gate.weight\n",
      "7168  2048  1.0  blocks.5.ffn.key.weight\n",
      "2048  2048  0    blocks.5.ffn.receptance.weight\n",
      "2048  7168  0    blocks.5.ffn.value.weight\n",
      "2048  2048  1.0  blocks.6.att.receptance.weight\n",
      "2048  2048  1.0  blocks.6.att.key.weight\n",
      "2048  2048  1.0  blocks.6.att.value.weight\n",
      "2048  2048  0    blocks.6.att.output.weight\n",
      "2048  2048  1.0  blocks.6.att.gate.weight\n",
      "7168  2048  1.0  blocks.6.ffn.key.weight\n",
      "2048  2048  0    blocks.6.ffn.receptance.weight\n",
      "2048  7168  0    blocks.6.ffn.value.weight\n",
      "2048  2048  1.0  blocks.7.att.receptance.weight\n",
      "2048  2048  1.0  blocks.7.att.key.weight\n",
      "2048  2048  1.0  blocks.7.att.value.weight\n",
      "2048  2048  0    blocks.7.att.output.weight\n",
      "2048  2048  1.0  blocks.7.att.gate.weight\n",
      "7168  2048  1.0  blocks.7.ffn.key.weight\n",
      "2048  2048  0    blocks.7.ffn.receptance.weight\n",
      "2048  7168  0    blocks.7.ffn.value.weight\n",
      "2048  2048  1.0  blocks.8.att.receptance.weight\n",
      "2048  2048  1.0  blocks.8.att.key.weight\n",
      "2048  2048  1.0  blocks.8.att.value.weight\n",
      "2048  2048  0    blocks.8.att.output.weight\n",
      "2048  2048  1.0  blocks.8.att.gate.weight\n",
      "7168  2048  1.0  blocks.8.ffn.key.weight\n",
      "2048  2048  0    blocks.8.ffn.receptance.weight\n",
      "2048  7168  0    blocks.8.ffn.value.weight\n",
      "2048  2048  1.0  blocks.9.att.receptance.weight\n",
      "2048  2048  1.0  blocks.9.att.key.weight\n",
      "2048  2048  1.0  blocks.9.att.value.weight\n",
      "2048  2048  0    blocks.9.att.output.weight\n",
      "2048  2048  1.0  blocks.9.att.gate.weight\n",
      "7168  2048  1.0  blocks.9.ffn.key.weight\n",
      "2048  2048  0    blocks.9.ffn.receptance.weight\n",
      "2048  7168  0    blocks.9.ffn.value.weight\n",
      "2048  2048  1.0  blocks.10.att.receptance.weight\n",
      "2048  2048  1.0  blocks.10.att.key.weight\n",
      "2048  2048  1.0  blocks.10.att.value.weight\n",
      "2048  2048  0    blocks.10.att.output.weight\n",
      "2048  2048  1.0  blocks.10.att.gate.weight\n",
      "7168  2048  1.0  blocks.10.ffn.key.weight\n",
      "2048  2048  0    blocks.10.ffn.receptance.weight\n",
      "2048  7168  0    blocks.10.ffn.value.weight\n",
      "2048  2048  1.0  blocks.11.att.receptance.weight\n",
      "2048  2048  1.0  blocks.11.att.key.weight\n",
      "2048  2048  1.0  blocks.11.att.value.weight\n",
      "2048  2048  0    blocks.11.att.output.weight\n",
      "2048  2048  1.0  blocks.11.att.gate.weight\n",
      "7168  2048  1.0  blocks.11.ffn.key.weight\n",
      "2048  2048  0    blocks.11.ffn.receptance.weight\n",
      "2048  7168  0    blocks.11.ffn.value.weight\n",
      "2048  2048  1.0  blocks.12.att.receptance.weight\n",
      "2048  2048  1.0  blocks.12.att.key.weight\n",
      "2048  2048  1.0  blocks.12.att.value.weight\n",
      "2048  2048  0    blocks.12.att.output.weight\n",
      "2048  2048  1.0  blocks.12.att.gate.weight\n",
      "7168  2048  1.0  blocks.12.ffn.key.weight\n",
      "2048  2048  0    blocks.12.ffn.receptance.weight\n",
      "2048  7168  0    blocks.12.ffn.value.weight\n",
      "2048  2048  1.0  blocks.13.att.receptance.weight\n",
      "2048  2048  1.0  blocks.13.att.key.weight\n",
      "2048  2048  1.0  blocks.13.att.value.weight\n",
      "2048  2048  0    blocks.13.att.output.weight\n",
      "2048  2048  1.0  blocks.13.att.gate.weight\n",
      "7168  2048  1.0  blocks.13.ffn.key.weight\n",
      "2048  2048  0    blocks.13.ffn.receptance.weight\n",
      "2048  7168  0    blocks.13.ffn.value.weight\n",
      "2048  2048  1.0  blocks.14.att.receptance.weight\n",
      "2048  2048  1.0  blocks.14.att.key.weight\n",
      "2048  2048  1.0  blocks.14.att.value.weight\n",
      "2048  2048  0    blocks.14.att.output.weight\n",
      "2048  2048  1.0  blocks.14.att.gate.weight\n",
      "7168  2048  1.0  blocks.14.ffn.key.weight\n",
      "2048  2048  0    blocks.14.ffn.receptance.weight\n",
      "2048  7168  0    blocks.14.ffn.value.weight\n",
      "2048  2048  1.0  blocks.15.att.receptance.weight\n",
      "2048  2048  1.0  blocks.15.att.key.weight\n",
      "2048  2048  1.0  blocks.15.att.value.weight\n",
      "2048  2048  0    blocks.15.att.output.weight\n",
      "2048  2048  1.0  blocks.15.att.gate.weight\n",
      "7168  2048  1.0  blocks.15.ffn.key.weight\n",
      "2048  2048  0    blocks.15.ffn.receptance.weight\n",
      "2048  7168  0    blocks.15.ffn.value.weight\n",
      "2048  2048  1.0  blocks.16.att.receptance.weight\n",
      "2048  2048  1.0  blocks.16.att.key.weight\n",
      "2048  2048  1.0  blocks.16.att.value.weight\n",
      "2048  2048  0    blocks.16.att.output.weight\n",
      "2048  2048  1.0  blocks.16.att.gate.weight\n",
      "7168  2048  1.0  blocks.16.ffn.key.weight\n",
      "2048  2048  0    blocks.16.ffn.receptance.weight\n",
      "2048  7168  0    blocks.16.ffn.value.weight\n",
      "2048  2048  1.0  blocks.17.att.receptance.weight\n",
      "2048  2048  1.0  blocks.17.att.key.weight\n",
      "2048  2048  1.0  blocks.17.att.value.weight\n",
      "2048  2048  0    blocks.17.att.output.weight\n",
      "2048  2048  1.0  blocks.17.att.gate.weight\n",
      "7168  2048  1.0  blocks.17.ffn.key.weight\n",
      "2048  2048  0    blocks.17.ffn.receptance.weight\n",
      "2048  7168  0    blocks.17.ffn.value.weight\n",
      "2048  2048  1.0  blocks.18.att.receptance.weight\n",
      "2048  2048  1.0  blocks.18.att.key.weight\n",
      "2048  2048  1.0  blocks.18.att.value.weight\n",
      "2048  2048  0    blocks.18.att.output.weight\n",
      "2048  2048  1.0  blocks.18.att.gate.weight\n",
      "7168  2048  1.0  blocks.18.ffn.key.weight\n",
      "2048  2048  0    blocks.18.ffn.receptance.weight\n",
      "2048  7168  0    blocks.18.ffn.value.weight\n",
      "2048  2048  1.0  blocks.19.att.receptance.weight\n",
      "2048  2048  1.0  blocks.19.att.key.weight\n",
      "2048  2048  1.0  blocks.19.att.value.weight\n",
      "2048  2048  0    blocks.19.att.output.weight\n",
      "2048  2048  1.0  blocks.19.att.gate.weight\n",
      "7168  2048  1.0  blocks.19.ffn.key.weight\n",
      "2048  2048  0    blocks.19.ffn.receptance.weight\n",
      "2048  7168  0    blocks.19.ffn.value.weight\n",
      "2048  2048  1.0  blocks.20.att.receptance.weight\n",
      "2048  2048  1.0  blocks.20.att.key.weight\n",
      "2048  2048  1.0  blocks.20.att.value.weight\n",
      "2048  2048  0    blocks.20.att.output.weight\n",
      "2048  2048  1.0  blocks.20.att.gate.weight\n",
      "7168  2048  1.0  blocks.20.ffn.key.weight\n",
      "2048  2048  0    blocks.20.ffn.receptance.weight\n",
      "2048  7168  0    blocks.20.ffn.value.weight\n",
      "2048  2048  1.0  blocks.21.att.receptance.weight\n",
      "2048  2048  1.0  blocks.21.att.key.weight\n",
      "2048  2048  1.0  blocks.21.att.value.weight\n",
      "2048  2048  0    blocks.21.att.output.weight\n",
      "2048  2048  1.0  blocks.21.att.gate.weight\n",
      "7168  2048  1.0  blocks.21.ffn.key.weight\n",
      "2048  2048  0    blocks.21.ffn.receptance.weight\n",
      "2048  7168  0    blocks.21.ffn.value.weight\n",
      "2048  2048  1.0  blocks.22.att.receptance.weight\n",
      "2048  2048  1.0  blocks.22.att.key.weight\n",
      "2048  2048  1.0  blocks.22.att.value.weight\n",
      "2048  2048  0    blocks.22.att.output.weight\n",
      "2048  2048  1.0  blocks.22.att.gate.weight\n",
      "7168  2048  1.0  blocks.22.ffn.key.weight\n",
      "2048  2048  0    blocks.22.ffn.receptance.weight\n",
      "2048  7168  0    blocks.22.ffn.value.weight\n",
      "2048  2048  1.0  blocks.23.att.receptance.weight\n",
      "2048  2048  1.0  blocks.23.att.key.weight\n",
      "2048  2048  1.0  blocks.23.att.value.weight\n",
      "2048  2048  0    blocks.23.att.output.weight\n",
      "2048  2048  1.0  blocks.23.att.gate.weight\n",
      "7168  2048  1.0  blocks.23.ffn.key.weight\n",
      "2048  2048  0    blocks.23.ffn.receptance.weight\n",
      "2048  7168  0    blocks.23.ffn.value.weight\n",
      "65536 2048  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L24-D2048 model with the init_model.py code\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 24 --n_embd 2048 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L24-D2048-world-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████| 424/424 [00:00<00:00, 3.68MB/s]\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/15.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  28%|█████▌              | 4.19M/15.2M [00:03<00:09, 1.20MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 15.2M/15.2M [00:06<00:00, 2.24MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:06<00:00,  6.80s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1316.89it/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|██| 10000/10000 [00:00<00:00, 140261.31 examples/s]\n",
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 9506.25 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 12874.87 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 1339/1339 [00:00<00:00, 2247.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 763/763 [00:00<00:00, 27513.45 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 4063.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Preload the dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 13:29:31,402] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation (train-ctx=1024, data-ctx=4096, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto', '--trainer.fast_dev_run=2', '--model.ctx_len=256', '--trainer.microbatch_size=1', '--model.load_model=../model/L24-D2048-world-init.pth'], args=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation (train-ctx=1024, data-ctx=4096, deepspeed_stage_2)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto', '--trainer.fast_dev_run=2', '--model.ctx_len=256', '--trainer.microbatch_size=1', '--model.load_model=../model/L24-D2048-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 2 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 763/763 [00:00<00:00, 54591.35 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 4626.92 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05032920837402344 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392035891/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/lightning_trainer.py\", line 296, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/lightning_trainer.py\", line 271, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 965, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 353, in setup\n",
      "    self.init_deepspeed()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 452, in init_deepspeed\n",
      "    self._initialize_deepspeed_train(self.model)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 488, in _initialize_deepspeed_train\n",
      "    model, deepspeed_optimizer = self._setup_model_and_optimizer(model, optimizer, scheduler)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/deepspeed.py\", line 424, in _setup_model_and_optimizer\n",
      "    deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 304, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1222, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1483, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 510, in __init__\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 645, in initialize_optimizer_states\n",
      "    self.optimizer.step()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/optim/lr_scheduler.py\", line 68, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py\", line 157, in step\n",
      "    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.88 GiB. GPU 0 has a total capacty of 22.13 GiB of which 716.62 MiB is free. Including non-PyTorch memory, this process has 21.23 GiB memory in use. Of the allocated memory 20.57 GiB is allocated by PyTorch, and 6.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Short training process - for quick testing / debugging\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"disabled\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (train-ctx=1024, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.fast_dev_run=2 \\\n",
    "        --model.ctx_len=256 \\\n",
    "        --trainer.microbatch_size=1 \\\n",
    "        --model.load_model=\"../model/L24-D2048-world-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 02:13:59,508] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 1 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=1', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 1 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=1', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 13520.66 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2512.12 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231119_021406-ecnw9v5g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-validation - microbatch 1 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/ecnw9v5g\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06810188293457031 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 751/751 [02:21<00:00,  5.30it/s, v_num=9v5g, train/loss=6.810]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▌                 | 1/8 [00:00<00:04,  1.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 2/8 [00:01<00:03,  1.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███████▌            | 3/8 [00:01<00:02,  1.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 4/8 [00:02<00:02,  1.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|████████████▌       | 5/8 [00:02<00:01,  1.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 6/8 [00:03<00:01,  1.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████▌  | 7/8 [00:03<00:00,  1.76it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 8/8 [00:04<00:00,  1.76it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 751/751 [02:26<00:00,  5.13it/s, v_num=9v5g, train/loss=6.810, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 751/751 [02:26<00:00,  5.13it/s, v_num=9v5g, train/loss=6.810, \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▁▄▆▆▇▇▇▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss █▇▆▆▅▄▄▃▃▃▃▃▂▂▂▂▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 750\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 4096.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 21730.81796\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 3076096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 750\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 6.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 6.77734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-validation - microbatch 1 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/ecnw9v5g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNjk3NDc5Mw==/version_details/v0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231119_021406-ecnw9v5g/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Empty out the checkpoint\n",
    "!cd \"{PROJECT_DIR}\" && rm -rf \"./checkpoint/infctx-v5-unit-test-baseline-4096/\"\n",
    "\n",
    "# Microbatch 1 training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - microbatch 1 (train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.microbatch_size=1 \\\n",
    "        --model.load_model=\"../model/L24-D2048-world-init.pth\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 02:16:53,648] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 2 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=2', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 2 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=2', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 13080.69 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2619.80 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231119_021701-zfo56uux\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-validation - microbatch 2 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/zfo56uux\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06867575645446777 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 376/376 [01:36<00:00,  3.89it/s, v_num=6uux, train/loss=6.620]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▌                 | 1/8 [00:00<00:04,  1.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 2/8 [00:01<00:03,  1.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███████▌            | 3/8 [00:01<00:02,  1.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 4/8 [00:02<00:02,  1.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|████████████▌       | 5/8 [00:02<00:01,  1.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 6/8 [00:03<00:01,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████▌  | 7/8 [00:03<00:00,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 8/8 [00:04<00:00,  1.80it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 376/376 [01:41<00:00,  3.71it/s, v_num=6uux, train/loss=6.620, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 376/376 [01:41<00:00,  3.71it/s, v_num=6uux, train/loss=6.620, \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ███████████████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▁▄▅▆▆▇▇▇▇▇▇▇▇███████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss █▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 2048.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 32231.84758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 3076096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 6.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 6.55469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-validation - microbatch 2 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/zfo56uux\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNjk3NDc5Mw==/version_details/v0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231119_021701-zfo56uux/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Empty out the checkpoint\n",
    "!cd \"{PROJECT_DIR}\" && rm -rf \"./checkpoint/infctx-v5-unit-test-baseline-4096/\"\n",
    "\n",
    "# Microbatch 1 training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - microbatch 2 (train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.microbatch_size=2 \\\n",
    "        --model.load_model=\"../model/L24-D2048-world-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 02:19:00,550] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 4 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=4', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 4 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=4', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         4\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 12358.60 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2425.86 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231119_021907-gozsvbkj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-validation - microbatch 4 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/gozsvbkj\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06836247444152832 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 188/188 [01:11<00:00,  2.62it/s, v_num=vbkj, train/loss=6.500]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▌                 | 1/8 [00:00<00:04,  1.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 2/8 [00:01<00:03,  1.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███████▌            | 3/8 [00:01<00:02,  1.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 4/8 [00:02<00:02,  1.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|████████████▌       | 5/8 [00:02<00:01,  1.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 6/8 [00:03<00:01,  1.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████▌  | 7/8 [00:03<00:00,  1.77it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 8/8 [00:04<00:00,  1.77it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 188/188 [01:16<00:00,  2.46it/s, v_num=vbkj, train/loss=6.500, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 188/188 [01:16<00:00,  2.46it/s, v_num=vbkj, train/loss=6.500, \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ███████████████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▁▄▅▆▆▆▇▇▇▇▇▇▇███████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss █▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 3072.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 43646.67269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 3076096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 6.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 6.53516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-validation - microbatch 4 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/gozsvbkj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNjk3NDc5Mw==/version_details/v0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231119_021907-gozsvbkj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Empty out the checkpoint\n",
    "!cd \"{PROJECT_DIR}\" && rm -rf \"./checkpoint/infctx-v5-unit-test-baseline-4096/\"\n",
    "\n",
    "# Microbatch 1 training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - microbatch 3 (train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.microbatch_size=3 \\\n",
    "        --model.load_model=\"../model/L24-D2048-world-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 02:20:44,542] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 8 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=8', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-validation - microbatch 8 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.microbatch_size=8', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 12554.94 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2501.82 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231119_022051-hu0v4l51\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-validation - microbatch 8 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/hu0v4l51\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0675058364868164 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|████| 94/94 [00:59<00:00,  1.58it/s, v_num=4l51, train/loss=6.560]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▌                 | 1/8 [00:00<00:04,  1.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 2/8 [00:01<00:03,  1.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███████▌            | 3/8 [00:01<00:02,  1.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 4/8 [00:02<00:02,  1.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|████████████▌       | 5/8 [00:02<00:01,  1.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 6/8 [00:03<00:01,  1.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████▌  | 7/8 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 8/8 [00:04<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 94/94 [01:04<00:00,  1.47it/s, v_num=4l51, train/loss=6.560, va`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 94/94 [01:04<00:00,  1.47it/s, v_num=4l51, train/loss=6.560, va\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ███████████████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▂▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss █▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 3584.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 52928.30149\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 3076096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 6.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 6.52734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-validation - microbatch 8 (train-ctx=4096, data-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/hu0v4l51\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNjk3NDc5Mw==/version_details/v0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231119_022051-hu0v4l51/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Empty out the checkpoint\n",
    "!cd \"{PROJECT_DIR}\" && rm -rf \"./checkpoint/infctx-v5-unit-test-baseline-4096/\"\n",
    "\n",
    "# Microbatch 1 training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - microbatch 8 (train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.microbatch_size=4 \\\n",
    "        --model.load_model=\"../model/L24-D2048-world-init.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

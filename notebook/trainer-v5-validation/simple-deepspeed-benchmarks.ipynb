{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 1, 2 & 3 benchmark\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate the trainer across deepspeed 1, 2 & 3 - with and without offload. All other training params remain constant. And benchmarking them accordingly\n",
    "\n",
    "## What does deepspeed 1, 2 & 3 do (With/Without CPU offload) ??\n",
    "\n",
    "Instead of simply splitting the dataset being trained, and having a full copy of nearly everything in all GPU's (aka DDP / DeepSpeed 1).\n",
    "\n",
    "Deepspeed 2, keeps a full copy of the model weights on each GPU, but splits the training gradient descent memory usage into multiple GPUs, or offload it into CPU memory (+ CPU offload option).\n",
    "\n",
    "Deepspeed 3, takes it a step further, and distributes the model weights across all the GPUs, drastically lowering the vram requirement, while increasing the amount of GPU to GPU traffic drastically. Gradient descent memory is still split across multiple GPUs, with the option to offload into CPU memory (Same as deepspeed 2)\n",
    "\n",
    "Finally, Deepspeed 3, also introduce options to further offload such model weights / gradient descent, more into CPU memory or NVMe. However this option was not enabled or explored in the following benchmarks.\n",
    "\n",
    "See more here: https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "\n",
    "## Benchmark results\n",
    "\n",
    "Benchmark was done on 20th Aug 2023. With Torch 2.0.1, Cuda 11.8. On 8x3090, via vast.ai\n",
    "All benchmarks was done with ctx length of 4096\n",
    "\n",
    "(@TODO - conslidate and update result)\n",
    "\n",
    "---\n",
    "\n",
    "| Deepspeed Strat       | Time (8 * 3090)  | % difference | VRAM Usage | RAM Usage (peak / mean) |\n",
    "|-----------------------|------------------|--------------|------------|-------------------------|\n",
    "| Stage 1               | 13 mins : 29 sec | -            | 65.11%     | 6.7 / 3.1 ~GB           |\n",
    "| Stage 2               | 13 mins : 29 sec | -            | 55.64%     | 6.7 / 3.1 ~GB           |\n",
    "| Stage 2 + CPU offload | 14 mins : 56 sec | 10.754 %     | 44.61%     | 9.4 / 8.0 ~GB           |\n",
    "| Stage 3               | 14 mins : 11 sec | 5.191 %      | 61.32%     | 6.7 / 2.3 ~GB           |\n",
    "| Stage 3 + CPU offload | 17 mins : 20 sec | 28.553 %     | 44.88%     | 7.4 / 7.2 ~GB           |\n",
    "\n",
    "---\n",
    "\n",
    "> ^ note in theory deepspeed 3 uses less vram then deepspeed 2, however it will also try to use up more ram then its needed for \"cache\" items if possible, maxing out to the same level as deepspeed 2 here\n",
    ">\n",
    "> Torch.JIT was enabled for deepspeed 2, But was disabled for deepspeed 3 (not compatible). Torch.compile was disabled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and apply your preferred settings\n",
    "\n",
    "Adjust your desired deepspeed settings, and gpu device count.\n",
    "\n",
    "Enable/Disable WANDB here as well ( Enabled by default, as we need the loss curve for this experiment )\n",
    "\n",
    "( note you will need to rerun this cell, if you restart your env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"infctx-v5-deepspeed-test\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-neox-v5base-init.pth\n",
      "Vocab size: 50277\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "50277 2048  -0.0001 emb.weight\n",
      "2048  2048  1.0  blocks.0.att.receptance.weight\n",
      "2048  2048  1.0  blocks.0.att.key.weight\n",
      "2048  2048  1.0  blocks.0.att.value.weight\n",
      "2048  2048  0    blocks.0.att.output.weight\n",
      "8192  2048  1.0  blocks.0.ffn.key.weight\n",
      "2048  2048  0    blocks.0.ffn.receptance.weight\n",
      "2048  8192  0    blocks.0.ffn.value.weight\n",
      "2048  2048  1.0  blocks.1.att.receptance.weight\n",
      "2048  2048  1.0  blocks.1.att.key.weight\n",
      "2048  2048  1.0  blocks.1.att.value.weight\n",
      "2048  2048  0    blocks.1.att.output.weight\n",
      "8192  2048  1.0  blocks.1.ffn.key.weight\n",
      "2048  2048  0    blocks.1.ffn.receptance.weight\n",
      "2048  8192  0    blocks.1.ffn.value.weight\n",
      "2048  2048  1.0  blocks.2.att.receptance.weight\n",
      "2048  2048  1.0  blocks.2.att.key.weight\n",
      "2048  2048  1.0  blocks.2.att.value.weight\n",
      "2048  2048  0    blocks.2.att.output.weight\n",
      "8192  2048  1.0  blocks.2.ffn.key.weight\n",
      "2048  2048  0    blocks.2.ffn.receptance.weight\n",
      "2048  8192  0    blocks.2.ffn.value.weight\n",
      "2048  2048  1.0  blocks.3.att.receptance.weight\n",
      "2048  2048  1.0  blocks.3.att.key.weight\n",
      "2048  2048  1.0  blocks.3.att.value.weight\n",
      "2048  2048  0    blocks.3.att.output.weight\n",
      "8192  2048  1.0  blocks.3.ffn.key.weight\n",
      "2048  2048  0    blocks.3.ffn.receptance.weight\n",
      "2048  8192  0    blocks.3.ffn.value.weight\n",
      "2048  2048  1.0  blocks.4.att.receptance.weight\n",
      "2048  2048  1.0  blocks.4.att.key.weight\n",
      "2048  2048  1.0  blocks.4.att.value.weight\n",
      "2048  2048  0    blocks.4.att.output.weight\n",
      "8192  2048  1.0  blocks.4.ffn.key.weight\n",
      "2048  2048  0    blocks.4.ffn.receptance.weight\n",
      "2048  8192  0    blocks.4.ffn.value.weight\n",
      "2048  2048  1.0  blocks.5.att.receptance.weight\n",
      "2048  2048  1.0  blocks.5.att.key.weight\n",
      "2048  2048  1.0  blocks.5.att.value.weight\n",
      "2048  2048  0    blocks.5.att.output.weight\n",
      "8192  2048  1.0  blocks.5.ffn.key.weight\n",
      "2048  2048  0    blocks.5.ffn.receptance.weight\n",
      "2048  8192  0    blocks.5.ffn.value.weight\n",
      "2048  2048  1.0  blocks.6.att.receptance.weight\n",
      "2048  2048  1.0  blocks.6.att.key.weight\n",
      "2048  2048  1.0  blocks.6.att.value.weight\n",
      "2048  2048  0    blocks.6.att.output.weight\n",
      "8192  2048  1.0  blocks.6.ffn.key.weight\n",
      "2048  2048  0    blocks.6.ffn.receptance.weight\n",
      "2048  8192  0    blocks.6.ffn.value.weight\n",
      "2048  2048  1.0  blocks.7.att.receptance.weight\n",
      "2048  2048  1.0  blocks.7.att.key.weight\n",
      "2048  2048  1.0  blocks.7.att.value.weight\n",
      "2048  2048  0    blocks.7.att.output.weight\n",
      "8192  2048  1.0  blocks.7.ffn.key.weight\n",
      "2048  2048  0    blocks.7.ffn.receptance.weight\n",
      "2048  8192  0    blocks.7.ffn.value.weight\n",
      "2048  2048  1.0  blocks.8.att.receptance.weight\n",
      "2048  2048  1.0  blocks.8.att.key.weight\n",
      "2048  2048  1.0  blocks.8.att.value.weight\n",
      "2048  2048  0    blocks.8.att.output.weight\n",
      "8192  2048  1.0  blocks.8.ffn.key.weight\n",
      "2048  2048  0    blocks.8.ffn.receptance.weight\n",
      "2048  8192  0    blocks.8.ffn.value.weight\n",
      "2048  2048  1.0  blocks.9.att.receptance.weight\n",
      "2048  2048  1.0  blocks.9.att.key.weight\n",
      "2048  2048  1.0  blocks.9.att.value.weight\n",
      "2048  2048  0    blocks.9.att.output.weight\n",
      "8192  2048  1.0  blocks.9.ffn.key.weight\n",
      "2048  2048  0    blocks.9.ffn.receptance.weight\n",
      "2048  8192  0    blocks.9.ffn.value.weight\n",
      "2048  2048  1.0  blocks.10.att.receptance.weight\n",
      "2048  2048  1.0  blocks.10.att.key.weight\n",
      "2048  2048  1.0  blocks.10.att.value.weight\n",
      "2048  2048  0    blocks.10.att.output.weight\n",
      "8192  2048  1.0  blocks.10.ffn.key.weight\n",
      "2048  2048  0    blocks.10.ffn.receptance.weight\n",
      "2048  8192  0    blocks.10.ffn.value.weight\n",
      "2048  2048  1.0  blocks.11.att.receptance.weight\n",
      "2048  2048  1.0  blocks.11.att.key.weight\n",
      "2048  2048  1.0  blocks.11.att.value.weight\n",
      "2048  2048  0    blocks.11.att.output.weight\n",
      "8192  2048  1.0  blocks.11.ffn.key.weight\n",
      "2048  2048  0    blocks.11.ffn.receptance.weight\n",
      "2048  8192  0    blocks.11.ffn.value.weight\n",
      "2048  2048  1.0  blocks.12.att.receptance.weight\n",
      "2048  2048  1.0  blocks.12.att.key.weight\n",
      "2048  2048  1.0  blocks.12.att.value.weight\n",
      "2048  2048  0    blocks.12.att.output.weight\n",
      "8192  2048  1.0  blocks.12.ffn.key.weight\n",
      "2048  2048  0    blocks.12.ffn.receptance.weight\n",
      "2048  8192  0    blocks.12.ffn.value.weight\n",
      "2048  2048  1.0  blocks.13.att.receptance.weight\n",
      "2048  2048  1.0  blocks.13.att.key.weight\n",
      "2048  2048  1.0  blocks.13.att.value.weight\n",
      "2048  2048  0    blocks.13.att.output.weight\n",
      "8192  2048  1.0  blocks.13.ffn.key.weight\n",
      "2048  2048  0    blocks.13.ffn.receptance.weight\n",
      "2048  8192  0    blocks.13.ffn.value.weight\n",
      "2048  2048  1.0  blocks.14.att.receptance.weight\n",
      "2048  2048  1.0  blocks.14.att.key.weight\n",
      "2048  2048  1.0  blocks.14.att.value.weight\n",
      "2048  2048  0    blocks.14.att.output.weight\n",
      "8192  2048  1.0  blocks.14.ffn.key.weight\n",
      "2048  2048  0    blocks.14.ffn.receptance.weight\n",
      "2048  8192  0    blocks.14.ffn.value.weight\n",
      "2048  2048  1.0  blocks.15.att.receptance.weight\n",
      "2048  2048  1.0  blocks.15.att.key.weight\n",
      "2048  2048  1.0  blocks.15.att.value.weight\n",
      "2048  2048  0    blocks.15.att.output.weight\n",
      "8192  2048  1.0  blocks.15.ffn.key.weight\n",
      "2048  2048  0    blocks.15.ffn.receptance.weight\n",
      "2048  8192  0    blocks.15.ffn.value.weight\n",
      "2048  2048  1.0  blocks.16.att.receptance.weight\n",
      "2048  2048  1.0  blocks.16.att.key.weight\n",
      "2048  2048  1.0  blocks.16.att.value.weight\n",
      "2048  2048  0    blocks.16.att.output.weight\n",
      "8192  2048  1.0  blocks.16.ffn.key.weight\n",
      "2048  2048  0    blocks.16.ffn.receptance.weight\n",
      "2048  8192  0    blocks.16.ffn.value.weight\n",
      "2048  2048  1.0  blocks.17.att.receptance.weight\n",
      "2048  2048  1.0  blocks.17.att.key.weight\n",
      "2048  2048  1.0  blocks.17.att.value.weight\n",
      "2048  2048  0    blocks.17.att.output.weight\n",
      "8192  2048  1.0  blocks.17.ffn.key.weight\n",
      "2048  2048  0    blocks.17.ffn.receptance.weight\n",
      "2048  8192  0    blocks.17.ffn.value.weight\n",
      "2048  2048  1.0  blocks.18.att.receptance.weight\n",
      "2048  2048  1.0  blocks.18.att.key.weight\n",
      "2048  2048  1.0  blocks.18.att.value.weight\n",
      "2048  2048  0    blocks.18.att.output.weight\n",
      "8192  2048  1.0  blocks.18.ffn.key.weight\n",
      "2048  2048  0    blocks.18.ffn.receptance.weight\n",
      "2048  8192  0    blocks.18.ffn.value.weight\n",
      "2048  2048  1.0  blocks.19.att.receptance.weight\n",
      "2048  2048  1.0  blocks.19.att.key.weight\n",
      "2048  2048  1.0  blocks.19.att.value.weight\n",
      "2048  2048  0    blocks.19.att.output.weight\n",
      "8192  2048  1.0  blocks.19.ffn.key.weight\n",
      "2048  2048  0    blocks.19.ffn.receptance.weight\n",
      "2048  8192  0    blocks.19.ffn.value.weight\n",
      "2048  2048  1.0  blocks.20.att.receptance.weight\n",
      "2048  2048  1.0  blocks.20.att.key.weight\n",
      "2048  2048  1.0  blocks.20.att.value.weight\n",
      "2048  2048  0    blocks.20.att.output.weight\n",
      "8192  2048  1.0  blocks.20.ffn.key.weight\n",
      "2048  2048  0    blocks.20.ffn.receptance.weight\n",
      "2048  8192  0    blocks.20.ffn.value.weight\n",
      "2048  2048  1.0  blocks.21.att.receptance.weight\n",
      "2048  2048  1.0  blocks.21.att.key.weight\n",
      "2048  2048  1.0  blocks.21.att.value.weight\n",
      "2048  2048  0    blocks.21.att.output.weight\n",
      "8192  2048  1.0  blocks.21.ffn.key.weight\n",
      "2048  2048  0    blocks.21.ffn.receptance.weight\n",
      "2048  8192  0    blocks.21.ffn.value.weight\n",
      "2048  2048  1.0  blocks.22.att.receptance.weight\n",
      "2048  2048  1.0  blocks.22.att.key.weight\n",
      "2048  2048  1.0  blocks.22.att.value.weight\n",
      "2048  2048  0    blocks.22.att.output.weight\n",
      "8192  2048  1.0  blocks.22.ffn.key.weight\n",
      "2048  2048  0    blocks.22.ffn.receptance.weight\n",
      "2048  8192  0    blocks.22.ffn.value.weight\n",
      "2048  2048  1.0  blocks.23.att.receptance.weight\n",
      "2048  2048  1.0  blocks.23.att.key.weight\n",
      "2048  2048  1.0  blocks.23.att.value.weight\n",
      "2048  2048  0    blocks.23.att.output.weight\n",
      "8192  2048  1.0  blocks.23.ffn.key.weight\n",
      "2048  2048  0    blocks.23.ffn.receptance.weight\n",
      "2048  8192  0    blocks.23.ffn.value.weight\n",
      "50277 2048  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 24 --n_embd 2048 \\\n",
    "        --vocab_size neox --skip-if-exists \\\n",
    "        \"../model/L24-D2048-neox-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 156.13it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 652.40it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-453a617a833a9ad3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-55facafd5f878002_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c858b3c9f0c6f4d0_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 17:48:48,418] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 17:49:29,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 17:49:30,259] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 17:49:30,281] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 17:49:31,486] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 17:49:31,502] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 17:49:32,209] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 17:49:32,426] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07094883918762207 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10155820846557617 seconds\n",
      "Time to load fused_adam op: 0.10171985626220703 seconds\n",
      "Time to load fused_adam op: 0.10140013694763184 seconds\n",
      "Time to load fused_adam op: 0.10138416290283203 seconds\n",
      "Time to load fused_adam op: 0.10138058662414551 seconds\n",
      "Time to load fused_adam op: 0.10143876075744629 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10181403160095215 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07054948806762695 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10218143463134766 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10222554206848145 seconds\n",
      "Time to load utils op: 0.10205793380737305 seconds\n",
      "Time to load utils op: 0.10200071334838867 seconds\n",
      "Time to load utils op: 0.10207867622375488 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10244083404541016 seconds\n",
      "Time to load utils op: 0.10245680809020996 seconds\n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006167888641357422 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005884170532226562 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0006535053253173828 seconds\n",
      "Time to load utils op: 0.0006339550018310547 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006251335144042969 seconds\n",
      "Time to load utils op: 0.0006687641143798828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007290840148925781 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003941059112548828 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 163/163 [13:16<00:00,  4.89s/it, v_num=5uv8, train/loss=6.720]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00,  1.52it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=5uv8, train/loss=6.720, \u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=5uv8, train/loss=6.720, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=5uv8, train/loss=6.720, \n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_2, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_2, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_2', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 527.78it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-453a617a833a9ad3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-55facafd5f878002_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c858b3c9f0c6f4d0_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 18:05:18,891] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 18:05:59,730] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 18:06:00,396] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 18:06:01,419] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 18:06:02,123] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 18:06:02,581] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 18:06:02,610] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 18:06:02,782] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06821012496948242 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10137701034545898 seconds\n",
      "Time to load fused_adam op: 0.10126161575317383 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10139870643615723 seconds\n",
      "Time to load fused_adam op: 0.10135126113891602 seconds\n",
      "Time to load fused_adam op: 0.1015779972076416 seconds\n",
      "Time to load fused_adam op: 0.10168123245239258 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10136723518371582 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06965088844299316 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10233759880065918 seconds\n",
      "Time to load utils op: 0.10262346267700195 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10221982002258301 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10248017311096191 seconds\n",
      "Time to load utils op: 0.10245347023010254 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10281705856323242 seconds\n",
      "Time to load utils op: 0.10257196426391602 seconds\n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006124973297119141 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005970001220703125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009539127349853516 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006566047668457031 seconds\n",
      "Time to load utils op: 0.0006811618804931641 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0014195442199707031 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006961822509765625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004532337188720703 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 163/163 [13:16<00:00,  4.89s/it, v_num=ozm1, train/loss=7.470]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00,  1.52it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=ozm1, train/loss=7.470, \u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=ozm1, train/loss=7.470, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 163/163 [13:29<00:00,  4.97s/it, v_num=ozm1, train/loss=7.470, \n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 + Offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_2_offload, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_2_offload, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 731.22it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-453a617a833a9ad3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-55facafd5f878002_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c858b3c9f0c6f4d0_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 18:21:50,770] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 18:22:30,332] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 18:22:33,508] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 18:22:34,449] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 18:22:34,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 18:22:34,861] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 18:22:34,978] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 18:22:35,039] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.318031072616577 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3535866737365723 seconds\n",
      "Time to load cpu_adam op: 2.361844062805176 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3515727519989014 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.365557909011841 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.366220474243164 seconds\n",
      "Time to load cpu_adam op: 2.362700939178467 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.369903087615967 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07186293601989746 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10246586799621582 seconds\n",
      "Time to load utils op: 0.10269975662231445 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10243797302246094 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10247206687927246 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10722112655639648 seconds\n",
      "Time to load utils op: 0.10273551940917969 seconds\n",
      "Time to load utils op: 0.10743427276611328 seconds\n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0165860652923584 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.04173684120178223 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0011684894561767578 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00992584228515625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007953643798828125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.013325691223144531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000743865966796875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004515647888183594 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 163/163 [14:38<00:00,  5.39s/it, v_num=e7hf, train/loss=6.560]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00,  1.23it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [14:56<00:00,  5.50s/it, v_num=e7hf, train/loss=6.560, \u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [14:56<00:00,  5.50s/it, v_num=e7hf, train/loss=6.560, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 163/163 [14:56<00:00,  5.50s/it, v_num=e7hf, train/loss=6.560, \n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2_offload, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_3, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_3', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_3, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_3', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3, disabling JIT using RWKV_JIT_ON=0\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 663.87it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-453a617a833a9ad3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-55facafd5f878002_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c858b3c9f0c6f4d0_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 18:40:07,206] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 18:40:50,032] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 18:40:51,619] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 18:40:51,938] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 18:40:52,604] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 18:40:53,137] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 18:40:53,284] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 18:40:53,571] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06856870651245117 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10165858268737793 seconds\n",
      "Time to load fused_adam op: 0.10169506072998047 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10156965255737305 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10173845291137695 seconds\n",
      "Time to load fused_adam op: 0.10136723518371582 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10185790061950684 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10255050659179688 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06813526153564453 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10243105888366699 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1021871566772461 seconds\n",
      "Time to load utils op: 0.10261702537536621 seconds\n",
      "Time to load utils op: 0.10255217552185059 seconds\n",
      "Time to load utils op: 0.10246634483337402 seconds\n",
      "Time to load utils op: 0.10227823257446289 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06905722618103027 seconds\n",
      "Parameter Offload: Total persistent parameters: 550400 in 316 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0006895065307617188 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005037784576416016 seconds\n",
      "Time to load utils op: 0.0006363391876220703 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007479190826416016 seconds\n",
      "Time to load utils op: 0.0007007122039794922 seconds\n",
      "Time to load utils op: 0.0007541179656982422 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0068089962005615234 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004458427429199219 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 12.9 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 163 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 512              \n",
      "3 | head   | Linear     | 102 M  | 12.9 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/163 [00:00<?, ?it/s][2023-08-20 18:42:15,054] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0: 100%|██| 163/163 [13:53<00:00,  5.11s/it, v_num=uye0, train/loss=6.560]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:05<00:05,  5.05s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [14:11<00:00,  5.23s/it, v_num=uye0, train/loss=6.560, \u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [14:11<00:00,  5.23s/it, v_num=uye0, train/loss=6.560, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 163/163 [14:11<00:00,  5.23s/it, v_num=uye0, train/loss=6.560, \n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3 + offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3 + offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_3_offload, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_3_offload', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/trainer-v5-validation/config/baseline-4096.yaml', '--trainer.logger.init_args.name=infctx-v5-deepspeed-test (deepspeed_stage_3_offload, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_3_offload', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "[RWKV.lightning_trainer.py] Detected deepspeed_stage_3_offload, disabling JIT using RWKV_JIT_ON=0\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 700.33it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-453a617a833a9ad3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-55facafd5f878002_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-c858b3c9f0c6f4d0_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-20 18:56:50,773] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "[rank: 6] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-20 18:57:27,896] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-20 18:57:32,148] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-20 18:57:35,143] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-20 18:57:35,718] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-20 18:57:36,093] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-20 18:57:36,135] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-20 18:57:36,781] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3151376247406006 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3503682613372803 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.354748249053955 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.350867509841919 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.362011671066284 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3680248260498047 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.357490062713623 seconds\n",
      "Time to load cpu_adam op: 2.361243724822998 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07529520988464355 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10221672058105469 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10230636596679688 seconds\n",
      "Time to load utils op: 0.10221529006958008 seconds\n",
      "Time to load utils op: 0.10243844985961914 seconds\n",
      "Time to load utils op: 0.10225367546081543 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10227370262145996 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0762176513671875 seconds\n",
      "Parameter Offload: Total persistent parameters: 550400 in 316 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006818771362304688 seconds\n",
      "Time to load utils op: 0.0006885528564453125 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007338523864746094 seconds\n",
      "Time to load utils op: 0.0007803440093994141 seconds\n",
      "Time to load utils op: 0.0007917881011962891 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0013995170593261719 seconds\n",
      "Time to load utils op: 0.0012483596801757812 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004684925079345703 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 12.9 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 163 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 512              \n",
      "3 | head   | Linear     | 102 M  | 12.9 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                          | 0/163 [00:00<?, ?it/s][2023-08-20 18:59:39,188] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0: 100%|██| 163/163 [16:56<00:00,  6.24s/it, v_num=f6cp, train/loss=6.690]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:05<00:05,  5.15s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [17:20<00:00,  6.38s/it, v_num=f6cp, train/loss=6.690, \u001b[A\n",
      "Epoch 0: 100%|█| 163/163 [17:20<00:00,  6.38s/it, v_num=f6cp, train/loss=6.690, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 163/163 [17:20<00:00,  6.38s/it, v_num=f6cp, train/loss=6.690, \n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/baseline-4096.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3_offload, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
